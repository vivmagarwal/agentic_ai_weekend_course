{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **Chains and LangChain Expression Language (LCEL)**\n\n## **Learning Objectives**\nBy the end of this notebook, you will be able to:\n- Build chains using the pipe operator (|) in LCEL\n- Connect multiple components into complex workflows\n- Implement parallel and sequential execution patterns\n- Use output parsers to structure LLM responses\n- Handle errors and implement fallbacks in chains\n- Create reusable chain components\n\n## **Why This Matters: Building Production Workflows**\n\n**In Real Applications:**\n- Chains connect prompts ‚Üí LLMs ‚Üí parsers ‚Üí tools\n- LCEL enables readable, maintainable pipelines\n- Complex workflows become simple to express\n\n**In RAG Systems:**\n- Query ‚Üí Retrieval ‚Üí Context ‚Üí Generation ‚Üí Formatting\n- All connected seamlessly with LCEL\n- Easy to modify and extend pipelines\n\n**In AI Agents:**\n- Tool selection ‚Üí Execution ‚Üí Response parsing\n- Decision trees and conditional logic\n- Parallel tool execution for efficiency\n\n## **Prerequisites**\n- Completed notebooks 00, 01, and 02\n- Understanding of prompts and templates\n- Basic knowledge of function composition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Setup: Install and Import Dependencies**\n\nRun this cell first to set up your environment:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai python-dotenv\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import (\n",
    "    StrOutputParser,\n",
    "    JsonOutputParser,\n",
    "    PydanticOutputParser,\n",
    "    CommaSeparatedListOutputParser\n",
    ")\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify setup\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ Environment ready! Let's build chains with LCEL.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please set your OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 1: Introduction to LCEL and the Pipe Operator**\n\n**Concept**: LCEL uses the pipe operator (|) to chain components together, creating readable workflows that process data step by step.\n\n### **Example 1: Your First Chain**\n\n**Problem**: Connect prompt ‚Üí LLM ‚Üí parser in a single chain\n**Expected Output**: A complete processing pipeline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n# **Define components**\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a {adjective} joke about {topic}.\"\n)\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n\noutput_parser = StrOutputParser()\n\n# **Chain components together with the pipe operator**\nchain = prompt | llm | output_parser\n\n# **The chain flows: prompt ‚Üí llm ‚Üí parser**\n# **Input dict ‚Üí formatted prompt ‚Üí LLM response ‚Üí parsed string**\n\n# **Run the chain**\nresult = chain.invoke({\n    \"adjective\": \"funny\",\n    \"topic\": \"programming\"\n})\n\nprint(\"Chain Execution:\")\nprint(\"=\" * 50)\nprint(\"Input ‚Üí Prompt ‚Üí LLM ‚Üí Parser ‚Üí Output\")\nprint(\"=\" * 50)\nprint(result)\n\n# **You can also see intermediate steps**\nprint(\"\\nüìä Chain Structure:\")\nprint(f\"1. Prompt Template: Takes {prompt.input_variables}\")\nprint(f\"2. LLM: {llm.model_name}\")\nprint(f\"3. Parser: {output_parser.__class__.__name__}\")\n```\n\n**Why LCEL is powerful:**\n- Clean, readable syntax\n- Components are composable\n- Easy to modify pipelines\n- Type-safe data flow\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Understanding Data Flow**\n\n**Problem**: Visualize how data transforms through the chain\n**Expected Output**: Clear understanding of each step"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n# **Create a chain with logging at each step**\ndef create_debug_chain():\n    \"\"\"Create a chain that shows data at each step\"\"\"\n    \n    # Step 1: Prompt Template\n    prompt = ChatPromptTemplate.from_template(\n        \"Translate '{text}' to {language}.\"\n    )\n    \n    # Step 2: LLM\n    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n    \n    # Step 3: Output Parser\n    parser = StrOutputParser()\n    \n    # Build the chain\n    chain = prompt | llm | parser\n    \n    return chain, prompt, llm, parser\n\n# **Create and inspect the chain**\nchain, prompt, llm, parser = create_debug_chain()\n\n# **Input data**\ninput_data = {\n    \"text\": \"Hello, how are you?\",\n    \"language\": \"Spanish\"\n}\n\nprint(\"üîç Data Flow Through Chain:\")\nprint(\"=\" * 50)\n\n# **Step 1: Show formatted prompt**\nformatted_prompt = prompt.format_messages(**input_data)\nprint(\"Step 1 - Formatted Prompt:\")\nfor msg in formatted_prompt:\n    print(f\"  {msg.__class__.__name__}: {msg.content}\")\n\n# **Step 2: LLM processes the prompt**\nllm_response = llm.invoke(formatted_prompt)\nprint(f\"\\nStep 2 - LLM Response (type: {type(llm_response).__name__}):\")\nprint(f\"  Content: {llm_response.content}\")\nprint(f\"  Metadata: {llm_response.response_metadata.get('model_name', 'N/A')}\")\n\n# **Step 3: Parser extracts content**\nparsed_output = parser.invoke(llm_response)\nprint(f\"\\nStep 3 - Parsed Output (type: {type(parsed_output).__name__}):\")\nprint(f\"  {parsed_output}\")\n\n# **Now run the complete chain**\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Complete Chain Execution:\")\nfinal_result = chain.invoke(input_data)\nprint(f\"Result: {final_result}\")\n\nprint(\"\\nüí° Each component transforms the data for the next step!\")\n```\n\n**Data transformation flow:**\n1. Dict ‚Üí ChatPromptTemplate ‚Üí Messages\n2. Messages ‚Üí LLM ‚Üí AIMessage\n3. AIMessage ‚Üí Parser ‚Üí String\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Chaining Multiple Operations**\n\n**Problem**: Build a chain that performs multiple transformations\n**Expected Output**: Complex multi-step pipeline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\n\n# **Create a multi-step chain**\n# **Step 1: Generate a story**\nstory_prompt = ChatPromptTemplate.from_template(\n    \"Write a 2-sentence story about {character} in {setting}.\"\n)\n\n# **Step 2: Extract the moral**\nmoral_prompt = ChatPromptTemplate.from_template(\n    \"What is the moral of this story: {story}\"\n)\n\n# **Step 3: Simplify for children**\nsimplify_prompt = ChatPromptTemplate.from_template(\n    \"Explain this moral to a 5-year-old: {moral}\"\n)\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\nparser = StrOutputParser()\n\n# **Build the complete chain**\ncomplete_chain = (\n    # Generate story\n    story_prompt \n    | llm \n    | parser\n    # Pass story to next prompt\n    | RunnableLambda(lambda x: {\"story\": x})\n    | moral_prompt\n    | llm\n    | parser\n    # Pass moral to simplification\n    | RunnableLambda(lambda x: {\"moral\": x})\n    | simplify_prompt\n    | llm\n    | parser\n)\n\n# **You can also build it step by step for clarity**\nstory_chain = story_prompt | llm | parser\nmoral_chain = moral_prompt | llm | parser\nsimplify_chain = simplify_prompt | llm | parser\n\n# **Manual step-by-step execution**\nprint(\"Step-by-Step Chain Execution:\")\nprint(\"=\" * 50)\n\ninput_data = {\"character\": \"a wise owl\", \"setting\": \"a magical forest\"}\n\n# **Step 1**\nstory = story_chain.invoke(input_data)\nprint(\"üìñ Story:\")\nprint(story)\n\n# **Step 2**\nmoral = moral_chain.invoke({\"story\": story})\nprint(\"\\nüí≠ Moral:\")\nprint(moral)\n\n# **Step 3**\nsimple_moral = simplify_chain.invoke({\"moral\": moral})\nprint(\"\\nüë∂ For Kids:\")\nprint(simple_moral)\n\n# **Now run the complete chain**\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Complete Chain Result:\")\nresult = complete_chain.invoke(input_data)\nprint(result)\n\nprint(\"\\n‚úÖ Chains can have multiple transformation steps!\")\n```\n\n**Multi-step chain benefits:**\n- Each step builds on the previous\n- Clear data flow\n- Easy to debug individual steps\n- Modular and reusable\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 1: Practice Building Chains**\n\n**Practice Focus**: Create your own chains using LCEL\n\n### **Exercise 1: Build a Translation Chain**\n\n**Task**: Create a chain that translates text to multiple languages\n**Expected Output**: Multi-language translation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a chain that:\n",
    "# 1. Takes input text\n",
    "# 2. Translates to Spanish\n",
    "# 3. Then translates the Spanish to French\n",
    "# Use prompt | llm | parser for each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\n\n# **Initialize components**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nparser = StrOutputParser()\n\n# **Create translation prompts**\nto_spanish = ChatPromptTemplate.from_template(\n    \"Translate this English text to Spanish: {text}\"\n)\n\nto_french = ChatPromptTemplate.from_template(\n    \"Translate this Spanish text to French: {spanish_text}\"\n)\n\n# **Build the translation chain**\ntranslation_chain = (\n    to_spanish\n    | llm\n    | parser\n    | RunnableLambda(lambda x: {\"spanish_text\": x})\n    | to_french\n    | llm\n    | parser\n)\n\n# **Test the chain**\noriginal_text = \"Good morning! How are you today?\"\n\nprint(\"Translation Chain:\")\nprint(\"=\" * 50)\nprint(f\"üá¨üáß English: {original_text}\")\n\n# **Get Spanish translation first (for demonstration)**\nspanish = (to_spanish | llm | parser).invoke({\"text\": original_text})\nprint(f\"üá™üá∏ Spanish: {spanish}\")\n\n# **Get final French translation**\nfrench = translation_chain.invoke({\"text\": original_text})\nprint(f\"üá´üá∑ French: {french}\")\n\nprint(\"\\n‚úÖ Chain successfully translated through multiple languages!\")\n```\n\n**What you learned:**\n- Chaining multiple translation steps\n- Using RunnableLambda for data transformation\n- Sequential processing with LCEL\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Create a Summary Chain**\n\n**Task**: Build a chain that summarizes text and extracts key points\n**Expected Output**: Summary and bullet points"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a chain that:\n",
    "# 1. Summarizes a long text\n",
    "# 2. Extracts 3 key points\n",
    "# 3. Formats as bullet points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n# **Initialize LLM**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nparser = StrOutputParser()\n\n# **Create prompts for each step**\nsummarize_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Summarize this text in 2-3 sentences:\n    \n    {text}\n    \n    Summary:\"\"\"\n)\n\nextract_points_prompt = ChatPromptTemplate.from_template(\n    \"\"\"From this summary, extract exactly 3 key points as bullet points:\n    \n    {summary}\n    \n    Key Points:\"\"\"\n)\n\n# **Build the summary chain**\nsummary_chain = (\n    summarize_prompt\n    | llm\n    | parser\n    | (lambda x: {\"summary\": x})\n    | extract_points_prompt\n    | llm\n    | parser\n)\n\n# **Test with sample text**\nlong_text = \"\"\"Artificial Intelligence (AI) has transformed how we interact with technology. \nFrom voice assistants like Siri and Alexa to recommendation systems on Netflix and Spotify, \nAI is everywhere. Machine learning models can now diagnose diseases, drive cars, and even \ncreate art. However, with great power comes great responsibility. We must consider the \nethical implications of AI, including privacy concerns, job displacement, and algorithmic bias. \nThe future of AI promises even more advances, with potential breakthroughs in general \nartificial intelligence that could revolutionize every aspect of human life.\"\"\"\n\nprint(\"Summary Chain Results:\")\nprint(\"=\" * 50)\nprint(\"üìÑ Original Text:\")\nprint(long_text[:150] + \"...\\n\")\n\n# **Get summary only**\nsummary = (summarize_prompt | llm | parser).invoke({\"text\": long_text})\nprint(\"üìù Summary:\")\nprint(summary)\n\n# **Get key points**\nkey_points = summary_chain.invoke({\"text\": long_text})\nprint(\"\\nüéØ Key Points:\")\nprint(key_points)\n\nprint(\"\\n‚úÖ Chain extracted summary and key points successfully!\")\n```\n\n**Key takeaway:**\n- Chains can progressively refine information\n- Each step focuses on a specific task\n- Output becomes more structured\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 2: Output Parsing in Chains**\n\n**Concept**: Output parsers convert LLM responses into structured data formats that can be used by applications.\n\n### **Example 1: JSON Output Parser**\n\n**Problem**: Extract structured JSON data from LLM responses\n**Expected Output**: Python dictionaries from text"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom pydantic import BaseModel, Field\n\n# **Define the structure we want**\nclass Product(BaseModel):\n    name: str = Field(description=\"Product name\")\n    price: float = Field(description=\"Price in dollars\")\n    category: str = Field(description=\"Product category\")\n    in_stock: bool = Field(description=\"Whether item is in stock\")\n    rating: float = Field(description=\"Customer rating out of 5\")\n\n# **Create JSON parser with schema**\njson_parser = JsonOutputParser(pydantic_object=Product)\n\n# **Create prompt with format instructions**\nprompt = ChatPromptTemplate.from_template(\n    \"\"\"Extract product information from this description:\n    \n    {description}\n    \n    {format_instructions}\n    \"\"\"\n)\n\n# **Build the chain**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\njson_chain = prompt | llm | json_parser\n\n# **Test with product descriptions**\ndescriptions = [\n    \"The SuperWidget 3000 is our best-selling gadget at $49.99. It's in the electronics category, currently in stock, with a 4.5 star rating.\",\n    \"Get the ComfyPillow Plus for just $29.99! This home goods item is flying off our shelves (in stock!) with an amazing 4.8 star customer rating.\"\n]\n\nprint(\"JSON Output Parsing:\")\nprint(\"=\" * 50)\n\nfor desc in descriptions:\n    result = json_chain.invoke({\n        \"description\": desc,\n        \"format_instructions\": json_parser.get_format_instructions()\n    })\n    \n    print(f\"\\nüìù Description: {desc[:50]}...\")\n    print(f\"üìä Parsed Data (type: {type(result).__name__}):\")\n    for key, value in result.items():\n        print(f\"  {key}: {value}\")\n\nprint(\"\\n‚úÖ JSON parser extracts structured data automatically!\")\n```\n\n**JSON parsing benefits:**\n- Structured data extraction\n- Type validation\n- Direct use in applications\n- Consistent format\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Pydantic Output Parser**\n\n**Problem**: Get type-safe, validated data from LLM\n**Expected Output**: Pydantic model instances"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom pydantic import BaseModel, Field, validator\nfrom typing import List\nfrom datetime import datetime\n\n# **Define complex Pydantic model with validation**\nclass Meeting(BaseModel):\n    title: str = Field(description=\"Meeting title\")\n    date: str = Field(description=\"Meeting date in YYYY-MM-DD format\")\n    time: str = Field(description=\"Meeting time in HH:MM format\")\n    duration_minutes: int = Field(description=\"Duration in minutes\")\n    attendees: List[str] = Field(description=\"List of attendee names\")\n    agenda_items: List[str] = Field(description=\"List of agenda items\")\n    location: str = Field(description=\"Meeting location or 'Virtual'\")\n    \n    @validator('duration_minutes')\n    def duration_must_be_positive(cls, v):\n        if v <= 0:\n            raise ValueError('Duration must be positive')\n        return v\n    \n    @validator('attendees')\n    def at_least_two_attendees(cls, v):\n        if len(v) < 2:\n            raise ValueError('Need at least 2 attendees')\n        return v\n\n# **Create Pydantic parser**\npydantic_parser = PydanticOutputParser(pydantic_object=Meeting)\n\n# **Create prompt**\nprompt = ChatPromptTemplate.from_template(\n    \"\"\"Extract meeting details from this email:\n    \n    {email}\n    \n    {format_instructions}\n    \"\"\"\n)\n\n# **Build chain**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nmeeting_chain = prompt | llm | pydantic_parser\n\n# **Test email**\nemail = \"\"\"Hi team,\n\nLet's have our quarterly review meeting on 2024-03-15 at 14:00. \nThe meeting will last about 90 minutes and will be held in Conference Room A.\n\nAttendees: John Smith, Sarah Johnson, Mike Chen, Lisa Anderson\n\nAgenda:\n1. Q1 Performance Review\n2. Budget Updates\n3. New Project Proposals\n4. Team Feedback\n\nSee you there!\n\"\"\"\n\n# **Parse the email**\nresult = meeting_chain.invoke({\n    \"email\": email,\n    \"format_instructions\": pydantic_parser.get_format_instructions()\n})\n\nprint(\"Pydantic Output Parsing:\")\nprint(\"=\" * 50)\nprint(f\"‚úÖ Parsed Meeting (type: {type(result).__name__}):\")\nprint(f\"  Title: {result.title}\")\nprint(f\"  Date: {result.date}\")\nprint(f\"  Time: {result.time}\")\nprint(f\"  Duration: {result.duration_minutes} minutes\")\nprint(f\"  Location: {result.location}\")\nprint(f\"  Attendees: {', '.join(result.attendees)}\")\nprint(f\"  Agenda:\")\nfor item in result.agenda_items:\n    print(f\"    - {item}\")\n\n# **Demonstrate validation**\nprint(\"\\nüîç Validation Check:\")\nprint(f\"  Attendee count: {len(result.attendees)} (‚úì >= 2)\")\nprint(f\"  Duration: {result.duration_minutes} min (‚úì > 0)\")\n\nprint(\"\\nüí° Pydantic provides type safety and validation!\")\n```\n\n**Pydantic parser advantages:**\n- Type safety with validation\n- Complex nested structures\n- Custom validation rules\n- IDE autocomplete support\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: List and Custom Parsers**\n\n**Problem**: Parse different output formats\n**Expected Output**: Lists and custom formats"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import (\n    CommaSeparatedListOutputParser,\n    StructuredOutputParser,\n    ResponseSchema\n)\n\n# **1. Comma-separated list parser**\nlist_parser = CommaSeparatedListOutputParser()\n\nlist_prompt = ChatPromptTemplate.from_template(\n    \"\"\"List 5 {category} items.\n    {format_instructions}\n    \"\"\"\n)\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\nlist_chain = list_prompt | llm | list_parser\n\n# **Test list parser**\ncategories = [\"programming languages\", \"fruits\", \"countries in Europe\"]\n\nprint(\"List Output Parsing:\")\nprint(\"=\" * 50)\n\nfor category in categories:\n    result = list_chain.invoke({\n        \"category\": category,\n        \"format_instructions\": list_parser.get_format_instructions()\n    })\n    print(f\"\\nüìù {category.title()}:\")\n    for i, item in enumerate(result, 1):\n        print(f\"  {i}. {item}\")\n\n# **2. Structured output parser with schemas**\nresponse_schemas = [\n    ResponseSchema(name=\"summary\", description=\"One sentence summary\"),\n    ResponseSchema(name=\"pros\", description=\"List of pros, separated by semicolons\"),\n    ResponseSchema(name=\"cons\", description=\"List of cons, separated by semicolons\"),\n    ResponseSchema(name=\"rating\", description=\"Rating from 1-10\")\n]\n\nstructured_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n\nreview_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Review this product: {product}\n    \n    {format_instructions}\n    \"\"\"\n)\n\nreview_chain = review_prompt | llm | structured_parser\n\n# **Test structured parser**\nprint(\"\\n\\nStructured Output Parsing:\")\nprint(\"=\" * 50)\n\nproduct = \"AI-powered smart home assistant\"\nreview = review_chain.invoke({\n    \"product\": product,\n    \"format_instructions\": structured_parser.get_format_instructions()\n})\n\nprint(f\"\\nüõçÔ∏è Product Review: {product}\")\nprint(f\"üìù Summary: {review['summary']}\")\nprint(f\"‚úÖ Pros: {review['pros']}\")\nprint(f\"‚ùå Cons: {review['cons']}\")\nprint(f\"‚≠ê Rating: {review['rating']}/10\")\n\nprint(\"\\n‚úÖ Different parsers for different output needs!\")\n```\n\n**Parser selection guide:**\n- CommaSeparatedListOutputParser: Simple lists\n- StructuredOutputParser: Key-value pairs\n- JsonOutputParser: Complex nested data\n- PydanticOutputParser: Type-safe with validation\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 2: Practice Output Parsing**\n\n**Practice Focus**: Use different parsers in your chains\n\n### **Exercise 1: Extract Event Information**\n\n**Task**: Create a chain that extracts event details as JSON\n**Expected Output**: Structured event data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a Pydantic model for an Event with:\n",
    "# - name, date, time, venue, ticket_price\n",
    "# Build a chain that extracts this from text descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom pydantic import BaseModel, Field\n\n# **Define Event model**\nclass Event(BaseModel):\n    name: str = Field(description=\"Event name\")\n    date: str = Field(description=\"Event date\")\n    time: str = Field(description=\"Event start time\")\n    venue: str = Field(description=\"Event venue/location\")\n    ticket_price: float = Field(description=\"Ticket price in dollars\")\n\n# **Create parser and prompt**\nevent_parser = PydanticOutputParser(pydantic_object=Event)\n\nevent_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Extract event information from this announcement:\n    \n    {announcement}\n    \n    {format_instructions}\n    \"\"\"\n)\n\n# **Build chain**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nevent_chain = event_prompt | llm | event_parser\n\n# **Test with event announcements**\nannouncements = [\n    \"\"\"Join us for the Annual Tech Conference on March 20th, 2024 at 9:00 AM. \n    The event will be held at the Convention Center. Tickets are $150 each.\"\"\",\n    \n    \"\"\"Don't miss the Summer Music Festival! Saturday, June 15th starting at 2 PM \n    at Central Park. Early bird tickets just $45!\"\"\"\n]\n\nprint(\"Event Information Extraction:\")\nprint(\"=\" * 50)\n\nfor announcement in announcements:\n    event = event_chain.invoke({\n        \"announcement\": announcement,\n        \"format_instructions\": event_parser.get_format_instructions()\n    })\n    \n    print(f\"\\nüìÖ Event Details:\")\n    print(f\"  Name: {event.name}\")\n    print(f\"  Date: {event.date}\")\n    print(f\"  Time: {event.time}\")\n    print(f\"  Venue: {event.venue}\")\n    print(f\"  Price: ${event.ticket_price}\")\n\nprint(\"\\n‚úÖ Successfully extracted structured event data!\")\n```\n\n**What you learned:**\n- Defining Pydantic models for data\n- Extracting structured information\n- Type-safe parsing in chains\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 3: Parallel and Advanced Chains**\n\n**Concept**: LCEL supports parallel execution and complex routing patterns for efficient workflows.\n\n### **Example 1: Parallel Execution**\n\n**Problem**: Run multiple chains simultaneously\n**Expected Output**: Parallel results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\nimport time\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\nparser = StrOutputParser()\n\n# **Create different analysis prompts**\nsentiment_prompt = ChatPromptTemplate.from_template(\n    \"Analyze the sentiment of this text (positive/negative/neutral): {text}\"\n)\n\nsummary_prompt = ChatPromptTemplate.from_template(\n    \"Summarize this text in one sentence: {text}\"\n)\n\nkeywords_prompt = ChatPromptTemplate.from_template(\n    \"Extract 3 main keywords from this text: {text}\"\n)\n\nlanguage_prompt = ChatPromptTemplate.from_template(\n    \"What language is this text written in: {text}\"\n)\n\n# **Create individual chains**\nsentiment_chain = sentiment_prompt | llm | parser\nsummary_chain = summary_prompt | llm | parser\nkeywords_chain = keywords_prompt | llm | parser\nlanguage_chain = language_prompt | llm | parser\n\n# **Create parallel chain**\nparallel_chain = RunnableParallel(\n    sentiment=sentiment_chain,\n    summary=summary_chain,\n    keywords=keywords_chain,\n    language=language_chain,\n    original=RunnablePassthrough()  # Pass through original text\n)\n\n# **Test text**\ntext = \"\"\"LangChain is an amazing framework for building AI applications. \nIt makes working with large language models so much easier and more enjoyable. \nThe community is growing rapidly and the documentation is excellent.\"\"\"\n\nprint(\"Parallel Chain Execution:\")\nprint(\"=\" * 50)\nprint(\"Analyzing text in parallel...\\n\")\n\n# **Time the parallel execution**\nstart_time = time.time()\nresults = parallel_chain.invoke({\"text\": text})\nparallel_time = time.time() - start_time\n\n# **Display results**\nprint(\"üìä Analysis Results:\")\nprint(f\"  Sentiment: {results['sentiment']}\")\nprint(f\"  Summary: {results['summary']}\")\nprint(f\"  Keywords: {results['keywords']}\")\nprint(f\"  Language: {results['language']}\")\nprint(f\"\\n‚è±Ô∏è Parallel execution time: {parallel_time:.2f}s\")\n\n# **Compare with sequential execution**\nprint(\"\\nComparing with sequential execution...\")\nstart_time = time.time()\nsentiment = sentiment_chain.invoke({\"text\": text})\nsummary = summary_chain.invoke({\"text\": text})\nkeywords = keywords_chain.invoke({\"text\": text})\nlanguage = language_chain.invoke({\"text\": text})\nsequential_time = time.time() - start_time\n\nprint(f\"‚è±Ô∏è Sequential execution time: {sequential_time:.2f}s\")\nprint(f\"\\nüöÄ Parallel is ~{sequential_time/parallel_time:.1f}x faster!\")\n\nprint(\"\\n‚úÖ Parallel execution runs multiple chains simultaneously!\")\n```\n\n**Parallel execution benefits:**\n- Significant speed improvement\n- Better resource utilization\n- All results at once\n- Clean syntax with RunnableParallel\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Conditional Routing**\n\n**Problem**: Route to different chains based on input\n**Expected Output**: Dynamic chain selection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnableBranch\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\nparser = StrOutputParser()\n\n# **Create specialized prompts for different content types**\ntechnical_prompt = ChatPromptTemplate.from_template(\n    \"\"\"You are a technical expert. Provide a detailed technical explanation of: {input}\n    Include technical terms and implementation details.\"\"\"\n)\n\nsimple_prompt = ChatPromptTemplate.from_template(\n    \"\"\"You are explaining to a beginner. Explain this in very simple terms: {input}\n    Use analogies and avoid technical jargon.\"\"\"\n)\n\ncreative_prompt = ChatPromptTemplate.from_template(\n    \"\"\"You are a creative writer. Write a creative story or poem about: {input}\n    Be imaginative and entertaining.\"\"\"\n)\n\n# **Create classifier to determine content type**\nclassifier_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Classify this query into one category: 'technical', 'simple', or 'creative'.\n    Query: {input}\n    Return only the category word.\"\"\"\n)\n\nclassifier_chain = classifier_prompt | llm | parser | (lambda x: x.strip().lower())\n\n# **Create specialized chains**\ntechnical_chain = technical_prompt | llm | parser\nsimple_chain = simple_prompt | llm | parser\ncreative_chain = creative_prompt | llm | parser\n\n# **Create routing function**\ndef route_query(input_dict):\n    \"\"\"Route to appropriate chain based on classification\"\"\"\n    classification = classifier_chain.invoke(input_dict)\n    \n    print(f\"üéØ Classified as: {classification}\")\n    \n    if \"technical\" in classification:\n        return technical_chain.invoke(input_dict)\n    elif \"simple\" in classification:\n        return simple_chain.invoke(input_dict)\n    elif \"creative\" in classification:\n        return creative_chain.invoke(input_dict)\n    else:\n        return simple_chain.invoke(input_dict)  # Default\n\n# **Create the routing chain**\nrouting_chain = RunnableLambda(route_query)\n\n# **Test with different queries**\ntest_queries = [\n    \"How does a neural network backpropagation algorithm work?\",\n    \"What is a computer?\",\n    \"Write a poem about artificial intelligence\"\n]\n\nprint(\"Conditional Routing Chain:\")\nprint(\"=\" * 50)\n\nfor query in test_queries:\n    print(f\"\\nüìù Query: {query}\")\n    response = routing_chain.invoke({\"input\": query})\n    print(f\"üì§ Response: {response[:200]}...\\n\")\n    print(\"-\" * 30)\n\nprint(\"\\n‚úÖ Routing enables dynamic chain selection!\")\n```\n\n**Routing benefits:**\n- Different handling for different inputs\n- Specialized processing pipelines\n- Dynamic workflow adaptation\n- Optimal response generation\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 3: Build Advanced Chains**\n\n**Practice Focus**: Create parallel and conditional chains\n\n### **Exercise 1: Build a Parallel Analysis Chain**\n\n**Task**: Create a chain that analyzes text in multiple ways simultaneously\n**Expected Output**: Multiple analysis results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a parallel chain that:\n",
    "# 1. Counts words\n",
    "# 2. Identifies the topic\n",
    "# 3. Detects the tone\n",
    "# Run all three in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableParallel, RunnableLambda\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nparser = StrOutputParser()\n\n# **Create analysis prompts**\nword_count_prompt = ChatPromptTemplate.from_template(\n    \"Count the number of words in this text and return just the number: {text}\"\n)\n\ntopic_prompt = ChatPromptTemplate.from_template(\n    \"What is the main topic of this text? Answer in 2-3 words: {text}\"\n)\n\ntone_prompt = ChatPromptTemplate.from_template(\n    \"What is the tone of this text (formal/informal/neutral/excited/serious)?: {text}\"\n)\n\n# **Create parallel analysis chain**\nanalysis_chain = RunnableParallel(\n    word_count=word_count_prompt | llm | parser,\n    topic=topic_prompt | llm | parser,\n    tone=tone_prompt | llm | parser,\n    # Bonus: add a simple character count\n    char_count=RunnableLambda(lambda x: str(len(x['text'])))\n)\n\n# **Test texts**\ntexts = [\n    \"\"\"Machine learning is revolutionizing how we solve problems. \n    From healthcare to finance, AI is making a huge impact!\"\"\",\n    \n    \"\"\"The quarterly report shows concerning trends. Revenue is down 15% \n    and we need to take immediate action to address these challenges.\"\"\"\n]\n\nprint(\"Parallel Text Analysis:\")\nprint(\"=\" * 50)\n\nfor i, text in enumerate(texts, 1):\n    print(f\"\\nüìÑ Text {i}: {text[:50]}...\")\n    \n    results = analysis_chain.invoke({\"text\": text})\n    \n    print(\"üìä Analysis Results:\")\n    print(f\"  Words: {results['word_count']}\")\n    print(f\"  Characters: {results['char_count']}\")\n    print(f\"  Topic: {results['topic']}\")\n    print(f\"  Tone: {results['tone']}\")\n\nprint(\"\\n‚úÖ Parallel analysis provides comprehensive insights instantly!\")\n```\n\n**What you learned:**\n- Running multiple analyses in parallel\n- Combining LLM and non-LLM operations\n- Efficient text processing\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Optional Extra Practice**\n\n### **Challenge: Build a Complete Document Processing Pipeline**\n\n**Task**: Create a chain that processes documents end-to-end\n**Expected Output**: Fully processed document with multiple outputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain_core.runnables import RunnableParallel, RunnableLambda\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\n# **Define output structure**\nclass DocumentAnalysis(BaseModel):\n    title: str = Field(description=\"Document title\")\n    summary: str = Field(description=\"Executive summary\")\n    key_points: List[str] = Field(description=\"Main points\")\n    sentiment: str = Field(description=\"Overall sentiment\")\n    recommendations: List[str] = Field(description=\"Action items\")\n    category: str = Field(description=\"Document category\")\n\n# **Initialize components**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nanalysis_parser = PydanticOutputParser(pydantic_object=DocumentAnalysis)\n\n# **Create comprehensive analysis prompt**\nanalysis_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Analyze this document comprehensively:\n    \n    {document}\n    \n    {format_instructions}\n    \"\"\"\n)\n\n# **Create quality check prompt**\nquality_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Rate the quality of this document from 1-10 and explain why:\n    {document}\n    \"\"\"\n)\n\n# **Create translation prompt**\ntranslation_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Translate the key message of this document to Spanish:\n    {document}\n    \"\"\"\n)\n\n# **Build the complete pipeline**\ndocument_pipeline = RunnableParallel(\n    # Main analysis\n    analysis=(\n        analysis_prompt \n        | llm \n        | analysis_parser\n    ),\n    # Quality assessment\n    quality=(\n        quality_prompt \n        | llm \n        | StrOutputParser()\n    ),\n    # Translation\n    spanish_summary=(\n        translation_prompt \n        | llm \n        | StrOutputParser()\n    ),\n    # Word count\n    statistics=RunnableLambda(\n        lambda x: {\n            \"word_count\": len(x['document'].split()),\n            \"char_count\": len(x['document']),\n            \"line_count\": len(x['document'].split('\\n'))\n        }\n    )\n)\n\n# **Test document**\ndocument = \"\"\"Strategic Plan 2024\n\nOur company faces both challenges and opportunities in the coming year. \nMarket conditions are volatile, but our strong product portfolio positions us well.\n\nKey Priorities:\n1. Expand into emerging markets\n2. Strengthen digital capabilities\n3. Improve operational efficiency\n\nWe must act decisively to maintain our competitive advantage while managing risks.\nSuccess will require coordinated efforts across all departments.\n\"\"\"\n\n# **Process the document**\nresults = document_pipeline.invoke({\n    \"document\": document,\n    \"format_instructions\": analysis_parser.get_format_instructions()\n})\n\nprint(\"üìÑ Complete Document Processing Pipeline\")\nprint(\"=\" * 60)\n\n# **Display comprehensive results**\nanalysis = results['analysis']\nprint(f\"\\nüìä Document Analysis:\")\nprint(f\"  Title: {analysis.title}\")\nprint(f\"  Category: {analysis.category}\")\nprint(f\"  Sentiment: {analysis.sentiment}\")\nprint(f\"\\n  Summary: {analysis.summary}\")\nprint(f\"\\n  Key Points:\")\nfor point in analysis.key_points:\n    print(f\"    ‚Ä¢ {point}\")\nprint(f\"\\n  Recommendations:\")\nfor rec in analysis.recommendations:\n    print(f\"    ‚Üí {rec}\")\n\nprint(f\"\\n‚≠ê Quality Assessment:\")\nprint(f\"  {results['quality'][:200]}...\")\n\nprint(f\"\\nüåç Spanish Summary:\")\nprint(f\"  {results['spanish_summary'][:200]}...\")\n\nprint(f\"\\nüìà Document Statistics:\")\nstats = results['statistics']\nprint(f\"  Words: {stats['word_count']}\")\nprint(f\"  Characters: {stats['char_count']}\")\nprint(f\"  Lines: {stats['line_count']}\")\n\nprint(\"\\n‚úÖ Complete pipeline processed document with multiple outputs!\")\n```\n\n**Pipeline capabilities:**\n- Comprehensive document analysis\n- Multiple parallel operations\n- Structured data extraction\n- Multi-language support\n- Statistical analysis\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Summary & Next Steps**\n\n### **What You've Learned**\n‚úÖ Building chains with the pipe operator (|) in LCEL  \n‚úÖ Connecting prompts, LLMs, and parsers into workflows  \n‚úÖ Using output parsers for structured data extraction  \n‚úÖ Implementing parallel execution with RunnableParallel  \n‚úÖ Creating conditional routing in chains  \n‚úÖ Building complex multi-step pipelines  \n\n### **Key Takeaways**\n1. **LCEL makes chains readable** - The pipe operator creates clear data flow\n2. **Output parsers structure data** - Convert text to JSON, Pydantic, lists\n3. **Parallel execution saves time** - Run multiple operations simultaneously\n4. **Chains are composable** - Build complex workflows from simple components\n5. **Routing enables flexibility** - Different paths for different inputs\n\n### **What's Next?**\nIn the next notebooks, you'll learn:\n- Document loading and processing\n- Embeddings and vector stores\n- Building complete RAG systems\n- Creating AI agents with tools\n- Production deployment strategies\n\n### **Resources**\n- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)\n- [Output Parsers Documentation](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n- [Runnable Interface](https://python.langchain.com/docs/expression_language/interface)\n- [LCEL Cookbook](https://python.langchain.com/docs/expression_language/cookbook/)\n\n---\n\nüéâ **Congratulations!** You've mastered chains and LCEL! You can now build sophisticated LLM workflows and processing pipelines."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}