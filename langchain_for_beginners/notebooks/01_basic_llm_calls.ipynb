{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **Basic LLM Calls with LangChain**\n\n## **Learning Objectives**\nBy the end of this notebook, you will be able to:\n- Understand different message types (System, Human, AI) and their roles\n- Build multi-turn conversations with proper message history\n- Work with different LLM providers (OpenAI, Anthropic, Google)\n- Stream responses for better user experience\n- Handle errors gracefully and implement retries\n- Optimize costs through model selection and token management\n\n## **Why This Matters: Building Conversational AI**\n\n**In Chatbots:**\n- System messages define personality and behavior\n- Message history enables context-aware responses\n- Streaming improves perceived responsiveness\n\n**In Production Applications:**\n- Error handling ensures reliability\n- Token management controls costs\n- Provider flexibility avoids vendor lock-in\n\n**In AI Assistants:**\n- Multi-turn conversations enable complex interactions\n- Role-based messaging creates specialized agents\n- Proper message structure improves response quality\n\n## **Prerequisites**\n- Completed notebook 00 (Introduction and Setup)\n- OpenAI API key configured\n- Basic understanding of chat interfaces"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Setup: Install and Import Dependencies**\n\nRun this cell first to set up your environment:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langchain-anthropic langchain-google-genai python-dotenv\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage, \n",
    "    SystemMessage, \n",
    "    AIMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please set your OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 1: Understanding Message Types**\n\n**Concept**: Different message types serve different purposes in LLM conversations. Understanding when and how to use each type is crucial for building effective AI applications.\n\n### **Example 1: The Three Core Message Types**\n\n**Problem**: Understand the role of each message type\n**Expected Output**: Clear demonstration of System, Human, and AI messages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n\n# **Initialize LLM**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\n# **1. SystemMessage: Sets the context and behavior**\nsystem_msg = SystemMessage(\n    content=\"\"\"You are a helpful Python tutor. \n    You explain concepts clearly with examples.\n    You're encouraging and patient with beginners.\"\"\"\n)\n\n# **2. HumanMessage: User input/questions**\nhuman_msg = HumanMessage(\n    content=\"What are Python lists and how do I use them?\"\n)\n\n# **3. AIMessage: Previous assistant responses (for context)**\n# **This would typically come from a previous interaction**\nai_msg = AIMessage(\n    content=\"I'd be happy to explain Python lists! Let me break it down for you...\"\n)\n\n# **Send messages to LLM**\nmessages = [system_msg, human_msg]\nresponse = llm.invoke(messages)\n\nprint(\"Message Types Demonstration:\")\nprint(\"=\" * 50)\nprint(f\"SystemMessage: {system_msg.content[:50]}...\")\nprint(f\"HumanMessage: {human_msg.content}\")\nprint(f\"\\nAI Response:\")\nprint(response.content[:300] + \"...\")\n```\n\n**Why message types matter:**\n- **SystemMessage**: Defines the AI's role, personality, and constraints\n- **HumanMessage**: Represents user input in the conversation\n- **AIMessage**: Stores previous AI responses for context continuity\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: System Messages Control Behavior**\n\n**Problem**: See how different system messages change LLM behavior\n**Expected Output**: Different response styles from the same question"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\nquestion = HumanMessage(content=\"Explain what recursion is.\")\n\n# **Personality 1: Formal Technical Expert**\nformal_system = SystemMessage(\n    content=\"You are a formal computer science professor. Use technical terminology and academic language.\"\n)\nformal_response = llm.invoke([formal_system, question])\n\n# **Personality 2: Friendly Tutor**\nfriendly_system = SystemMessage(\n    content=\"You are a friendly coding buddy. Use simple language, analogies, and be encouraging.\"\n)\nfriendly_response = llm.invoke([friendly_system, question])\n\n# **Personality 3: Pirate Programmer**\npirate_system = SystemMessage(\n    content=\"You are a pirate who happens to be a programmer. Speak like a pirate but explain accurately.\"\n)\npirate_response = llm.invoke([pirate_system, question])\n\nprint(\"Same Question, Different System Messages:\")\nprint(\"=\" * 50)\nprint(\"\\nüéì FORMAL PROFESSOR:\")\nprint(formal_response.content[:200] + \"...\")\nprint(\"\\nüòä FRIENDLY TUTOR:\")\nprint(friendly_response.content[:200] + \"...\")\nprint(\"\\nüè¥‚Äç‚ò†Ô∏è PIRATE PROGRAMMER:\")\nprint(pirate_response.content[:200] + \"...\")\n```\n\n**Key insight:**\nSystem messages are powerful tools for:\n- Setting tone and personality\n- Defining expertise level\n- Establishing constraints and rules\n- Creating specialized agents\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Building Multi-Turn Conversations**\n\n**Problem**: Create a conversation with memory of previous exchanges\n**Expected Output**: AI remembers context from earlier messages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\n# **Build a conversation history**\nconversation = [\n    SystemMessage(content=\"You are a helpful assistant. Remember details from our conversation.\"),\n    HumanMessage(content=\"Hi! My name is Alex and I'm learning Python.\"),\n    AIMessage(content=\"Hello Alex! It's great to meet you. Python is an excellent language to learn. What brings you to Python?\"),\n    HumanMessage(content=\"I want to build web applications.\"),\n    AIMessage(content=\"That's fantastic, Alex! Python is perfect for web development with frameworks like Django and Flask.\"),\n    HumanMessage(content=\"What was my name again? And what did I say I wanted to build?\")\n]\n\n# **LLM will have context of entire conversation**\nresponse = llm.invoke(conversation)\n\nprint(\"Multi-Turn Conversation with Memory:\")\nprint(\"=\" * 50)\nprint(\"\\nüìù Conversation History:\")\nfor i, msg in enumerate(conversation[1:], 1):  # Skip system message\n    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n    print(f\"{i}. {role}: {msg.content[:60]}...\")\n\nprint(\"\\nü§ñ AI Response (with memory):\")\nprint(response.content)\nprint(\"\\n‚úÖ Notice how the AI remembers Alex's name and goals!\")\n```\n\n**Why conversation history matters:**\n- Maintains context across interactions\n- Enables follow-up questions\n- Creates more natural conversations\n- Essential for chatbot applications\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 1: Practice with Message Types**\n\n**Practice Focus**: Create and use different message types to control LLM behavior\n\n### **Exercise 1: Create Your Own System Message**\n\n**Task**: Create an LLM with a specific personality\n**Expected Output**: LLM responds in character"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create an LLM that acts as a motivational fitness coach\n",
    "# Ask it for advice about starting an exercise routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# **Initialize LLM**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n\n# **Create fitness coach personality**\nsystem_message = SystemMessage(\n    content=\"\"\"You are an enthusiastic and motivational fitness coach. \n    You're supportive, energetic, and always encourage people to start small and build up.\n    Use motivational language and practical tips.\n    Include emojis to be more engaging. üí™\"\"\"\n)\n\n# **User question**\nuser_question = HumanMessage(\n    content=\"I haven't exercised in years and want to get back in shape. Where do I start?\"\n)\n\n# **Get response**\nresponse = llm.invoke([system_message, user_question])\n\nprint(\"üèÉ‚Äç‚ôÇÔ∏è Your Fitness Coach Says:\")\nprint(\"=\" * 50)\nprint(response.content)\n```\n\n**Why this works:**\n- System message defines clear personality\n- Specific instructions guide response style\n- Character stays consistent across interactions\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Build a Multi-Turn Conversation**\n\n**Task**: Create a conversation where the AI remembers previous information\n**Expected Output**: AI recalls details from earlier in the conversation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Build a 3-turn conversation where:\n",
    "# 1. You introduce yourself and mention a hobby\n",
    "# 2. Ask a question related to that hobby\n",
    "# 3. Ask if the AI remembers your name and hobby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\n# **Start building conversation**\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant. Pay attention to details the user shares.\"),\n    HumanMessage(content=\"Hi! I'm Sarah and I love photography, especially landscape photography.\")\n]\n\n# **First response**\nresponse1 = llm.invoke(messages)\nmessages.append(response1)  # Add AI response to history\nprint(\"Turn 1 - AI Response:\")\nprint(response1.content[:200] + \"...\\n\")\n\n# **Second turn**\nmessages.append(HumanMessage(content=\"What camera settings would you recommend for sunset shots?\"))\nresponse2 = llm.invoke(messages)\nmessages.append(response2)\nprint(\"Turn 2 - AI Response:\")\nprint(response2.content[:200] + \"...\\n\")\n\n# **Third turn - test memory**\nmessages.append(HumanMessage(content=\"By the way, do you remember my name and what type of photography I mentioned?\"))\nresponse3 = llm.invoke(messages)\nprint(\"Turn 3 - Memory Test:\")\nprint(response3.content)\nprint(\"\\n‚úÖ The AI remembers Sarah and landscape photography!\")\n```\n\n**Key lesson:**\n- Each message is added to conversation history\n- LLM receives full context with each call\n- This enables coherent multi-turn dialogues\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 3: Compare Different System Messages**\n\n**Task**: See how system messages affect responses to the same question\n**Expected Output**: Three different response styles"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create 3 different system messages:\n",
    "# 1. A 5-year-old explaining things simply\n",
    "# 2. A technical expert using jargon\n",
    "# 3. A poet who answers in rhyme\n",
    "# Ask all three: \"What is artificial intelligence?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\nquestion = HumanMessage(content=\"What is artificial intelligence?\")\n\n# **Three different personalities**\npersonalities = [\n    (\"üë∂ 5-Year-Old\", SystemMessage(\n        content=\"You are a 5-year-old child. Explain things in very simple terms using simple words and comparisons to toys or games.\"\n    )),\n    (\"üî¨ Technical Expert\", SystemMessage(\n        content=\"You are a computer science PhD. Use technical terminology, mention algorithms, neural networks, and mathematical concepts.\"\n    )),\n    (\"üé≠ Poet\", SystemMessage(\n        content=\"You are a poet who answers everything in rhyming verse. Make your explanations flow with rhythm and rhyme.\"\n    ))\n]\n\nprint(\"Same Question, Different Perspectives:\")\nprint(\"=\" * 50)\n\nfor name, system_msg in personalities:\n    response = llm.invoke([system_msg, question])\n    print(f\"\\n{name}:\")\n    print(response.content[:250] + \"...\\n\")\n\nprint(\"üí° System messages completely change how the LLM responds!\")\n```\n\n**Takeaway:**\n- System messages are your primary tool for controlling LLM behavior\n- Same question can yield vastly different responses\n- Choose system messages based on your audience and use case\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 2: Working with Different Providers**\n\n**Concept**: LangChain provides a unified interface for multiple LLM providers, allowing you to switch between them easily.\n\n### **Example 1: Provider Comparison**\n\n**Problem**: Use multiple LLM providers with the same interface\n**Expected Output**: Responses from different providers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n# **from langchain_anthropic import ChatAnthropic  # Uncomment if you have Anthropic key**\n# **from langchain_google_genai import ChatGoogleGenerativeAI  # Uncomment if you have Google key**\n\n# **Same interface for all providers**\nproviders = []\n\n# **OpenAI (always available if you have the key)**\nif os.getenv(\"OPENAI_API_KEY\"):\n    providers.append((\n        \"OpenAI GPT-4-mini\",\n        ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n    ))\n\n# **Anthropic Claude (uncomment if you have key)**\n# **if os.getenv(\"ANTHROPIC_API_KEY\"):**\n# **providers.append((**\n# **\"Anthropic Claude\",**\n# **ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0.7)**\n# **))**\n\n# **Google Gemini (uncomment if you have key)**\n# **if os.getenv(\"GOOGLE_API_KEY\"):**\n# **providers.append((**\n# **\"Google Gemini\",**\n# **ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)**\n# **))**\n\n# **Test with same prompt**\nprompt = \"Write a haiku about programming in Python.\"\n\nprint(\"Provider Comparison:\")\nprint(\"=\" * 50)\n\nfor name, llm in providers:\n    try:\n        response = llm.invoke(prompt)\n        print(f\"\\n{name}:\")\n        print(response.content)\n    except Exception as e:\n        print(f\"\\n{name}: Error - {str(e)[:50]}\")\n\nprint(\"\\nüí° Same code works with all providers!\")\n```\n\n**Why provider flexibility matters:**\n- Avoid vendor lock-in\n- Use best model for each task\n- Cost optimization (different pricing)\n- Redundancy and failover options\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Model Selection Strategy**\n\n**Problem**: Choose the right model for different tasks\n**Expected Output**: Understanding of model trade-offs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nimport time\n\n# **Different models for different purposes**\nmodels = {\n    \"fast_cheap\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7),\n    \"balanced\": ChatOpenAI(model=\"gpt-4o\", temperature=0.7),\n    # \"powerful\": ChatOpenAI(model=\"gpt-4\", temperature=0.7),  # Most capable but expensive\n}\n\n# **Test task**\nsimple_task = \"What is 2+2?\"\ncomplex_task = \"Explain quantum entanglement using an analogy suitable for a 10-year-old.\"\n\nprint(\"Model Selection Strategy:\")\nprint(\"=\" * 50)\n\n# **Test simple task**\nprint(\"\\nüìù Simple Task: 'What is 2+2?'\")\nfor name, llm in models.items():\n    start = time.time()\n    response = llm.invoke(simple_task)\n    elapsed = time.time() - start\n    print(f\"{name}: {response.content[:30]}... (Time: {elapsed:.2f}s)\")\n\n# **Test complex task**\nprint(\"\\nüß† Complex Task: 'Explain quantum entanglement...'\")\nfor name, llm in models.items():\n    start = time.time()\n    response = llm.invoke(complex_task)\n    elapsed = time.time() - start\n    print(f\"\\n{name} (Time: {elapsed:.2f}s):\")\n    print(response.content[:200] + \"...\")\n\nprint(\"\\nüí° Model Selection Guidelines:\")\nprint(\"- gpt-4o-mini: Fast, cheap, great for simple tasks\")\nprint(\"- gpt-4o: Balanced performance and cost\")\nprint(\"- gpt-4: Most capable for complex reasoning (but expensive)\")\n```\n\n**Model selection best practices:**\n- Use cheaper models for simple tasks\n- Reserve expensive models for complex reasoning\n- Consider latency requirements\n- Test different models for your use case\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Fallback Strategy**\n\n**Problem**: Implement fallback when primary model fails\n**Expected Output**: Automatic failover to backup model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom typing import Optional\n\ndef llm_with_fallback(\n    prompt: str,\n    primary_model: str = \"gpt-4o\",\n    fallback_model: str = \"gpt-4o-mini\"\n) -> Optional[str]:\n    \"\"\"Try primary model, fallback if it fails\"\"\"\n    \n    models = [\n        (primary_model, ChatOpenAI(model=primary_model, temperature=0.7, timeout=10)),\n        (fallback_model, ChatOpenAI(model=fallback_model, temperature=0.7, timeout=10))\n    ]\n    \n    for model_name, llm in models:\n        try:\n            print(f\"Trying {model_name}...\")\n            response = llm.invoke(prompt)\n            print(f\"‚úÖ Success with {model_name}\")\n            return response.content\n        except Exception as e:\n            print(f\"‚ùå {model_name} failed: {str(e)[:50]}\")\n            if model_name != fallback_model:\n                print(f\"Falling back to {fallback_model}...\")\n            continue\n    \n    return \"All models failed. Please try again later.\"\n\n# **Test the fallback strategy**\nresult = llm_with_fallback(\n    \"What are the benefits of using TypeScript over JavaScript?\",\n    primary_model=\"gpt-4o\",  # This might fail if you don't have access\n    fallback_model=\"gpt-4o-mini\"  # This should work\n)\n\nprint(\"\\nResponse:\")\nprint(result[:300] + \"...\")\n\nprint(\"\\nüí° Fallback strategies ensure reliability in production!\")\n```\n\n**Why fallback strategies are important:**\n- API rate limits or outages\n- Cost optimization (try cheaper first)\n- Ensure availability\n- Graceful degradation\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 2: Practice with Different Providers**\n\n**Practice Focus**: Work with different models and implement provider strategies\n\n### **Exercise 1: Model Comparison**\n\n**Task**: Compare different OpenAI models for speed and quality\n**Expected Output**: Performance comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Compare gpt-4o-mini and gpt-4o (if available)\n",
    "# Ask both to \"Generate 3 creative names for a coffee shop\"\n",
    "# Measure response time for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nimport time\n\n# **Create different model instances**\nmodels_to_test = [\n    (\"gpt-4o-mini\", ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)),\n    # Add gpt-4o if you have access\n    # (\"gpt-4o\", ChatOpenAI(model=\"gpt-4o\", temperature=0.8)),\n]\n\nprompt = \"Generate 3 creative names for a coffee shop that also sells books.\"\n\nprint(\"Model Performance Comparison:\")\nprint(\"=\" * 50)\n\nresults = []\nfor model_name, llm in models_to_test:\n    try:\n        start_time = time.time()\n        response = llm.invoke(prompt)\n        end_time = time.time()\n        \n        elapsed = end_time - start_time\n        results.append((model_name, elapsed, response.content))\n        \n        print(f\"\\nüìä {model_name}:\")\n        print(f\"Time: {elapsed:.2f} seconds\")\n        print(f\"Response: {response.content[:200]}...\")\n    except Exception as e:\n        print(f\"\\n‚ùå {model_name}: {str(e)[:50]}\")\n\n# **Summary**\nif results:\n    fastest = min(results, key=lambda x: x[1])\n    print(f\"\\nüèÜ Fastest: {fastest[0]} ({fastest[1]:.2f}s)\")\n```\n\n**What you learned:**\n- Different models have different speeds\n- Trade-off between speed and capability\n- Choose models based on requirements\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Create a Model Selector**\n\n**Task**: Build a function that selects models based on task complexity\n**Expected Output**: Automatic model selection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a function that:\n",
    "# - Uses gpt-4o-mini for questions < 20 words\n",
    "# - Uses gpt-4o for longer, complex questions\n",
    "# Test with both simple and complex questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\ndef smart_model_selector(question: str) -> tuple[str, str]:\n    \"\"\"Select appropriate model based on question complexity\"\"\"\n    \n    # Simple heuristic: use word count and keywords\n    word_count = len(question.split())\n    complex_keywords = ['explain', 'analyze', 'compare', 'evaluate', 'design']\n    has_complex_keyword = any(keyword in question.lower() for keyword in complex_keywords)\n    \n    # Select model\n    if word_count < 20 and not has_complex_keyword:\n        model_name = \"gpt-4o-mini\"\n        reason = f\"Simple question ({word_count} words)\"\n    else:\n        model_name = \"gpt-4o\"  # Use gpt-4o-mini if you don't have access\n        reason = f\"Complex question ({word_count} words, complex={'yes' if has_complex_keyword else 'no'})\"\n    \n    # Create and use the selected model\n    llm = ChatOpenAI(model=model_name, temperature=0.7)\n    \n    try:\n        response = llm.invoke(question)\n        return model_name, reason, response.content\n    except:\n        # Fallback to mini if primary fails\n        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n        response = llm.invoke(question)\n        return \"gpt-4o-mini (fallback)\", reason, response.content\n\n# **Test with different questions**\ntest_questions = [\n    \"What is Python?\",  # Simple\n    \"Explain the differences between supervised and unsupervised machine learning, providing examples of algorithms for each category and discussing their real-world applications.\"  # Complex\n]\n\nprint(\"Smart Model Selection:\")\nprint(\"=\" * 50)\n\nfor question in test_questions:\n    model, reason, response = smart_model_selector(question)\n    print(f\"\\nüìù Question: {question[:50]}...\")\n    print(f\"ü§ñ Selected Model: {model}\")\n    print(f\"üìä Reason: {reason}\")\n    print(f\"üí¨ Response: {response[:150]}...\\n\")\n```\n\n**Key insights:**\n- Automatic model selection saves costs\n- Simple heuristics can be effective\n- Always have fallback options\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 3: Streaming Responses**\n\n**Concept**: Stream LLM responses for better user experience, especially for long outputs.\n\n### **Example 1: Basic Streaming**\n\n**Problem**: Stream response tokens as they arrive\n**Expected Output**: Text appearing word by word"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nimport sys\n\n# **Initialize LLM with streaming**\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    streaming=True  # Enable streaming\n)\n\nprompt = \"Tell me a short story about a robot learning to paint. Make it 3 paragraphs.\"\n\nprint(\"Streaming Response:\")\nprint(\"=\" * 50)\n\n# **Stream the response**\nfull_response = \"\"\nfor chunk in llm.stream(prompt):\n    content = chunk.content\n    if content:\n        print(content, end=\"\", flush=True)\n        full_response += content\n\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"\\nüìù Total length: {len(full_response)} characters\")\nprint(\"‚úÖ Streaming provides better UX for long responses!\")\n```\n\n**Why streaming matters:**\n- Better perceived performance\n- Users see progress immediately\n- Can stop generation early if needed\n- Essential for chat interfaces\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Streaming with Progress Indicator**\n\n**Problem**: Add visual feedback during streaming\n**Expected Output**: Progress tracking while streaming"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nimport time\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n\ndef stream_with_progress(prompt: str):\n    \"\"\"Stream response with progress tracking\"\"\"\n    \n    print(\"ü§ñ AI is thinking...\")\n    print(\"-\" * 50)\n    \n    chunks = []\n    start_time = time.time()\n    first_token_time = None\n    \n    for i, chunk in enumerate(llm.stream(prompt)):\n        if chunk.content:\n            if first_token_time is None:\n                first_token_time = time.time()\n                print(f\"‚ö° First token in {first_token_time - start_time:.2f}s\\n\")\n            \n            chunks.append(chunk.content)\n            print(chunk.content, end=\"\", flush=True)\n            \n            # Show progress every 10 chunks\n            if i % 10 == 0 and i > 0:\n                elapsed = time.time() - start_time\n                # print(f\" [{i} chunks, {elapsed:.1f}s]\", end=\"\", flush=True)\n    \n    total_time = time.time() - start_time\n    full_response = \"\".join(chunks)\n    \n    print(f\"\\n\\n\" + \"=\" * 50)\n    print(f\"üìä Streaming Stats:\")\n    print(f\"  ‚Ä¢ Total chunks: {len(chunks)}\")\n    print(f\"  ‚Ä¢ Total time: {total_time:.2f}s\")\n    print(f\"  ‚Ä¢ Characters: {len(full_response)}\")\n    print(f\"  ‚Ä¢ Tokens/sec: ~{len(chunks)/total_time:.1f}\")\n    \n    return full_response\n\n# **Test streaming with progress**\nresponse = stream_with_progress(\n    \"Explain the concept of recursion in programming with an example.\"\n)\n```\n\n**Streaming best practices:**\n- Show \"thinking\" indicator before first token\n- Track time to first token (TTFT)\n- Consider adding abort functionality\n- Buffer output for better display\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 3: Practice Streaming**\n\n**Practice Focus**: Implement streaming responses with different features\n\n### **Exercise 1: Basic Streaming**\n\n**Task**: Stream a response and count the chunks\n**Expected Output**: Streamed text with chunk count"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a streaming LLM\n",
    "# Stream a response to \"What are the benefits of Python?\"\n",
    "# Count and display the number of chunks received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# **Create streaming LLM**\nstreaming_llm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    streaming=True\n)\n\nprompt = \"What are the top 5 benefits of Python programming?\"\n\nprint(\"Streaming Response:\")\nprint(\"=\" * 50)\n\n# **Stream and count chunks**\nchunk_count = 0\nfull_text = \"\"\n\nfor chunk in streaming_llm.stream(prompt):\n    if chunk.content:\n        chunk_count += 1\n        full_text += chunk.content\n        print(chunk.content, end=\"\", flush=True)\n\nprint(f\"\\n\\nüìä Streaming complete!\")\nprint(f\"  ‚Ä¢ Chunks received: {chunk_count}\")\nprint(f\"  ‚Ä¢ Total characters: {len(full_text)}\")\nprint(f\"  ‚Ä¢ Average chunk size: {len(full_text)/chunk_count:.1f} chars\")\n```\n\n**What you learned:**\n- Streaming provides real-time feedback\n- Responses come in chunks\n- Can process data as it arrives\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Streaming with Word Counter**\n\n**Task**: Stream a response and show a live word count\n**Expected Output**: Streaming text with running word count"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Stream a response and display a running word count\n",
    "# Update the count as new words arrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nstreaming_llm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n\ndef stream_with_word_count(prompt: str):\n    \"\"\"Stream response with live word count\"\"\"\n    \n    print(\"üìù Streaming with word count:\")\n    print(\"=\" * 50)\n    \n    word_count = 0\n    buffer = \"\"\n    full_response = \"\"\n    \n    for chunk in streaming_llm.stream(prompt):\n        if chunk.content:\n            # Add to buffer\n            buffer += chunk.content\n            full_response += chunk.content\n            \n            # Count complete words (when we hit a space)\n            if ' ' in buffer:\n                words = buffer.split(' ')\n                # All but last element are complete words\n                word_count += len(words) - 1\n                buffer = words[-1]  # Keep incomplete word in buffer\n            \n            # Display chunk\n            print(chunk.content, end=\"\", flush=True)\n    \n    # Count final word if buffer not empty\n    if buffer.strip():\n        word_count += 1\n    \n    print(f\"\\n\\nüìä Final Statistics:\")\n    print(f\"  ‚Ä¢ Word count: {word_count}\")\n    print(f\"  ‚Ä¢ Character count: {len(full_response)}\")\n    print(f\"  ‚Ä¢ Avg word length: {len(full_response)/word_count:.1f} chars\")\n\n# **Test it**\nstream_with_word_count(\n    \"Write a brief explanation of machine learning in about 50 words.\"\n)\n```\n\n**Key takeaway:**\n- Can process streaming data in real-time\n- Useful for progress tracking\n- Enables interactive features\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Optional Extra Practice**\n\n### **Challenge 1: Build a Chat Interface**\n\n**Task**: Create a simple chat loop with memory\n**Expected Output**: Interactive chat session"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n\ndef simple_chat_interface():\n    \"\"\"Simple chat interface with memory\"\"\"\n    \n    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, streaming=True)\n    \n    # Initialize conversation with system message\n    messages = [\n        SystemMessage(content=\"\"\"You are a helpful AI assistant. \n        Keep your responses concise and friendly.\n        Remember details from our conversation.\"\"\")\n    ]\n    \n    print(\"ü§ñ AI Chat Interface\")\n    print(\"Type 'quit' to exit\")\n    print(\"=\" * 50)\n    \n    while True:\n        # Get user input\n        user_input = input(\"\\nYou: \")\n        \n        if user_input.lower() in ['quit', 'exit', 'bye']:\n            print(\"AI: Goodbye! It was nice chatting with you!\")\n            break\n        \n        # Add user message to history\n        messages.append(HumanMessage(content=user_input))\n        \n        # Get and stream AI response\n        print(\"AI: \", end=\"\")\n        full_response = \"\"\n        \n        for chunk in llm.stream(messages):\n            if chunk.content:\n                print(chunk.content, end=\"\", flush=True)\n                full_response += chunk.content\n        \n        # Add AI response to history\n        messages.append(AIMessage(content=full_response))\n        \n        # Limit conversation history to last 10 messages (+ system)\n        if len(messages) > 11:\n            messages = [messages[0]] + messages[-10:]\n    \n    print(f\"\\n\\nüìä Chat Statistics:\")\n    print(f\"  ‚Ä¢ Total messages: {len(messages) - 1}\")  # Exclude system\n    print(f\"  ‚Ä¢ Your messages: {sum(1 for m in messages if isinstance(m, HumanMessage))}\")\n    print(f\"  ‚Ä¢ AI messages: {sum(1 for m in messages if isinstance(m, AIMessage))}\")\n\n# **Uncomment to run the chat interface**\n# **simple_chat_interface()**\n\nprint(\"üí° To test the chat interface, uncomment the last line!\")\nprint(\"The chat will remember your conversation history.\")\n```\n\n**What this demonstrates:**\n- Full conversation memory\n- Streaming for better UX\n- Message history management\n- Interactive AI application\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Challenge 2: Multi-Model Comparison Tool**\n\n**Task**: Build a tool that compares responses from different models\n**Expected Output**: Side-by-side model comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nimport time\nfrom typing import Dict, List\n\ndef compare_models(\n    prompt: str,\n    models: List[str] = [\"gpt-4o-mini\"],\n    temperature: float = 0.7\n) -> Dict:\n    \"\"\"Compare responses from different models\"\"\"\n    \n    results = {}\n    \n    print(f\"üìù Prompt: {prompt}\")\n    print(\"=\" * 70)\n    \n    for model_name in models:\n        try:\n            print(f\"\\nü§ñ {model_name}:\")\n            print(\"-\" * 40)\n            \n            # Initialize model\n            llm = ChatOpenAI(\n                model=model_name,\n                temperature=temperature,\n                max_tokens=150\n            )\n            \n            # Measure response time\n            start_time = time.time()\n            response = llm.invoke(prompt)\n            elapsed = time.time() - start_time\n            \n            # Store results\n            results[model_name] = {\n                \"response\": response.content,\n                \"time\": elapsed,\n                \"tokens\": len(response.content.split())\n            }\n            \n            # Display results\n            print(response.content[:300])\n            print(f\"\\n‚è±Ô∏è Time: {elapsed:.2f}s\")\n            print(f\"üìä ~{results[model_name]['tokens']} words\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error: {str(e)[:50]}\")\n            results[model_name] = {\"error\": str(e)}\n    \n    # Summary comparison\n    print(\"\\n\" + \"=\" * 70)\n    print(\"üìä COMPARISON SUMMARY:\")\n    print(\"-\" * 40)\n    \n    valid_results = {k: v for k, v in results.items() if \"error\" not in v}\n    \n    if valid_results:\n        fastest = min(valid_results.items(), key=lambda x: x[1][\"time\"])\n        longest = max(valid_results.items(), key=lambda x: x[1][\"tokens\"])\n        \n        print(f\"‚ö° Fastest: {fastest[0]} ({fastest[1]['time']:.2f}s)\")\n        print(f\"üìù Most detailed: {longest[0]} ({longest[1]['tokens']} words)\")\n    \n    return results\n\n# **Test the comparison tool**\nresults = compare_models(\n    prompt=\"Explain the concept of 'technical debt' in software development.\",\n    models=[\"gpt-4o-mini\"],  # Add more models if you have access\n    temperature=0.7\n)\n\nprint(\"\\nüí° Add more models to the list for better comparison!\")\n```\n\n**Why this is useful:**\n- Compare model capabilities\n- Benchmark performance\n- Choose best model for your use case\n- Cost-benefit analysis\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Summary & Next Steps**\n\n### **What You've Learned**\n‚úÖ Different message types and their roles (System, Human, AI)  \n‚úÖ Building multi-turn conversations with memory  \n‚úÖ Working with different LLM providers  \n‚úÖ Streaming responses for better UX  \n‚úÖ Model selection strategies  \n‚úÖ Error handling and fallback patterns  \n\n### **Key Takeaways**\n1. **System messages control behavior** - Use them to define personality and constraints\n2. **Message history enables context** - Build conversations by maintaining message arrays\n3. **Provider flexibility is powerful** - Same code works with OpenAI, Anthropic, Google, etc.\n4. **Streaming improves UX** - Users see progress immediately\n5. **Choose models wisely** - Balance cost, speed, and capability\n\n### **What's Next?**\nIn the next notebook (`02_prompts_and_templates.ipynb`), you'll learn:\n- Creating reusable prompt templates\n- Variable substitution and formatting\n- Few-shot prompting techniques\n- Advanced prompt engineering patterns\n- Building prompt libraries\n\n### **Resources**\n- [LangChain Message Types](https://python.langchain.com/docs/modules/model_io/chat/message_types)\n- [Streaming Documentation](https://python.langchain.com/docs/modules/model_io/chat/streaming)\n- [Model Pricing Comparison](https://openai.com/pricing)\n- [LangChain Providers](https://python.langchain.com/docs/integrations/providers)\n\n---\n\nüéâ **Congratulations!** You've mastered basic LLM calls with LangChain! You can now build conversational AI applications with multiple providers and streaming support."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}