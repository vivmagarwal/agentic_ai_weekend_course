{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading and Processing with LangChain\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Load documents from various sources (PDF, TXT, HTML, CSV, JSON)\n",
    "- Implement different text splitting strategies\n",
    "- Preserve and extract metadata from documents\n",
    "- Handle large documents efficiently\n",
    "- Preprocess text for optimal embedding generation\n",
    "- Build document processing pipelines\n",
    "\n",
    "## Why This Matters: Foundation for RAG Systems\n",
    "\n",
    "**In RAG Applications:**\n",
    "- Documents are the knowledge source\n",
    "- Proper splitting affects retrieval quality\n",
    "- Metadata enables filtering and context\n",
    "\n",
    "**In Enterprise Systems:**\n",
    "- Process diverse document formats\n",
    "- Handle large document collections\n",
    "- Maintain document structure and relationships\n",
    "\n",
    "**In Search Applications:**\n",
    "- Chunk size affects search relevance\n",
    "- Metadata improves search filtering\n",
    "- Preprocessing enhances retrieval\n",
    "\n",
    "## Prerequisites\n",
    "- Completed notebooks 00-04\n",
    "- Basic understanding of file formats\n",
    "- Familiarity with text processing concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Dependencies\n",
    "\n",
    "Run this cell first to set up your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-community langchain-openai python-dotenv \n",
    "!pip install -q pypdf beautifulsoup4 lxml pandas\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "\n",
    "# Document loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    DirectoryLoader,\n",
    "    WebBaseLoader\n",
    ")\n",
    "\n",
    "# Text splitters\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownTextSplitter,\n",
    "    PythonCodeTextSplitter\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify setup\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ Environment ready! Let's load and process documents.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please set your OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 1: Document Loading Fundamentals\n",
    "\n",
    "**Concept**: Load documents from various sources while preserving metadata and structure.\n",
    "\n",
    "### Example 1: Loading Text and PDF Documents\n",
    "\n",
    "**Problem**: Load documents from different file formats\n",
    "**Expected Output**: Document objects with content and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create sample documents for demonstration\n",
    "def create_sample_documents():\n",
    "    \"\"\"Create sample text files for testing\"\"\"\n",
    "    \n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    # Sample text document\n",
    "    text_content = \"\"\"# Introduction to RAG Systems\n",
    "\n",
    "Retrieval Augmented Generation (RAG) combines the power of large language models \n",
    "with external knowledge bases. This approach allows LLMs to access up-to-date \n",
    "information and reduce hallucinations.\n",
    "\n",
    "## Key Components\n",
    "1. Document Loading: Import knowledge from various sources\n",
    "2. Text Splitting: Break documents into manageable chunks\n",
    "3. Embedding Generation: Convert text to vector representations\n",
    "4. Vector Storage: Store embeddings for efficient retrieval\n",
    "5. Query Processing: Find relevant information for user queries\n",
    "\n",
    "## Benefits\n",
    "- Reduced hallucinations\n",
    "- Access to current information\n",
    "- Domain-specific knowledge\n",
    "- Scalable knowledge management\n",
    "\"\"\"\n",
    "    \n",
    "    text_file = os.path.join(temp_dir, \"rag_intro.txt\")\n",
    "    with open(text_file, 'w') as f:\n",
    "        f.write(text_content)\n",
    "    \n",
    "    return temp_dir, text_file\n",
    "\n",
    "# Create sample files\n",
    "temp_dir, text_file = create_sample_documents()\n",
    "\n",
    "print(\"Document Loading Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Load text document\n",
    "text_loader = TextLoader(text_file)\n",
    "text_docs = text_loader.load()\n",
    "\n",
    "print(\"\\nüìÑ Text Document:\")\n",
    "print(f\"  Documents loaded: {len(text_docs)}\")\n",
    "print(f\"  Content preview: {text_docs[0].page_content[:150]}...\")\n",
    "print(f\"  Metadata: {text_docs[0].metadata}\")\n",
    "\n",
    "# 2. Create and load a more complex document with metadata\n",
    "complex_doc = Document(\n",
    "    page_content=\"\"\"Machine learning models can be categorized into supervised, \n",
    "    unsupervised, and reinforcement learning. Each approach has distinct \n",
    "    use cases and requirements.\"\"\",\n",
    "    metadata={\n",
    "        \"source\": \"ml_basics.txt\",\n",
    "        \"author\": \"AI Expert\",\n",
    "        \"date\": \"2024-01-15\",\n",
    "        \"category\": \"machine learning\",\n",
    "        \"importance\": \"high\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Document with Rich Metadata:\")\n",
    "print(f\"  Content: {complex_doc.page_content[:100]}...\")\n",
    "print(f\"  Metadata fields:\")\n",
    "for key, value in complex_doc.metadata.items():\n",
    "    print(f\"    - {key}: {value}\")\n",
    "\n",
    "# 3. Web-based document loading\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load from a web page (using a reliable example URL)\n",
    "web_loader = WebBaseLoader(\"https://python.langchain.com/docs/get_started/introduction\")\n",
    "\n",
    "try:\n",
    "    web_docs = web_loader.load()\n",
    "    print(\"\\nüåê Web Document:\")\n",
    "    print(f\"  Documents loaded: {len(web_docs)}\")\n",
    "    print(f\"  URL: {web_docs[0].metadata.get('source', 'N/A')}\")\n",
    "    print(f\"  Title: {web_docs[0].metadata.get('title', 'N/A')}\")\n",
    "    print(f\"  Content preview: {web_docs[0].page_content[:150]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Web loading failed: {str(e)[:100]}\")\n",
    "    print(\"This is normal if you're offline or the URL is blocked.\")\n",
    "\n",
    "# Clean up\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"\\n‚úÖ Documents loaded with content and metadata preserved!\")\n",
    "```\n",
    "\n",
    "**Document loading benefits:**\n",
    "- Uniform interface for all formats\n",
    "- Automatic metadata extraction\n",
    "- Content preservation\n",
    "- Memory-efficient loading\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Loading Structured Data (CSV, JSON)\n",
    "\n",
    "**Problem**: Load structured data as documents\n",
    "**Expected Output**: Documents from tabular and JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import CSVLoader, JSONLoader\n",
    "import tempfile\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Create sample structured data\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# 1. Create sample CSV\n",
    "csv_file = os.path.join(temp_dir, \"products.csv\")\n",
    "csv_data = [\n",
    "    [\"product_id\", \"name\", \"category\", \"price\", \"description\"],\n",
    "    [\"1\", \"Laptop Pro\", \"Electronics\", \"1299\", \"High-performance laptop with 16GB RAM\"],\n",
    "    [\"2\", \"Wireless Mouse\", \"Accessories\", \"29\", \"Ergonomic wireless mouse with long battery life\"],\n",
    "    [\"3\", \"USB-C Hub\", \"Accessories\", \"49\", \"7-in-1 USB-C hub with HDMI and SD card reader\"],\n",
    "    [\"4\", \"Monitor 4K\", \"Electronics\", \"599\", \"27-inch 4K monitor with HDR support\"]\n",
    "]\n",
    "\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "# 2. Create sample JSON\n",
    "json_file = os.path.join(temp_dir, \"articles.json\")\n",
    "json_data = [\n",
    "    {\n",
    "        \"title\": \"Introduction to LangChain\",\n",
    "        \"author\": \"AI Developer\",\n",
    "        \"date\": \"2024-01-10\",\n",
    "        \"content\": \"LangChain is a framework for building applications with LLMs.\",\n",
    "        \"tags\": [\"AI\", \"LLM\", \"Framework\"]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Building RAG Systems\",\n",
    "        \"author\": \"ML Engineer\",\n",
    "        \"date\": \"2024-01-15\",\n",
    "        \"content\": \"RAG systems combine retrieval with generation for better results.\",\n",
    "        \"tags\": [\"RAG\", \"Retrieval\", \"AI\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "print(\"Structured Data Loading:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load CSV data\n",
    "csv_loader = CSVLoader(\n",
    "    file_path=csv_file,\n",
    "    source_column=\"product_id\",  # Use as source in metadata\n",
    "    csv_args={\n",
    "        'delimiter': ',',\n",
    "        'quotechar': '\"'\n",
    "    }\n",
    ")\n",
    "csv_docs = csv_loader.load()\n",
    "\n",
    "print(\"\\nüìä CSV Documents:\")\n",
    "print(f\"  Documents created: {len(csv_docs)}\")\n",
    "for i, doc in enumerate(csv_docs[:2], 1):\n",
    "    print(f\"\\n  Document {i}:\")\n",
    "    print(f\"    Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"    Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"    Row: {doc.metadata.get('row', 'N/A')}\")\n",
    "\n",
    "# Load JSON data with specific schema\n",
    "def json_metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    \"\"\"Extract metadata from JSON record\"\"\"\n",
    "    metadata[\"title\"] = record.get(\"title\", \"\")\n",
    "    metadata[\"author\"] = record.get(\"author\", \"\")\n",
    "    metadata[\"date\"] = record.get(\"date\", \"\")\n",
    "    metadata[\"tags\"] = \", \".join(record.get(\"tags\", []))\n",
    "    return metadata\n",
    "\n",
    "json_loader = JSONLoader(\n",
    "    file_path=json_file,\n",
    "    jq_schema='.[].content',  # Extract content field\n",
    "    metadata_func=json_metadata_func,\n",
    "    is_content_key_jq_parsable=True\n",
    ")\n",
    "\n",
    "# Note: JSONLoader requires jq library\n",
    "# Alternative: Load JSON manually\n",
    "with open(json_file, 'r') as f:\n",
    "    json_content = json.load(f)\n",
    "\n",
    "json_docs = []\n",
    "for item in json_content:\n",
    "    doc = Document(\n",
    "        page_content=item['content'],\n",
    "        metadata={\n",
    "            'title': item['title'],\n",
    "            'author': item['author'],\n",
    "            'date': item['date'],\n",
    "            'tags': ', '.join(item['tags']),\n",
    "            'source': json_file\n",
    "        }\n",
    "    )\n",
    "    json_docs.append(doc)\n",
    "\n",
    "print(\"\\nüìã JSON Documents:\")\n",
    "print(f\"  Documents created: {len(json_docs)}\")\n",
    "for i, doc in enumerate(json_docs, 1):\n",
    "    print(f\"\\n  Document {i}:\")\n",
    "    print(f\"    Content: {doc.page_content}\")\n",
    "    print(f\"    Title: {doc.metadata.get('title')}\")\n",
    "    print(f\"    Author: {doc.metadata.get('author')}\")\n",
    "    print(f\"    Tags: {doc.metadata.get('tags')}\")\n",
    "\n",
    "# Clean up\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"\\n‚úÖ Structured data successfully converted to documents!\")\n",
    "```\n",
    "\n",
    "**Structured data benefits:**\n",
    "- Each row/record becomes a document\n",
    "- Metadata from columns/fields\n",
    "- Maintains data relationships\n",
    "- Query-able by attributes\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Directory Loading and Filtering\n",
    "\n",
    "**Problem**: Load multiple documents from directories\n",
    "**Expected Output**: Batch-loaded documents with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a sample directory structure\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Create subdirectories\n",
    "docs_dir = os.path.join(temp_dir, \"documents\")\n",
    "os.makedirs(docs_dir)\n",
    "os.makedirs(os.path.join(docs_dir, \"technical\"))\n",
    "os.makedirs(os.path.join(docs_dir, \"business\"))\n",
    "\n",
    "# Create sample files\n",
    "files = [\n",
    "    (\"technical/api_guide.txt\", \"API Documentation: Our REST API provides endpoints for user management...\"),\n",
    "    (\"technical/setup.txt\", \"Setup Instructions: Install Python 3.10+ and run pip install requirements.txt...\"),\n",
    "    (\"technical/architecture.md\", \"# System Architecture\\nOur system uses microservices...\"),\n",
    "    (\"business/proposal.txt\", \"Business Proposal: We propose implementing an AI solution...\"),\n",
    "    (\"business/requirements.txt\", \"Requirements: The system must handle 1000 requests per second...\"),\n",
    "    (\"readme.md\", \"# Project Overview\\nThis project implements a RAG system...\")\n",
    "]\n",
    "\n",
    "for filepath, content in files:\n",
    "    full_path = os.path.join(docs_dir, filepath)\n",
    "    with open(full_path, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Directory Loading Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Load all text files from directory\n",
    "loader_all = DirectoryLoader(\n",
    "    docs_dir,\n",
    "    glob=\"**/*.txt\",  # All .txt files in any subdirectory\n",
    "    loader_cls=TextLoader,\n",
    "    recursive=True,  # Search subdirectories\n",
    "    show_progress=True  # Show loading progress\n",
    ")\n",
    "\n",
    "docs_all = loader_all.load()\n",
    "\n",
    "print(f\"\\nüìÅ All Text Files:\")\n",
    "print(f\"  Total documents: {len(docs_all)}\")\n",
    "for doc in docs_all:\n",
    "    source = doc.metadata['source']\n",
    "    # Extract relative path\n",
    "    rel_path = source.replace(docs_dir, '').lstrip('/')\n",
    "    print(f\"  - {rel_path}: {len(doc.page_content)} chars\")\n",
    "\n",
    "# 2. Load only technical documents\n",
    "loader_technical = DirectoryLoader(\n",
    "    os.path.join(docs_dir, \"technical\"),\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "docs_technical = loader_technical.load()\n",
    "\n",
    "print(f\"\\nüîß Technical Documents Only:\")\n",
    "print(f\"  Total documents: {len(docs_technical)}\")\n",
    "for doc in docs_technical:\n",
    "    filename = os.path.basename(doc.metadata['source'])\n",
    "    print(f\"  - {filename}: {doc.page_content[:50]}...\")\n",
    "\n",
    "# 3. Custom loader with metadata enhancement\n",
    "class CustomTextLoader(TextLoader):\n",
    "    \"\"\"Custom loader that adds extra metadata\"\"\"\n",
    "    \n",
    "    def load(self) -> List[Document]:\n",
    "        docs = super().load()\n",
    "        for doc in docs:\n",
    "            # Add custom metadata\n",
    "            filepath = doc.metadata['source']\n",
    "            filename = os.path.basename(filepath)\n",
    "            \n",
    "            # Determine category from path\n",
    "            if 'technical' in filepath:\n",
    "                category = 'technical'\n",
    "            elif 'business' in filepath:\n",
    "                category = 'business'\n",
    "            else:\n",
    "                category = 'general'\n",
    "            \n",
    "            # Add metadata\n",
    "            doc.metadata['filename'] = filename\n",
    "            doc.metadata['category'] = category\n",
    "            doc.metadata['char_count'] = len(doc.page_content)\n",
    "            doc.metadata['word_count'] = len(doc.page_content.split())\n",
    "        \n",
    "        return docs\n",
    "\n",
    "# Load with custom loader\n",
    "loader_custom = DirectoryLoader(\n",
    "    docs_dir,\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=CustomTextLoader,\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "docs_custom = loader_custom.load()\n",
    "\n",
    "print(f\"\\nüé® Custom Loading with Enhanced Metadata:\")\n",
    "print(f\"  Total documents: {len(docs_custom)}\")\n",
    "for doc in docs_custom[:3]:\n",
    "    print(f\"\\n  Document: {doc.metadata['filename']}\")\n",
    "    print(f\"    Category: {doc.metadata['category']}\")\n",
    "    print(f\"    Words: {doc.metadata['word_count']}\")\n",
    "    print(f\"    Chars: {doc.metadata['char_count']}\")\n",
    "\n",
    "# 4. Filter documents after loading\n",
    "technical_docs = [doc for doc in docs_custom if doc.metadata.get('category') == 'technical']\n",
    "long_docs = [doc for doc in docs_custom if doc.metadata.get('word_count', 0) > 10]\n",
    "\n",
    "print(f\"\\nüîç Filtered Documents:\")\n",
    "print(f\"  Technical docs: {len(technical_docs)}\")\n",
    "print(f\"  Long docs (>10 words): {len(long_docs)}\")\n",
    "\n",
    "# Clean up\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"\\n‚úÖ Directory loading with filtering and custom metadata!\")\n",
    "```\n",
    "\n",
    "**Directory loading advantages:**\n",
    "- Batch process multiple files\n",
    "- Recursive subdirectory search\n",
    "- Glob pattern filtering\n",
    "- Custom metadata enrichment\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 1: Practice Document Loading\n",
    "\n",
    "**Practice Focus**: Load various document types with metadata\n",
    "\n",
    "### Exercise 1: Create a Multi-Format Document Loader\n",
    "\n",
    "**Task**: Build a loader that handles multiple file formats\n",
    "**Expected Output**: Unified document loading regardless of format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a function that:\n",
    "# 1. Detects file type from extension\n",
    "# 2. Uses appropriate loader\n",
    "# 3. Adds consistent metadata\n",
    "# 4. Returns loaded documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def universal_document_loader(file_path: str) -> List[Document]:\n",
    "    \"\"\"Load documents from any supported format\"\"\"\n",
    "    \n",
    "    # Get file extension\n",
    "    _, ext = os.path.splitext(file_path.lower())\n",
    "    \n",
    "    # Select appropriate loader\n",
    "    loaders = {\n",
    "        '.txt': TextLoader,\n",
    "        '.md': TextLoader,\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.csv': CSVLoader,\n",
    "    }\n",
    "    \n",
    "    loader_class = loaders.get(ext)\n",
    "    \n",
    "    if not loader_class:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "    \n",
    "    # Load document\n",
    "    try:\n",
    "        if ext == '.csv':\n",
    "            loader = loader_class(file_path, csv_args={'delimiter': ','})\n",
    "        else:\n",
    "            loader = loader_class(file_path)\n",
    "        \n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Enhance metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata['file_type'] = ext\n",
    "            doc.metadata['file_name'] = os.path.basename(file_path)\n",
    "            doc.metadata['file_size'] = os.path.getsize(file_path)\n",
    "            doc.metadata['char_count'] = len(doc.page_content)\n",
    "            doc.metadata['line_count'] = doc.page_content.count('\\n') + 1\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test with different file types\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Create test files\n",
    "test_files = [\n",
    "    (\"test.txt\", \"This is a text file with some content about AI.\"),\n",
    "    (\"readme.md\", \"# Markdown File\\n\\nThis is a **markdown** document.\"),\n",
    "    (\"data.csv\", \"name,age,city\\nAlice,30,NYC\\nBob,25,LA\")\n",
    "]\n",
    "\n",
    "print(\"Universal Document Loader Test:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_docs = []\n",
    "for filename, content in test_files:\n",
    "    filepath = os.path.join(temp_dir, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    docs = universal_document_loader(filepath)\n",
    "    all_docs.extend(docs)\n",
    "    \n",
    "    print(f\"\\nüìÑ Loaded: {filename}\")\n",
    "    print(f\"  Documents: {len(docs)}\")\n",
    "    for doc in docs:\n",
    "        print(f\"  Type: {doc.metadata['file_type']}\")\n",
    "        print(f\"  Size: {doc.metadata['file_size']} bytes\")\n",
    "        print(f\"  Chars: {doc.metadata['char_count']}\")\n",
    "\n",
    "print(f\"\\nüìä Total documents loaded: {len(all_docs)}\")\n",
    "\n",
    "# Clean up\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"\\n‚úÖ Universal loader handles multiple formats!\")\n",
    "```\n",
    "\n",
    "**What you learned:**\n",
    "- Dynamic loader selection\n",
    "- Consistent metadata across formats\n",
    "- Error handling\n",
    "- File type detection\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 2: Text Splitting Strategies\n",
    "\n",
    "**Concept**: Split documents into chunks optimized for embedding and retrieval.\n",
    "\n",
    "### Example 1: Recursive Character Text Splitter\n",
    "\n",
    "**Problem**: Split text while preserving semantic coherence\n",
    "**Expected Output**: Optimally sized chunks with overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample document\n",
    "long_text = \"\"\"# Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn for themselves.\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "### 1. Supervised Learning\n",
    "Supervised learning is the most common type of machine learning. In this approach, the algorithm learns from labeled training data. Every example in the training dataset includes the input data and the correct output. The algorithm makes predictions based on this training and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.\n",
    "\n",
    "Common algorithms include:\n",
    "- Linear Regression\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Support Vector Machines\n",
    "\n",
    "### 2. Unsupervised Learning\n",
    "Unsupervised learning is used when the training data doesn't include labels. The system tries to learn without a teacher by finding patterns in the data. It's mainly used for clustering and association problems.\n",
    "\n",
    "Common algorithms include:\n",
    "- K-Means Clustering\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN\n",
    "- Principal Component Analysis\n",
    "\n",
    "### 3. Reinforcement Learning\n",
    "Reinforcement learning is about taking suitable actions to maximize reward in a particular situation. It learns from the consequences of its actions, rather than from being taught explicitly. The agent learns to achieve a goal in an uncertain, potentially complex environment.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Machine learning has numerous applications across various industries:\n",
    "\n",
    "1. **Healthcare**: Disease diagnosis, drug discovery, personalized treatment\n",
    "2. **Finance**: Fraud detection, risk assessment, algorithmic trading\n",
    "3. **Retail**: Recommendation systems, demand forecasting, price optimization\n",
    "4. **Transportation**: Autonomous vehicles, route optimization, traffic prediction\n",
    "5. **Entertainment**: Content recommendation, game AI, music generation\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To begin with machine learning, you'll need:\n",
    "- Strong foundation in mathematics (linear algebra, calculus, statistics)\n",
    "- Programming skills (Python is most popular)\n",
    "- Understanding of data preprocessing\n",
    "- Knowledge of various algorithms and their use cases\n",
    "\"\"\"\n",
    "\n",
    "print(\"Text Splitting Strategies:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Basic recursive splitter\n",
    "basic_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Maximum chunk size\n",
    "    chunk_overlap=50,  # Overlap between chunks\n",
    "    length_function=len,  # How to measure chunk size\n",
    ")\n",
    "\n",
    "doc = Document(page_content=long_text, metadata={\"source\": \"ml_guide.md\"})\n",
    "basic_chunks = basic_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"\\nüìù Basic Splitting:\")\n",
    "print(f\"  Original length: {len(long_text)} chars\")\n",
    "print(f\"  Chunks created: {len(basic_chunks)}\")\n",
    "print(f\"  Chunk sizes: {[len(chunk.page_content) for chunk in basic_chunks]}\")\n",
    "\n",
    "# Show first chunk\n",
    "print(f\"\\n  First chunk ({len(basic_chunks[0].page_content)} chars):\")\n",
    "print(f\"  {basic_chunks[0].page_content[:200]}...\")\n",
    "\n",
    "# 2. Semantic-aware splitter with custom separators\n",
    "semantic_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Hierarchical separators\n",
    "    keep_separator=True,  # Keep the separator in the chunk\n",
    ")\n",
    "\n",
    "semantic_chunks = semantic_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"\\nüß† Semantic-Aware Splitting:\")\n",
    "print(f\"  Chunks created: {len(semantic_chunks)}\")\n",
    "print(f\"  Average chunk size: {sum(len(c.page_content) for c in semantic_chunks) / len(semantic_chunks):.0f} chars\")\n",
    "\n",
    "# Analyze chunk boundaries\n",
    "for i, chunk in enumerate(semantic_chunks[:3], 1):\n",
    "    first_line = chunk.page_content.split('\\n')[0][:50]\n",
    "    print(f\"\\n  Chunk {i} starts with: {first_line}...\")\n",
    "\n",
    "# 3. Preserve metadata through splitting\n",
    "metadata_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,  # Add character position to metadata\n",
    ")\n",
    "\n",
    "doc_with_metadata = Document(\n",
    "    page_content=long_text,\n",
    "    metadata={\n",
    "        \"source\": \"ml_guide.md\",\n",
    "        \"author\": \"AI Expert\",\n",
    "        \"category\": \"education\",\n",
    "        \"importance\": \"high\"\n",
    "    }\n",
    ")\n",
    "\n",
    "metadata_chunks = metadata_splitter.split_documents([doc_with_metadata])\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Splitting with Metadata Preservation:\")\n",
    "print(f\"  Chunks created: {len(metadata_chunks)}\")\n",
    "print(f\"\\n  Sample chunk metadata:\")\n",
    "for key, value in metadata_chunks[0].metadata.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# 4. Compare overlap effects\n",
    "print(f\"\\nüîÑ Overlap Analysis:\")\n",
    "\n",
    "no_overlap_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "high_overlap_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "\n",
    "no_overlap_chunks = no_overlap_splitter.split_documents([doc])\n",
    "high_overlap_chunks = high_overlap_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"  No overlap: {len(no_overlap_chunks)} chunks\")\n",
    "print(f\"  High overlap (200): {len(high_overlap_chunks)} chunks\")\n",
    "print(f\"  Overlap increases chunks by {len(high_overlap_chunks) - len(no_overlap_chunks)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Different splitting strategies for different needs!\")\n",
    "```\n",
    "\n",
    "**Splitting strategy benefits:**\n",
    "- Maintains semantic coherence\n",
    "- Overlap preserves context\n",
    "- Hierarchical separators respect structure\n",
    "- Metadata preserved through splits\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Specialized Splitters\n",
    "\n",
    "**Problem**: Use format-specific splitters for better results\n",
    "**Expected Output**: Optimized splitting for different content types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import (\n",
    "    MarkdownTextSplitter,\n",
    "    PythonCodeTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    CharacterTextSplitter\n",
    ")\n",
    "\n",
    "print(\"Specialized Text Splitters:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Markdown splitter\n",
    "markdown_text = \"\"\"# Main Title\n",
    "\n",
    "This is the introduction paragraph with some important information.\n",
    "\n",
    "## Section 1: Getting Started\n",
    "\n",
    "Here's how to begin with the basics. This section covers fundamental concepts.\n",
    "\n",
    "### Subsection 1.1: Installation\n",
    "\n",
    "Install using pip:\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "### Subsection 1.2: Configuration\n",
    "\n",
    "Configure your environment variables:\n",
    "```python\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'your-key'\n",
    "```\n",
    "\n",
    "## Section 2: Advanced Topics\n",
    "\n",
    "This section covers more complex topics for experienced users.\n",
    "\n",
    "### Subsection 2.1: Custom Components\n",
    "\n",
    "You can create custom components by extending base classes.\n",
    "\"\"\"\n",
    "\n",
    "md_splitter = MarkdownTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "\n",
    "md_chunks = md_splitter.split_text(markdown_text)\n",
    "\n",
    "print(\"\\nüìù Markdown Splitting:\")\n",
    "print(f\"  Chunks created: {len(md_chunks)}\")\n",
    "for i, chunk in enumerate(md_chunks[:3], 1):\n",
    "    print(f\"\\n  Chunk {i}:\")\n",
    "    preview = chunk[:100].replace('\\n', ' ')\n",
    "    print(f\"    {preview}...\")\n",
    "\n",
    "# 2. Python code splitter\n",
    "python_code = '''\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Process documents for embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=500):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.splitter = self._create_splitter()\n",
    "    \n",
    "    def _create_splitter(self):\n",
    "        \"\"\"Create text splitter\"\"\"\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "    \n",
    "    def process_document(self, document):\n",
    "        \"\"\"Process a single document\"\"\"\n",
    "        chunks = self.splitter.split_documents([document])\n",
    "        return self._add_metadata(chunks)\n",
    "    \n",
    "    def _add_metadata(self, chunks):\n",
    "        \"\"\"Add metadata to chunks\"\"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata['chunk_index'] = i\n",
    "            chunk.metadata['total_chunks'] = len(chunks)\n",
    "        return chunks\n",
    "\n",
    "def main():\n",
    "    processor = DocumentProcessor(chunk_size=300)\n",
    "    # Process documents\n",
    "    pass\n",
    "'''\n",
    "\n",
    "python_splitter = PythonCodeTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "code_chunks = python_splitter.split_text(python_code)\n",
    "\n",
    "print(\"\\nüêç Python Code Splitting:\")\n",
    "print(f\"  Chunks created: {len(code_chunks)}\")\n",
    "print(\"  Respects code structure (classes, functions)\")\n",
    "\n",
    "# 3. Token-based splitter (for precise token counts)\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Token splitter for precise model input control\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,  # Number of tokens\n",
    "    chunk_overlap=10,\n",
    "    encoding_name=\"cl100k_base\"  # GPT-4 encoding\n",
    ")\n",
    "\n",
    "sample_text = \"\"\"Artificial intelligence is transforming industries worldwide. \n",
    "From healthcare to finance, AI applications are becoming increasingly sophisticated. \n",
    "Machine learning models can now perform tasks that were once thought to be \n",
    "exclusively human domain.\"\"\"\n",
    "\n",
    "token_chunks = token_splitter.split_text(sample_text)\n",
    "\n",
    "print(\"\\nüéØ Token-Based Splitting:\")\n",
    "print(f\"  Chunks created: {len(token_chunks)}\")\n",
    "print(f\"  Precise token count per chunk: ~100 tokens\")\n",
    "print(f\"  Useful for models with token limits\")\n",
    "\n",
    "# 4. Compare different splitters on same text\n",
    "comparison_text = \"\"\"LangChain is a framework for developing applications powered by language models. \n",
    "It enables applications that are context-aware and reason. LangChain provides modular components \n",
    "and off-the-shelf chains for working with language models. It's designed to be easy to use, \n",
    "flexible, and powerful.\"\"\"\n",
    "\n",
    "splitters = {\n",
    "    \"Character\": CharacterTextSplitter(chunk_size=100, chunk_overlap=20, separator=\" \"),\n",
    "    \"Recursive\": RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20),\n",
    "    \"Token\": TokenTextSplitter(chunk_size=30, chunk_overlap=5)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Splitter Comparison:\")\n",
    "for name, splitter in splitters.items():\n",
    "    chunks = splitter.split_text(comparison_text)\n",
    "    print(f\"  {name}: {len(chunks)} chunks\")\n",
    "\n",
    "print(\"\\n‚úÖ Choose splitters based on content type and use case!\")\n",
    "```\n",
    "\n",
    "**Specialized splitter advantages:**\n",
    "- Markdown: Respects document structure\n",
    "- Code: Preserves function/class boundaries\n",
    "- Token: Precise model input control\n",
    "- Each optimized for its content type\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 2: Practice Text Splitting\n",
    "\n",
    "**Practice Focus**: Implement optimal splitting strategies\n",
    "\n",
    "### Exercise 1: Build an Adaptive Splitter\n",
    "\n",
    "**Task**: Create a splitter that adapts chunk size based on content\n",
    "**Expected Output**: Dynamic chunk sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a function that:\n",
    "# 1. Analyzes document structure\n",
    "# 2. Chooses appropriate chunk size\n",
    "# 3. Splits with optimal overlap\n",
    "# 4. Validates chunk quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "def adaptive_document_splitter(document: Document, \n",
    "                              min_chunk_size: int = 200,\n",
    "                              max_chunk_size: int = 1000) -> List[Document]:\n",
    "    \"\"\"Adaptively split document based on content analysis\"\"\"\n",
    "    \n",
    "    content = document.page_content\n",
    "    \n",
    "    # Analyze document characteristics\n",
    "    avg_sentence_length = len(content) / (content.count('.') + 1)\n",
    "    avg_paragraph_length = len(content) / (content.count('\\n\\n') + 1)\n",
    "    has_code = '```' in content or 'def ' in content or 'class ' in content\n",
    "    has_lists = any(marker in content for marker in ['- ', '* ', '1. '])\n",
    "    \n",
    "    # Determine optimal chunk size\n",
    "    if has_code:\n",
    "        # Larger chunks for code to keep functions together\n",
    "        chunk_size = min(max_chunk_size, int(avg_paragraph_length * 1.5))\n",
    "        overlap = 50\n",
    "    elif avg_sentence_length > 100:\n",
    "        # Larger chunks for complex text\n",
    "        chunk_size = min(max_chunk_size, 800)\n",
    "        overlap = 100\n",
    "    elif has_lists:\n",
    "        # Medium chunks to keep lists together\n",
    "        chunk_size = min(max_chunk_size, 500)\n",
    "        overlap = 50\n",
    "    else:\n",
    "        # Standard chunks for regular text\n",
    "        chunk_size = min(max_chunk_size, 400)\n",
    "        overlap = 40\n",
    "    \n",
    "    # Ensure chunk_size is reasonable\n",
    "    chunk_size = max(min_chunk_size, chunk_size)\n",
    "    \n",
    "    # Create splitter with adaptive parameters\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "        keep_separator=True\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = splitter.split_documents([document])\n",
    "    \n",
    "    # Add splitting metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata.update({\n",
    "            'chunk_index': i,\n",
    "            'total_chunks': len(chunks),\n",
    "            'chunk_size_used': chunk_size,\n",
    "            'overlap_used': overlap,\n",
    "            'adaptive_reason': 'code' if has_code else 'complex' if avg_sentence_length > 100 else 'standard'\n",
    "        })\n",
    "    \n",
    "    # Validate chunks\n",
    "    validated_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # Skip very small chunks (likely artifacts)\n",
    "        if len(chunk.page_content.strip()) < 50:\n",
    "            continue\n",
    "        # Merge with previous if too small\n",
    "        if validated_chunks and len(chunk.page_content) < min_chunk_size / 2:\n",
    "            validated_chunks[-1].page_content += \"\\n\" + chunk.page_content\n",
    "        else:\n",
    "            validated_chunks.append(chunk)\n",
    "    \n",
    "    return validated_chunks\n",
    "\n",
    "# Test with different document types\n",
    "test_docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"This is a simple document. It has short sentences. \n",
    "        The content is straightforward. Nothing complex here.\"\"\",\n",
    "        metadata={\"type\": \"simple\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"def process_data(data):\n",
    "    # This is a code example\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if item > 0:\n",
    "            result.append(item * 2)\n",
    "    return result\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.data = []\"\"\",\n",
    "        metadata={\"type\": \"code\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"The implications of quantum computing on cryptography are profound and multifaceted, requiring careful consideration of both the technological capabilities and the timeline for practical implementation. Current encryption methods, particularly those based on factoring large prime numbers, will become vulnerable to quantum algorithms like Shor's algorithm, necessitating a transition to quantum-resistant cryptographic methods.\"\"\",\n",
    "        metadata={\"type\": \"complex\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Adaptive Document Splitting:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for doc in test_docs:\n",
    "    chunks = adaptive_document_splitter(doc)\n",
    "    print(f\"\\nüìÑ Document type: {doc.metadata['type']}\")\n",
    "    print(f\"  Chunks created: {len(chunks)}\")\n",
    "    if chunks:\n",
    "        print(f\"  Chunk size used: {chunks[0].metadata.get('chunk_size_used')}\")\n",
    "        print(f\"  Adaptive reason: {chunks[0].metadata.get('adaptive_reason')}\")\n",
    "        print(f\"  First chunk length: {len(chunks[0].page_content)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Adaptive splitter adjusts to content characteristics!\")\n",
    "```\n",
    "\n",
    "**What you learned:**\n",
    "- Content analysis for parameters\n",
    "- Dynamic chunk sizing\n",
    "- Chunk validation\n",
    "- Metadata enrichment\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 3: Document Processing Pipeline\n",
    "\n",
    "**Concept**: Build complete pipelines from loading to preprocessing.\n",
    "\n",
    "### Example 1: End-to-End Processing Pipeline\n",
    "\n",
    "**Problem**: Create a complete document processing workflow\n",
    "**Expected Output**: Ready-for-embedding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "class DocumentProcessingPipeline:\n",
    "    \"\"\"Complete pipeline for document processing\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 500,\n",
    "                 chunk_overlap: int = 50,\n",
    "                 min_chunk_size: int = 100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.processed_count = 0\n",
    "    \n",
    "    def process(self, \n",
    "                documents: List[Document],\n",
    "                clean_text: bool = True,\n",
    "                add_summary: bool = False) -> List[Document]:\n",
    "        \"\"\"Process documents through the complete pipeline\"\"\"\n",
    "        \n",
    "        print(f\"üîÑ Processing {len(documents)} documents...\")\n",
    "        \n",
    "        # Step 1: Clean and normalize\n",
    "        if clean_text:\n",
    "            documents = self._clean_documents(documents)\n",
    "            print(f\"  ‚úÖ Cleaned text\")\n",
    "        \n",
    "        # Step 2: Split documents\n",
    "        chunks = self._split_documents(documents)\n",
    "        print(f\"  ‚úÖ Split into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Step 3: Enrich metadata\n",
    "        chunks = self._enrich_metadata(chunks)\n",
    "        print(f\"  ‚úÖ Enriched metadata\")\n",
    "        \n",
    "        # Step 4: Filter chunks\n",
    "        chunks = self._filter_chunks(chunks)\n",
    "        print(f\"  ‚úÖ Filtered to {len(chunks)} quality chunks\")\n",
    "        \n",
    "        # Step 5: Add summaries (optional)\n",
    "        if add_summary:\n",
    "            chunks = self._add_summaries(chunks)\n",
    "            print(f\"  ‚úÖ Added summaries\")\n",
    "        \n",
    "        self.processed_count += len(chunks)\n",
    "        return chunks\n",
    "    \n",
    "    def _clean_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Clean and normalize document text\"\"\"\n",
    "        cleaned = []\n",
    "        for doc in documents:\n",
    "            # Remove excessive whitespace\n",
    "            text = re.sub(r'\\s+', ' ', doc.page_content)\n",
    "            # Remove special characters but keep punctuation\n",
    "            text = re.sub(r'[^\\w\\s\\.!?,;:\\'\\\"\\-\\(\\)\\[\\]{}]', '', text)\n",
    "            # Normalize quotes\n",
    "            text = text.replace('"', '\"').replace('"', '\"')\n",
    "            # Trim\n",
    "            text = text.strip()\n",
    "            \n",
    "            cleaned_doc = Document(\n",
    "                page_content=text,\n",
    "                metadata={**doc.metadata, 'cleaned': True}\n",
    "            )\n",
    "            cleaned.append(cleaned_doc)\n",
    "        return cleaned\n",
    "    \n",
    "    def _split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks\"\"\"\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            add_start_index=True\n",
    "        )\n",
    "        \n",
    "        return splitter.split_documents(documents)\n",
    "    \n",
    "    def _enrich_metadata(self, chunks: List[Document]) -> List[Document]:\n",
    "        \"\"\"Add additional metadata to chunks\"\"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Add processing metadata\n",
    "            chunk.metadata.update({\n",
    "                'chunk_id': f\"chunk_{self.processed_count}_{i}\",\n",
    "                'processed_at': datetime.now().isoformat(),\n",
    "                'char_count': len(chunk.page_content),\n",
    "                'word_count': len(chunk.page_content.split()),\n",
    "                'has_numbers': bool(re.search(r'\\d', chunk.page_content)),\n",
    "                'has_urls': bool(re.search(r'https?://', chunk.page_content)),\n",
    "                'sentence_count': chunk.page_content.count('.') + \n",
    "                                 chunk.page_content.count('!') + \n",
    "                                 chunk.page_content.count('?')\n",
    "            })\n",
    "        return chunks\n",
    "    \n",
    "    def _filter_chunks(self, chunks: List[Document]) -> List[Document]:\n",
    "        \"\"\"Filter out low-quality chunks\"\"\"\n",
    "        filtered = []\n",
    "        for chunk in chunks:\n",
    "            # Skip too small chunks\n",
    "            if chunk.metadata['char_count'] < self.min_chunk_size:\n",
    "                continue\n",
    "            # Skip chunks that are mostly numbers\n",
    "            if chunk.metadata['has_numbers']:\n",
    "                num_count = len(re.findall(r'\\d', chunk.page_content))\n",
    "                if num_count > len(chunk.page_content) * 0.5:\n",
    "                    continue\n",
    "            filtered.append(chunk)\n",
    "        return filtered\n",
    "    \n",
    "    def _add_summaries(self, chunks: List[Document]) -> List[Document]:\n",
    "        \"\"\"Add brief summaries to chunks (mock implementation)\"\"\"\n",
    "        for chunk in chunks:\n",
    "            # In real implementation, use LLM to generate summary\n",
    "            first_sentence = chunk.page_content.split('.')[0] + '.'\n",
    "            chunk.metadata['summary'] = first_sentence[:100]\n",
    "        return chunks\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get processing statistics\"\"\"\n",
    "        return {\n",
    "            'total_processed': self.processed_count,\n",
    "            'chunk_size': self.chunk_size,\n",
    "            'chunk_overlap': self.chunk_overlap\n",
    "        }\n",
    "\n",
    "# Test the pipeline\n",
    "pipeline = DocumentProcessingPipeline(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    min_chunk_size=50\n",
    ")\n",
    "\n",
    "# Create test documents\n",
    "test_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"   This is a TEST document!!!   It has some \n",
    "        weird    spacing and special chars @#$%.  We'll clean it up.\n",
    "        \n",
    "        It also has multiple paragraphs with useful information about \n",
    "        artificial intelligence and machine learning applications.\"\"\",\n",
    "        metadata={'source': 'test1.txt', 'type': 'article'}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Another document with https://example.com links and \n",
    "        numbers like 12345. This helps test our metadata extraction.\n",
    "        \n",
    "        We can identify different types of content automatically.\"\"\",\n",
    "        metadata={'source': 'test2.txt', 'type': 'web'}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Process documents\n",
    "processed_chunks = pipeline.process(\n",
    "    test_documents,\n",
    "    clean_text=True,\n",
    "    add_summary=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Pipeline Results:\")\n",
    "print(f\"  Input documents: {len(test_documents)}\")\n",
    "print(f\"  Output chunks: {len(processed_chunks)}\")\n",
    "\n",
    "# Show sample processed chunk\n",
    "if processed_chunks:\n",
    "    sample = processed_chunks[0]\n",
    "    print(f\"\\nüìÑ Sample Processed Chunk:\")\n",
    "    print(f\"  Content: {sample.page_content[:100]}...\")\n",
    "    print(f\"  Metadata:\")\n",
    "    for key, value in sample.metadata.items():\n",
    "        if key != 'page_content':\n",
    "            print(f\"    - {key}: {value}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = pipeline.get_statistics()\n",
    "print(f\"\\nüìà Processing Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Complete pipeline processes documents end-to-end!\")\n",
    "```\n",
    "\n",
    "**Pipeline benefits:**\n",
    "- Automated multi-step processing\n",
    "- Quality control at each stage\n",
    "- Rich metadata for retrieval\n",
    "- Scalable and reusable\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "‚úÖ Loading documents from various sources (text, PDF, CSV, JSON, web)  \n",
    "‚úÖ Directory loading with filtering and batch processing  \n",
    "‚úÖ Text splitting strategies (recursive, semantic, token-based)  \n",
    "‚úÖ Specialized splitters for different content types  \n",
    "‚úÖ Metadata preservation and enrichment  \n",
    "‚úÖ Complete document processing pipelines  \n",
    "\n",
    "### Key Takeaways\n",
    "1. **Choose the right loader** - Different formats need different loaders\n",
    "2. **Split thoughtfully** - Chunk size affects retrieval quality\n",
    "3. **Preserve metadata** - Essential for filtering and context\n",
    "4. **Clean and validate** - Quality in, quality out\n",
    "5. **Build pipelines** - Automate repetitive processing\n",
    "\n",
    "### What's Next?\n",
    "In the next notebook (`06_embeddings_and_vectors.ipynb`), you'll learn:\n",
    "- Generating embeddings from text\n",
    "- Understanding vector representations\n",
    "- Working with vector stores\n",
    "- Similarity search techniques\n",
    "- Optimizing embedding strategies\n",
    "\n",
    "### Resources\n",
    "- [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "- [Text Splitters Guide](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- [Document Processing Best Practices](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You've mastered document loading and processing! You're ready to build the foundation for RAG systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}