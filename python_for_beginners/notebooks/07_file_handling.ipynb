{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **File Handling in Python**\n\n## **Learning Objectives**\nBy the end of this section, you will be able to:\n- Read and write text files using Python's built-in file operations\n- Use context managers (with statement) for safe file handling\n- Construct cross-platform file paths using os.path.join\n- Handle file-related errors gracefully\n- Apply file operations to real-world AI/RAG/Agentic AI workflows\n\n## **Why This Matters: Real-World AI/RAG/Agentic Applications**\n\n**In AI Systems:**\n- Load training data and datasets from disk\n- Save model configurations, checkpoints, and predictions\n- Process batch files for inference pipelines\n- Log training metrics and experimental results\n\n**In RAG Pipelines:**\n- Load documents from various sources (PDFs, text files, markdown)\n- Cache embeddings and vector representations to disk\n- Save retrieved context and conversation history\n- Build document indexes from file directories\n- Store preprocessed chunks for faster retrieval\n\n**In Agentic AI:**\n- Read tool configurations and API credentials\n- Save agent conversation logs and decision traces\n- Load prompt templates from files\n- Store action results and intermediate outputs\n- Manage file-based state persistence across agent runs\n\n## **Prerequisites**\n- Basic Python syntax (variables, strings)\n- Understanding of functions\n- Basic knowledge of error handling (try/except)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Instructor Activity 1**\n**Concept**: Basic file writing and reading operations\n\n### **Example 1: Writing to a File**\n\n**Problem**: Create a text file and write a simple message to it\n\n**Expected Output**: A file named `hello.txt` containing \"Hello, World!\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Open file in write mode ('w')**\n# **'w' mode creates a new file or overwrites existing content**\nfile = open('hello.txt', 'w')\n\n# **Write content to the file**\nfile.write('Hello, World!')\n\n# **IMPORTANT: Always close the file to save changes**\nfile.close()\n\nprint(\"File 'hello.txt' created successfully!\")\n```\n\n**Why this works:**\n- `open(filename, mode)` opens a file connection\n- Mode `'w'` means \"write\" - creates file if it doesn't exist, overwrites if it does\n- `.write()` method writes string content to the file\n- `.close()` is crucial - it saves changes and frees system resources\n- Without closing, data might not be written to disk!\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Reading from a File**\n\n**Problem**: Read and display the content of the file we just created\n\n**Expected Output**: `\"Hello, World!\"`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Open file in read mode ('r')**\nfile = open('hello.txt', 'r')\n\n# **Read the entire file content**\ncontent = file.read()\n\n# **Close the file**\nfile.close()\n\nprint(\"File content:\", content)\n# **Output: File content: Hello, World!**\n```\n\n**Why this works:**\n- Mode `'r'` means \"read\" - opens file for reading only\n- `.read()` reads the entire file content as a string\n- Always close files after reading to free resources\n- If file doesn't exist, you'll get a `FileNotFoundError`\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Appending to a File**\n\n**Problem**: Add a new line to the existing file without overwriting\n\n**Expected Output**: File contains both original content and new line"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Open file in append mode ('a')**\nfile = open('hello.txt', 'a')\n\n# **Add a new line (\\n for newline character)**\nfile.write('\\nThis is a new line!')\n\nfile.close()\n\n# **Read and display the updated content**\nfile = open('hello.txt', 'r')\ncontent = file.read()\nfile.close()\n\nprint(\"Updated content:\")\nprint(content)\n# **Output:**\n# **Hello, World!**\n# **This is a new line!**\n```\n\n**Why this works:**\n- Mode `'a'` means \"append\" - adds content to the end without deleting existing data\n- `'\\n'` is the newline character - starts content on a new line\n- Unlike `'w'` mode, `'a'` mode preserves existing content\n- Perfect for logging or adding entries to files\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 1**\n**Practice**: Basic file writing and reading operations\n\n### **Exercise 1: Create a Shopping List**\n\n**Task**: Write a list of items to a file named `shopping_list.txt`\n\n**Given**: `items = ['milk', 'bread', 'eggs', 'cheese']`\n\n**Expected Output**: File containing each item on a separate line"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "items = ['milk', 'bread', 'eggs', 'cheese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nitems = ['milk', 'bread', 'eggs', 'cheese']\n\n# **Open file for writing**\nfile = open('shopping_list.txt', 'w')\n\n# **Write each item on a new line**\nfor item in items:\n    file.write(item + '\\n')  # Add newline after each item\n\nfile.close()\n\nprint(\"Shopping list saved to shopping_list.txt\")\n```\n\n**Why this works:**\n- Loop through the list to write each item\n- Add `'\\n'` after each item to place them on separate lines\n- This creates a readable, line-by-line format\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Read and Count Lines**\n\n**Task**: Read the shopping list file and count how many items it contains\n\n**Expected Output**: `\"Total items: 4\"`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Open file for reading**\nfile = open('shopping_list.txt', 'r')\n\n# **Read all lines into a list**\nlines = file.readlines()  # readlines() returns a list of all lines\n\nfile.close()\n\n# **Count non-empty lines**\ncount = len([line for line in lines if line.strip()])  # strip() removes whitespace\n\nprint(f\"Total items: {count}\")\n# **Output: Total items: 4**\n```\n\n**Why this works:**\n- `.readlines()` returns a list where each element is a line from the file\n- `.strip()` removes leading/trailing whitespace (including `'\\n'`)\n- List comprehension filters out any empty lines\n- `len()` gives us the count of items\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 3: Add Item to Shopping List**\n\n**Task**: Append \"butter\" to the existing shopping list\n\n**Expected Output**: File now contains 5 items total"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Open file in append mode**\nfile = open('shopping_list.txt', 'a')\n\n# **Add new item**\nfile.write('butter\\n')\n\nfile.close()\n\n# **Verify by reading the file**\nfile = open('shopping_list.txt', 'r')\ncontent = file.read()\nfile.close()\n\nprint(\"Updated shopping list:\")\nprint(content)\n```\n\n**Why this works:**\n- Append mode `'a'` adds content without removing existing items\n- Including `'\\n'` ensures proper line formatting\n- Reading afterward verifies the append operation worked\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 2**\n**Concept**: Context managers for safe file handling (the `with` statement)\n\n### **Example 1: Using `with` for Reading**\n\n**Problem**: Read a file safely without worrying about closing it\n\n**Expected Output**: File content displayed, file automatically closed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Using 'with' statement - file closes automatically!**\nwith open('hello.txt', 'r') as file:\n    content = file.read()\n    print(\"Content:\", content)\n\n# **File is automatically closed here, even if an error occurs**\nprint(\"File closed:\", file.closed)  # Should print True\n```\n\n**Why this works:**\n- The `with` statement is a context manager - handles resource cleanup automatically\n- No need to call `.close()` explicitly\n- File closes automatically when the `with` block ends\n- Even if an error occurs inside the block, file still closes properly\n- This is the **Pythonic way** and **best practice** for file handling\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Using `with` for Writing**\n\n**Problem**: Write data to a file safely\n\n**Expected Output**: Data written and file automatically closed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Write data using context manager**\ndata = [\"Line 1\", \"Line 2\", \"Line 3\"]\n\nwith open('output.txt', 'w') as file:\n    for line in data:\n        file.write(line + '\\n')\n\nprint(\"File written and closed automatically\")\n\n# **Verify content**\nwith open('output.txt', 'r') as file:\n    print(file.read())\n```\n\n**Why this works:**\n- `with` ensures file is properly closed and changes are saved\n- Prevents data loss from forgetting to close files\n- More concise and cleaner code\n- Industry standard for file operations\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Reading Line by Line (Memory Efficient)**\n\n**Problem**: Read a large file without loading everything into memory\n\n**Expected Output**: Process each line individually"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Create a sample file first**\nwith open('numbers.txt', 'w') as file:\n    for i in range(1, 11):\n        file.write(f\"Line {i}\\n\")\n\n# **Read and process line by line - memory efficient!**\nline_count = 0\nwith open('numbers.txt', 'r') as file:\n    for line in file:  # Iterates one line at a time\n        print(line.strip())  # strip() removes the newline character\n        line_count += 1\n\nprint(f\"\\nProcessed {line_count} lines\")\n```\n\n**Why this works:**\n- Iterating directly over the file object reads one line at a time\n- Very memory efficient - only one line in memory at once\n- Perfect for large files (GB+ size)\n- Essential for processing large datasets in AI/ML workflows\n- `.strip()` removes trailing newline characters\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 2**\n**Practice**: Context managers for safe file handling\n\n### **Exercise 1: Log Messages with Context Manager**\n\n**Task**: Create a log file that records three timestamped messages using `with` statement\n\n**Expected Output**: File `app.log` with three log entries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Use 'with' to create a log file with these messages:\n",
    "messages = [\n",
    "    \"Application started\",\n",
    "    \"User logged in\",\n",
    "    \"Data processed successfully\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nmessages = [\n    \"Application started\",\n    \"User logged in\",\n    \"Data processed successfully\"\n]\n\n# **Write log entries using context manager**\nwith open('app.log', 'w') as file:\n    for i, message in enumerate(messages, 1):\n        file.write(f\"[Log {i}] {message}\\n\")\n\nprint(\"Log file created\")\n\n# **Read and display the log**\nwith open('app.log', 'r') as file:\n    print(\"\\nLog contents:\")\n    print(file.read())\n```\n\n**Why this works:**\n- `enumerate(messages, 1)` provides both index and value, starting from 1\n- F-string formats each log entry with a number\n- `with` ensures file is properly closed after writing\n- Second `with` block safely reads back the log\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Process File Line by Line**\n\n**Task**: Read `numbers.txt` and count how many lines contain the word \"Line\"\n\n**Expected Output**: Count of matching lines"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# First create numbers.txt with some content, then count lines containing \"Line\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Create sample file**\nwith open('numbers.txt', 'w') as file:\n    file.write(\"Line 1\\n\")\n    file.write(\"Line 2\\n\")\n    file.write(\"Something else\\n\")\n    file.write(\"Line 3\\n\")\n    file.write(\"Another thing\\n\")\n\n# **Count lines containing \"Line\"**\ncount = 0\nwith open('numbers.txt', 'r') as file:\n    for line in file:\n        if \"Line\" in line:\n            count += 1\n\nprint(f\"Lines containing 'Line': {count}\")\n# **Output: Lines containing 'Line': 3**\n```\n\n**Why this works:**\n- Iterate line by line for memory efficiency\n- Use `in` operator to check if substring exists\n- Counter increments for each match\n- File automatically closes when done\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 3: Multiple Operations with Context Manager**\n\n**Task**: Read from one file, transform the content (uppercase), and write to another file\n\n**Expected Output**: New file with uppercase content"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Read from 'hello.txt', convert to uppercase, write to 'hello_upper.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Read from source file**\nwith open('hello.txt', 'r') as source:\n    content = source.read()\n\n# **Transform content to uppercase**\nuppercase_content = content.upper()\n\n# **Write to destination file**\nwith open('hello_upper.txt', 'w') as dest:\n    dest.write(uppercase_content)\n\nprint(\"Content transformed and saved\")\n\n# **Verify**\nwith open('hello_upper.txt', 'r') as file:\n    print(\"New content:\", file.read())\n```\n\n**Why this works:**\n- Separate `with` blocks for reading and writing\n- Each file operation is safely handled\n- Transform happens between read and write\n- Pattern useful for data processing pipelines\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 3**\n**Concept**: Safe path handling with `os.path.join` for cross-platform compatibility\n\n### **Example 1: Understanding Path Issues**\n\n**Problem**: Demonstrate why hardcoded paths can cause problems\n\n**Expected Output**: Understanding of platform-specific path separators"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\n# **BAD: Hardcoded Windows-style path**\n# **This will FAIL on Mac/Linux!**\nwindows_path = \"data\\\\files\\\\input.txt\"  # Uses backslashes\n\n# **BAD: Hardcoded Unix-style path**\n# **This might work but isn't portable**\nunix_path = \"data/files/input.txt\"  # Uses forward slashes\n\n# **GOOD: Using os.path.join - works everywhere!**\ncross_platform_path = os.path.join(\"data\", \"files\", \"input.txt\")\n\nprint(\"Windows path:\", windows_path)\nprint(\"Unix path:\", unix_path)\nprint(\"Cross-platform path:\", cross_platform_path)\nprint(\"\\nYour system uses:\", os.sep)  # Shows your system's separator\n```\n\n**Why this works:**\n- Windows uses backslash `\\` as path separator\n- Mac/Linux use forward slash `/`\n- `os.path.join()` automatically uses the correct separator for your OS\n- Makes your code portable across all platforms\n- Essential for production code that runs on different systems\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Creating Nested Directory Paths**\n\n**Problem**: Build a path to a file in nested directories\n\n**Expected Output**: Properly formatted path for your operating system"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\n# **Build path components**\nproject_dir = \"my_project\"\ndata_dir = \"data\"\nsubdirectory = \"raw\"\nfilename = \"dataset.txt\"\n\n# **Combine into complete path**\nfile_path = os.path.join(project_dir, data_dir, subdirectory, filename)\n\nprint(\"Complete path:\", file_path)\n# **On Windows: my_project\\data\\raw\\dataset.txt**\n# **On Mac/Linux: my_project/data/raw/dataset.txt**\n\n# **Create the directory structure**\ndirectory = os.path.join(project_dir, data_dir, subdirectory)\nos.makedirs(directory, exist_ok=True)  # exist_ok=True prevents error if exists\n\n# **Now write to that path**\nwith open(file_path, 'w') as file:\n    file.write(\"Sample data for ML training\")\n\nprint(f\"File created at: {file_path}\")\n```\n\n**Why this works:**\n- `os.path.join()` accepts multiple path components\n- `os.makedirs()` creates all necessary parent directories\n- `exist_ok=True` prevents errors if directory already exists\n- Pattern is essential for organizing project files and datasets\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Working with Absolute and Relative Paths**\n\n**Problem**: Convert between relative and absolute paths safely\n\n**Expected Output**: Full absolute paths to files"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\n# **Get current working directory**\ncurrent_dir = os.getcwd()\nprint(\"Current directory:\", current_dir)\n\n# **Create a relative path**\nrelative_path = os.path.join(\"data\", \"output.txt\")\nprint(\"\\nRelative path:\", relative_path)\n\n# **Convert to absolute path**\nabsolute_path = os.path.abspath(relative_path)\nprint(\"Absolute path:\", absolute_path)\n\n# **Check if path exists**\nprint(\"\\nPath exists?\", os.path.exists(absolute_path))\n\n# **Get directory name from a path**\ndirectory = os.path.dirname(absolute_path)\nprint(\"Directory:\", directory)\n\n# **Get just the filename**\nfilename = os.path.basename(absolute_path)\nprint(\"Filename:\", filename)\n```\n\n**Why this works:**\n- `os.getcwd()` returns current working directory\n- `os.path.abspath()` converts relative paths to absolute\n- `os.path.exists()` checks if a path exists\n- `os.path.dirname()` extracts directory from full path\n- `os.path.basename()` extracts filename from full path\n- These utilities are crucial for robust file handling\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 3**\n**Practice**: Safe path handling with os.path.join\n\n### **Exercise 1: Create Project Structure**\n\n**Task**: Create a directory structure and save a configuration file in it\n\n**Structure**:\n```\nml_project/\n  config/\n    settings.txt\n```\n\n**Expected Output**: File created at correct nested location"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\n# **Define path components**\nproject = \"ml_project\"\nconfig_dir = \"config\"\nconfig_file = \"settings.txt\"\n\n# **Build directory path**\ndir_path = os.path.join(project, config_dir)\n\n# **Create directory structure**\nos.makedirs(dir_path, exist_ok=True)\n\n# **Build complete file path**\nfile_path = os.path.join(dir_path, config_file)\n\n# **Write configuration**\nwith open(file_path, 'w') as file:\n    file.write(\"learning_rate=0.001\\n\")\n    file.write(\"batch_size=32\\n\")\n    file.write(\"epochs=100\\n\")\n\nprint(f\"Configuration saved to: {file_path}\")\nprint(f\"Absolute path: {os.path.abspath(file_path)}\")\n```\n\n**Why this works:**\n- `os.path.join()` builds paths safely\n- `os.makedirs()` creates nested directories\n- `exist_ok=True` prevents errors on re-runs\n- Pattern mirrors real ML project organization\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: List Files in Directory**\n\n**Task**: Create several files in a directory, then list all `.txt` files\n\n**Expected Output**: List of text files in the directory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\n# **Create a documents directory**\ndocs_dir = \"documents\"\nos.makedirs(docs_dir, exist_ok=True)\n\n# **Create several files**\nfiles_to_create = [\"note1.txt\", \"note2.txt\", \"data.csv\", \"readme.txt\"]\n\nfor filename in files_to_create:\n    file_path = os.path.join(docs_dir, filename)\n    with open(file_path, 'w') as file:\n        file.write(f\"Content of {filename}\")\n\nprint(\"Files created successfully\\n\")\n\n# **List all .txt files**\nprint(\"Text files in directory:\")\nfor filename in os.listdir(docs_dir):\n    if filename.endswith('.txt'):\n        full_path = os.path.join(docs_dir, filename)\n        print(f\"  - {filename} (size: {os.path.getsize(full_path)} bytes)\")\n```\n\n**Why this works:**\n- `os.listdir()` returns all files/folders in a directory\n- `.endswith('.txt')` filters for text files only\n- `os.path.getsize()` returns file size in bytes\n- Always use `os.path.join()` when working with directory listings\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 3: Safe File Path Checking**\n\n**Task**: Write a function that safely checks if a file exists before reading it\n\n**Expected Output**: Function that handles missing files gracefully"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os\n",
    "\n",
    "def safe_read_file(directory, filename):\n",
    "    # Build path and check existence, then read\n",
    "    pass\n",
    "\n",
    "# Test with existing and non-existing files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\ndef safe_read_file(directory, filename):\n    \"\"\"Safely read a file with proper path handling and error checking.\"\"\"\n    \n    # Build the complete path\n    file_path = os.path.join(directory, filename)\n    \n    # Check if file exists\n    if not os.path.exists(file_path):\n        return f\"Error: File '{file_path}' does not exist\"\n    \n    # Check if it's actually a file (not a directory)\n    if not os.path.isfile(file_path):\n        return f\"Error: '{file_path}' is not a file\"\n    \n    # Read and return content\n    try:\n        with open(file_path, 'r') as file:\n            return file.read()\n    except Exception as e:\n        return f\"Error reading file: {str(e)}\"\n\n# **Test with existing file**\nresult1 = safe_read_file(\"documents\", \"note1.txt\")\nprint(\"Reading existing file:\")\nprint(result1)\n\n# **Test with non-existing file**\nresult2 = safe_read_file(\"documents\", \"missing.txt\")\nprint(\"\\nReading non-existing file:\")\nprint(result2)\n```\n\n**Why this works:**\n- `os.path.exists()` checks if path exists\n- `os.path.isfile()` verifies it's a file, not a directory\n- Try/except catches any unexpected errors\n- Returns meaningful error messages\n- This defensive programming prevents crashes in production\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 4**\n**Concept**: Real-world AI/RAG/Agentic applications with file handling\n\n### **Example 1: Building a Document Loader for RAG**\n\n**Problem**: Load multiple documents from a directory for RAG pipeline\n\n**Expected Output**: Dictionary mapping filenames to content"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\ndef load_documents_for_rag(directory):\n    \"\"\"\n    Load all text files from a directory for RAG indexing.\n    Returns a dictionary: {filename: content}\n    \"\"\"\n    documents = {}\n    \n    # Check if directory exists\n    if not os.path.exists(directory):\n        print(f\"Directory '{directory}' not found\")\n        return documents\n    \n    # Iterate through all files in directory\n    for filename in os.listdir(directory):\n        # Only process text files\n        if filename.endswith('.txt'):\n            file_path = os.path.join(directory, filename)\n            \n            # Read file content\n            try:\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    content = file.read()\n                    documents[filename] = content\n                    print(f\"Loaded: {filename} ({len(content)} characters)\")\n            except Exception as e:\n                print(f\"Error loading {filename}: {e}\")\n    \n    return documents\n\n# **Create sample documents**\nrag_docs = \"rag_documents\"\nos.makedirs(rag_docs, exist_ok=True)\n\n# **Sample AI-related documents**\nwith open(os.path.join(rag_docs, \"intro_to_rag.txt\"), 'w') as f:\n    f.write(\"RAG (Retrieval-Augmented Generation) combines retrieval with LLMs.\")\n\nwith open(os.path.join(rag_docs, \"embeddings.txt\"), 'w') as f:\n    f.write(\"Embeddings are vector representations of text for semantic search.\")\n\nwith open(os.path.join(rag_docs, \"agents.txt\"), 'w') as f:\n    f.write(\"AI agents use tools and reasoning to accomplish complex tasks.\")\n\n# **Load documents**\nprint(\"Loading documents for RAG pipeline:\\n\")\ndocs = load_documents_for_rag(rag_docs)\n\nprint(f\"\\nTotal documents loaded: {len(docs)}\")\n```\n\n**Why this works:**\n- Pattern used in real RAG systems to load knowledge base\n- `encoding='utf-8'` handles international characters\n- Error handling prevents one bad file from breaking everything\n- Returns structured data ready for embedding generation\n- This is the first step in building a RAG pipeline\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Caching AI Agent Results**\n\n**Problem**: Save agent outputs to cache file for faster retrieval\n\n**Expected Output**: Cache system that saves and loads agent results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\nimport json\n\nclass AgentCache:\n    \"\"\"Cache system for AI agent results.\"\"\"\n    \n    def __init__(self, cache_dir=\"agent_cache\"):\n        self.cache_dir = cache_dir\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    def save_result(self, query, result):\n        \"\"\"Save an agent result to cache.\"\"\"\n        # Create a simple filename from query (sanitized)\n        filename = query.replace(\" \", \"_\")[:50] + \".json\"\n        file_path = os.path.join(self.cache_dir, filename)\n        \n        # Save as JSON\n        cache_data = {\n            \"query\": query,\n            \"result\": result\n        }\n        \n        with open(file_path, 'w') as f:\n            json.dump(cache_data, f, indent=2)\n        \n        print(f\"Cached: {filename}\")\n    \n    def load_result(self, query):\n        \"\"\"Load a cached result if it exists.\"\"\"\n        filename = query.replace(\" \", \"_\")[:50] + \".json\"\n        file_path = os.path.join(self.cache_dir, filename)\n        \n        if os.path.exists(file_path):\n            with open(file_path, 'r') as f:\n                cache_data = json.load(f)\n            print(f\"Cache hit: {filename}\")\n            return cache_data[\"result\"]\n        \n        print(f\"Cache miss: {query}\")\n        return None\n\n# **Example usage**\ncache = AgentCache()\n\n# **Simulate agent processing a query**\nquery1 = \"What is machine learning\"\nresult1 = \"Machine learning is a subset of AI that enables systems to learn from data.\"\n\n# **Save to cache**\ncache.save_result(query1, result1)\n\n# **Later, try to load from cache**\nprint(\"\\nAttempting to load from cache:\")\ncached_result = cache.load_result(query1)\nprint(f\"Result: {cached_result}\")\n\n# **Try with query not in cache**\nprint(\"\\nTrying non-cached query:\")\ncache.load_result(\"What is deep learning\")\n```\n\n**Why this works:**\n- Caching saves expensive API calls to LLMs\n- JSON format is human-readable and easily parseable\n- `os.path.exists()` checks cache before processing\n- Real agentic systems use similar patterns to reduce costs\n- Cache directory organization keeps files manageable\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Logging Training Metrics**\n\n**Problem**: Append training metrics to a log file during ML training\n\n**Expected Output**: Log file with metrics from multiple epochs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\nfrom datetime import datetime\n\ndef log_training_metrics(log_dir, epoch, loss, accuracy):\n    \"\"\"\n    Append training metrics to a log file.\n    Used during ML model training to track progress.\n    \"\"\"\n    # Ensure log directory exists\n    os.makedirs(log_dir, exist_ok=True)\n    \n    # Create log file path\n    log_file = os.path.join(log_dir, \"training.log\")\n    \n    # Get current timestamp\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Format log entry\n    log_entry = f\"[{timestamp}] Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.2%}\\n\"\n    \n    # Append to log file\n    with open(log_file, 'a') as f:\n        f.write(log_entry)\n    \n    print(f\"Logged: Epoch {epoch}\")\n\n# **Simulate training loop**\nprint(\"Simulating model training...\\n\")\n\n# **Create logs directory**\nlogs_dir = os.path.join(\"ml_project\", \"logs\")\n\n# **Simulate 5 training epochs**\nimport random\nfor epoch in range(1, 6):\n    # Simulate improving metrics\n    loss = 2.0 / (epoch + 0.5)\n    accuracy = 0.5 + (epoch * 0.08)\n    \n    log_training_metrics(logs_dir, epoch, loss, accuracy)\n\n# **Read and display the log**\nprint(\"\\nTraining log contents:\")\nlog_path = os.path.join(logs_dir, \"training.log\")\nwith open(log_path, 'r') as f:\n    print(f.read())\n```\n\n**Why this works:**\n- Append mode `'a'` adds metrics without overwriting previous epochs\n- Timestamps help track training duration\n- Formatted output is human-readable\n- Pattern used in real ML frameworks (TensorFlow, PyTorch)\n- Logs are essential for debugging and comparing experiments\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 4**\n**Practice**: Real-world AI/RAG/Agentic applications\n\n### **Exercise 1: Document Chunker for RAG**\n\n**Task**: Read a large document and split it into chunks, saving each chunk as a separate file\n\n**Scenario**: In RAG systems, large documents are split into smaller chunks for embedding\n\n**Expected Output**: Multiple chunk files created"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os\n",
    "\n",
    "def chunk_document(input_file, output_dir, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Split a document into chunks and save each chunk.\n",
    "    chunk_size: number of characters per chunk\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Create a sample document first, then chunk it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\ndef chunk_document(input_file, output_dir, chunk_size=100):\n    \"\"\"\n    Split a document into chunks for RAG processing.\n    chunk_size: number of characters per chunk\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Read the document\n    with open(input_file, 'r') as f:\n        content = f.read()\n    \n    # Split into chunks\n    chunks = []\n    for i in range(0, len(content), chunk_size):\n        chunk = content[i:i + chunk_size]\n        chunks.append(chunk)\n    \n    # Save each chunk\n    for idx, chunk in enumerate(chunks, 1):\n        chunk_file = os.path.join(output_dir, f\"chunk_{idx:03d}.txt\")\n        with open(chunk_file, 'w') as f:\n            f.write(chunk)\n    \n    print(f\"Created {len(chunks)} chunks in '{output_dir}'\")\n    return len(chunks)\n\n# **Create a sample document**\nsample_doc = \"sample_document.txt\"\nwith open(sample_doc, 'w') as f:\n    f.write(\n        \"Retrieval-Augmented Generation (RAG) is a technique that combines \"\n        \"information retrieval with language generation. It retrieves relevant \"\n        \"documents from a knowledge base and uses them to generate accurate responses. \"\n        \"This approach helps AI systems provide more factual and grounded answers.\"\n    )\n\n# **Chunk the document**\nchunk_count = chunk_document(sample_doc, \"chunks_output\", chunk_size=100)\n\n# **Display first chunk as example**\nprint(\"\\nFirst chunk content:\")\nwith open(os.path.join(\"chunks_output\", \"chunk_001.txt\"), 'r') as f:\n    print(f.read())\n```\n\n**Why this works:**\n- Chunking is essential for RAG - LLMs have token limits\n- Fixed-size chunks are simple (real systems use semantic chunking)\n- Each chunk becomes a separate embedding in vector database\n- Numbered filenames maintain order\n- `f\"{idx:03d}\"` formats numbers with leading zeros (001, 002, etc.)\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Conversation History Manager**\n\n**Task**: Build a system to save and load conversation history for an AI agent\n\n**Expected Output**: Functions to append messages and retrieve full conversation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_message(conversation_id, role, message):\n",
    "    \"\"\"Save a conversation message (role: 'user' or 'assistant').\"\"\"\n",
    "    pass\n",
    "\n",
    "def load_conversation(conversation_id):\n",
    "    \"\"\"Load all messages in a conversation.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\nimport json\nfrom datetime import datetime\n\nCONVERSATIONS_DIR = \"conversations\"\n\ndef save_message(conversation_id, role, message):\n    \"\"\"\n    Save a conversation message.\n    role: 'user' or 'assistant'\n    \"\"\"\n    # Create conversations directory\n    os.makedirs(CONVERSATIONS_DIR, exist_ok=True)\n    \n    # Build file path\n    file_path = os.path.join(CONVERSATIONS_DIR, f\"{conversation_id}.jsonl\")\n    \n    # Create message entry\n    entry = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"role\": role,\n        \"message\": message\n    }\n    \n    # Append to file (JSONL format - one JSON per line)\n    with open(file_path, 'a') as f:\n        f.write(json.dumps(entry) + '\\n')\n\ndef load_conversation(conversation_id):\n    \"\"\"Load all messages in a conversation.\"\"\"\n    file_path = os.path.join(CONVERSATIONS_DIR, f\"{conversation_id}.jsonl\")\n    \n    if not os.path.exists(file_path):\n        return []\n    \n    messages = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            if line.strip():  # Skip empty lines\n                messages.append(json.loads(line))\n    \n    return messages\n\n# **Simulate a conversation**\nconv_id = \"chat_001\"\n\nsave_message(conv_id, \"user\", \"What is RAG?\")\nsave_message(conv_id, \"assistant\", \"RAG stands for Retrieval-Augmented Generation...\")\nsave_message(conv_id, \"user\", \"How does it work?\")\nsave_message(conv_id, \"assistant\", \"It combines document retrieval with LLMs...\")\n\nprint(\"Conversation saved\\n\")\n\n# **Load and display conversation**\nprint(\"Loading conversation history:\\n\")\nhistory = load_conversation(conv_id)\n\nfor msg in history:\n    print(f\"[{msg['role'].upper()}]: {msg['message']}\")\n    print()\n```\n\n**Why this works:**\n- JSONL format (JSON Lines) is efficient for append-only logs\n- Each message is a separate line - easy to parse and stream\n- Timestamps help with conversation analysis\n- Pattern used in real chatbots and AI assistants\n- Conversation history enables context-aware responses\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 3: Model Checkpoint Manager**\n\n**Task**: Save and load model checkpoints (simulated) during training\n\n**Expected Output**: System that saves checkpoints and can restore from the best one"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, epoch, metrics):\n",
    "    \"\"\"Save model checkpoint with metrics.\"\"\"\n",
    "    pass\n",
    "\n",
    "def find_best_checkpoint(checkpoint_dir, metric='accuracy'):\n",
    "    \"\"\"Find checkpoint with best metric value.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\nimport json\n\ndef save_checkpoint(checkpoint_dir, epoch, metrics):\n    \"\"\"\n    Save model checkpoint with training metrics.\n    In real systems, this would also save model weights.\n    \"\"\"\n    # Create checkpoint directory\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Create checkpoint data\n    checkpoint = {\n        \"epoch\": epoch,\n        \"metrics\": metrics\n    }\n    \n    # Save checkpoint\n    checkpoint_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.json\")\n    with open(checkpoint_file, 'w') as f:\n        json.dump(checkpoint, f, indent=2)\n    \n    print(f\"Checkpoint saved: epoch {epoch}\")\n\ndef find_best_checkpoint(checkpoint_dir, metric='accuracy'):\n    \"\"\"\n    Find the checkpoint with the best metric value.\n    Higher values are better.\n    \"\"\"\n    if not os.path.exists(checkpoint_dir):\n        print(\"No checkpoints found\")\n        return None\n    \n    best_checkpoint = None\n    best_value = -float('inf')\n    \n    # Scan all checkpoint files\n    for filename in os.listdir(checkpoint_dir):\n        if filename.endswith('.json'):\n            file_path = os.path.join(checkpoint_dir, filename)\n            \n            with open(file_path, 'r') as f:\n                checkpoint = json.load(f)\n            \n            # Check if this checkpoint is better\n            value = checkpoint['metrics'].get(metric, -float('inf'))\n            if value > best_value:\n                best_value = value\n                best_checkpoint = checkpoint\n    \n    return best_checkpoint\n\n# **Simulate training with checkpoints**\nprint(\"Simulating training with checkpoints\\n\")\n\ncheckpoint_dir = os.path.join(\"ml_project\", \"checkpoints\")\n\n# **Simulate 5 epochs**\nfor epoch in range(1, 6):\n    metrics = {\n        \"loss\": 2.0 / (epoch + 0.5),\n        \"accuracy\": 0.5 + (epoch * 0.08),\n        \"val_loss\": 2.2 / (epoch + 0.5)\n    }\n    save_checkpoint(checkpoint_dir, epoch, metrics)\n\n# **Find best checkpoint**\nprint(\"\\nFinding best checkpoint...\")\nbest = find_best_checkpoint(checkpoint_dir, metric='accuracy')\n\nif best:\n    print(f\"\\nBest checkpoint found:\")\n    print(f\"  Epoch: {best['epoch']}\")\n    print(f\"  Metrics: {best['metrics']}\")\n```\n\n**Why this works:**\n- Checkpointing prevents data loss if training crashes\n- Saving at each epoch allows resuming from any point\n- Finding best checkpoint helps select optimal model\n- Real ML frameworks (PyTorch, TensorFlow) use similar patterns\n- JSON makes checkpoints human-readable and debuggable\n- In production, you'd also save model weights (not just metrics)\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Optional Extra Practice**\n**Challenge yourself with these integrated problems**\n\n### **Challenge 1: Error-Resilient File Processor**\n\n**Task**: Create a function that processes multiple files, handling errors gracefully\n\n**Requirements**:\n- Read all `.txt` files from a directory\n- Count words in each file\n- Handle missing files and read errors\n- Return a summary report\n\n**Expected Output**: Dictionary with file statistics and error log"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\n\ndef process_files_with_error_handling(directory):\n    \"\"\"\n    Process all text files in directory with comprehensive error handling.\n    Returns statistics and error log.\n    \"\"\"\n    results = {\n        \"files_processed\": [],\n        \"errors\": [],\n        \"total_words\": 0,\n        \"total_files\": 0\n    }\n    \n    # Check if directory exists\n    if not os.path.exists(directory):\n        results[\"errors\"].append(f\"Directory '{directory}' not found\")\n        return results\n    \n    # Process each file\n    for filename in os.listdir(directory):\n        if not filename.endswith('.txt'):\n            continue\n        \n        file_path = os.path.join(directory, filename)\n        \n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            # Count words\n            word_count = len(content.split())\n            \n            # Record success\n            results[\"files_processed\"].append({\n                \"filename\": filename,\n                \"word_count\": word_count\n            })\n            results[\"total_words\"] += word_count\n            results[\"total_files\"] += 1\n            \n        except FileNotFoundError:\n            results[\"errors\"].append(f\"File not found: {filename}\")\n        except UnicodeDecodeError:\n            results[\"errors\"].append(f\"Encoding error: {filename}\")\n        except Exception as e:\n            results[\"errors\"].append(f\"Error processing {filename}: {str(e)}\")\n    \n    return results\n\n# **Test the function**\nresults = process_files_with_error_handling(\"rag_documents\")\n\nprint(\"=== File Processing Report ===\")\nprint(f\"\\nFiles processed: {results['total_files']}\")\nprint(f\"Total words: {results['total_words']}\")\n\nprint(\"\\nDetails:\")\nfor file_info in results[\"files_processed\"]:\n    print(f\"  {file_info['filename']}: {file_info['word_count']} words\")\n\nif results[\"errors\"]:\n    print(\"\\nErrors encountered:\")\n    for error in results[\"errors\"]:\n        print(f\"  - {error}\")\n```\n\n**Why this works:**\n- Comprehensive error handling prevents crashes\n- Specific exception types allow targeted error messages\n- Results dictionary provides detailed report\n- Pattern essential for production data pipelines\n- Handles encoding issues common in real-world files\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Challenge 2: RAG Document Index Builder**\n\n**Task**: Build a complete document indexing system for RAG\n\n**Requirements**:\n- Load documents from nested directories\n- Create metadata for each document (path, size, word count)\n- Save index as JSON file\n- Provide search function to find documents\n\n**Expected Output**: Complete indexing system with search capability"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\nimport json\n\nclass DocumentIndexer:\n    \"\"\"Simple document indexer for RAG systems.\"\"\"\n    \n    def __init__(self, index_file=\"document_index.json\"):\n        self.index_file = index_file\n        self.index = []\n    \n    def build_index(self, root_directory):\n        \"\"\"Recursively index all text files in directory tree.\"\"\"\n        print(f\"Building index from: {root_directory}\\n\")\n        \n        # Walk through directory tree\n        for dirpath, dirnames, filenames in os.walk(root_directory):\n            for filename in filenames:\n                if filename.endswith('.txt'):\n                    file_path = os.path.join(dirpath, filename)\n                    \n                    try:\n                        # Read file\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            content = f.read()\n                        \n                        # Create metadata\n                        metadata = {\n                            \"filename\": filename,\n                            \"path\": file_path,\n                            \"size_bytes\": os.path.getsize(file_path),\n                            \"word_count\": len(content.split()),\n                            \"preview\": content[:100]  # First 100 chars\n                        }\n                        \n                        self.index.append(metadata)\n                        print(f\"Indexed: {filename}\")\n                        \n                    except Exception as e:\n                        print(f\"Error indexing {filename}: {e}\")\n        \n        print(f\"\\nTotal documents indexed: {len(self.index)}\")\n    \n    def save_index(self):\n        \"\"\"Save index to JSON file.\"\"\"\n        with open(self.index_file, 'w') as f:\n            json.dump(self.index, f, indent=2)\n        print(f\"Index saved to: {self.index_file}\")\n    \n    def load_index(self):\n        \"\"\"Load index from JSON file.\"\"\"\n        if os.path.exists(self.index_file):\n            with open(self.index_file, 'r') as f:\n                self.index = json.load(f)\n            print(f\"Index loaded: {len(self.index)} documents\")\n        else:\n            print(\"No index file found\")\n    \n    def search(self, keyword):\n        \"\"\"Search for documents containing keyword.\"\"\"\n        results = []\n        keyword_lower = keyword.lower()\n        \n        for doc in self.index:\n            # Search in filename and preview\n            if (keyword_lower in doc['filename'].lower() or \n                keyword_lower in doc['preview'].lower()):\n                results.append(doc)\n        \n        return results\n\n# **Example usage**\nindexer = DocumentIndexer()\n\n# **Build index from rag_documents directory**\nindexer.build_index(\"rag_documents\")\n\n# **Save index**\nindexer.save_index()\n\n# **Search example**\nprint(\"\\n=== Search Results ===\")\nresults = indexer.search(\"RAG\")\nprint(f\"Found {len(results)} documents containing 'RAG':\\n\")\n\nfor doc in results:\n    print(f\"File: {doc['filename']}\")\n    print(f\"Words: {doc['word_count']}\")\n    print(f\"Preview: {doc['preview'][:50]}...\")\n    print()\n```\n\n**Why this works:**\n- `os.walk()` recursively traverses directory trees\n- Metadata enables efficient document retrieval without re-reading files\n- JSON index is portable and human-readable\n- Search functionality provides basic retrieval capability\n- Real RAG systems extend this with vector embeddings and semantic search\n- Pattern is foundation for document management in AI applications\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Summary**\n\nYou've learned:\n- Basic file operations (read, write, append)\n- Context managers (`with` statement) for safe file handling\n- Cross-platform path handling with `os.path.join`\n- Error handling for robust file operations\n- Real-world patterns for AI/RAG/Agentic systems\n\n**Key Takeaways**:\n1. Always use `with` statement for file operations\n2. Always use `os.path.join()` for cross-platform paths\n3. Handle errors gracefully with try/except\n4. Use appropriate file modes (`'r'`, `'w'`, `'a'`)\n5. Consider memory efficiency for large files (line-by-line reading)\n\n**Next Steps**:\n- Learn about JSON and CSV file handling\n- Explore file operations in pandas for data analysis\n- Study binary file handling for images and models\n- Practice with real datasets and API responses"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}