{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a87c73",
   "metadata": {},
   "source": [
    "# 07 - RAG Systems: Building Knowledge-Enhanced Applications\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll build complete Retrieval-Augmented Generation (RAG) systems that combine LLMs with external knowledge sources. You'll learn how to create applications that can answer questions using your own documents.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Build a complete RAG pipeline from document loading to answer generation\n",
    "- Implement different retrieval strategies for better results\n",
    "- Handle context length limitations and optimize retrieval\n",
    "- Create multi-query RAG for comprehensive answers\n",
    "- Add metadata filtering for targeted retrieval\n",
    "- Evaluate and optimize RAG system performance\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of notebooks 01-06 (especially 05 on document loading and 06 on embeddings)\n",
    "- Understanding of vector stores and similarity search\n",
    "- Basic knowledge of document processing\n",
    "\n",
    "## Back-and-Forth Teaching Pattern\n",
    "This notebook follows our pattern:\n",
    "1. **Instructor Activity**: Demonstrates a concept with complete examples\n",
    "2. **Learner Activity**: You apply the concept with guidance and hidden solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's install and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-community langchain-openai chromadb faiss-cpu pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample_docs",
   "metadata": {},
   "source": [
    "## Create Sample Documents\n",
    "\n",
    "Let's create some sample documents to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_docs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents about AI topics\n",
    "sample_texts = [\n",
    "    \"\"\"Machine Learning Fundamentals\n",
    "    Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience.\n",
    "    There are three main types: supervised learning (with labeled data), unsupervised learning (finding patterns),\n",
    "    and reinforcement learning (learning through rewards). Common algorithms include decision trees, neural networks,\n",
    "    and support vector machines. Applications range from image recognition to recommendation systems.\"\"\",\n",
    "    \n",
    "    \"\"\"Natural Language Processing\n",
    "    NLP is a field of AI that helps computers understand, interpret, and manipulate human language.\n",
    "    Key techniques include tokenization, named entity recognition, sentiment analysis, and language modeling.\n",
    "    Modern NLP uses transformer models like BERT and GPT. Applications include chatbots, translation,\n",
    "    and text summarization. Challenges include context understanding and handling ambiguity.\"\"\",\n",
    "    \n",
    "    \"\"\"Computer Vision Applications\n",
    "    Computer vision enables machines to interpret and understand visual information from the world.\n",
    "    Core tasks include object detection, image classification, facial recognition, and semantic segmentation.\n",
    "    Convolutional Neural Networks (CNNs) are the backbone of most vision systems. Real-world applications\n",
    "    include autonomous vehicles, medical imaging, and augmented reality. Recent advances include\n",
    "    vision transformers and self-supervised learning.\"\"\",\n",
    "    \n",
    "    \"\"\"Deep Learning Architecture\n",
    "    Deep learning uses neural networks with multiple layers to progressively extract higher-level features.\n",
    "    Key architectures include CNNs for images, RNNs for sequences, and Transformers for various tasks.\n",
    "    Training requires large datasets and computational resources. Techniques like transfer learning\n",
    "    and fine-tuning help adapt pre-trained models. Challenges include interpretability and overfitting.\"\"\",\n",
    "    \n",
    "    \"\"\"AI Ethics and Bias\n",
    "    Ethical AI development requires addressing bias, fairness, transparency, and accountability.\n",
    "    Bias can enter through training data, algorithm design, or deployment contexts. Mitigation strategies\n",
    "    include diverse datasets, fairness metrics, and regular audits. Important considerations include\n",
    "    privacy protection, explainable AI, and responsible deployment. Regulatory frameworks are emerging globally.\"\"\"\n",
    "]\n",
    "\n",
    "# Save documents to files\n",
    "for i, text in enumerate(sample_texts):\n",
    "    with open(f\"ai_doc_{i}.txt\", \"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(\"Sample documents created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 1: Building a Basic RAG Pipeline\n",
    "\n",
    "Let's build a complete RAG system from scratch, understanding each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_rag_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and split documents\n",
    "documents = []\n",
    "for i in range(5):\n",
    "    loader = TextLoader(f\"ai_doc_{i}.txt\")\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(splits)} document chunks\")\n",
    "print(f\"\\nExample chunk: {splits[0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_rag_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"ai_knowledge_base\"\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"Vector store created with retriever configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_rag_3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build RAG chain using LCEL\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create RAG prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an AI expert assistant. Use the following context to answer the question.\n",
    "    If you don't know the answer based on the context, say so.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Build the RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain built successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_rag_4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test the RAG system\n",
    "questions = [\n",
    "    \"What are the three main types of machine learning?\",\n",
    "    \"How do CNNs relate to computer vision?\",\n",
    "    \"What are the key ethical considerations in AI?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_rag_5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: RAG with source citations\n",
    "rag_prompt_with_sources = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an AI expert assistant. Use the following context to answer the question.\n",
    "    Include relevant source references in your answer.\n",
    "    \n",
    "    Context (with sources):\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer (include source references):\"\"\"\n",
    ")\n",
    "\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get('source', f'Document {i+1}')\n",
    "        formatted.append(f\"[Source: {source}]\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "rag_chain_with_sources = (\n",
    "    {\"context\": retriever | format_docs_with_sources, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_with_sources\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test with sources\n",
    "question = \"What techniques are used in NLP?\"\n",
    "answer = rag_chain_with_sources.invoke(question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 1: Build Your Own RAG System\n",
    "\n",
    "Now it's your turn! Build a RAG system for a customer support knowledge base.\n",
    "\n",
    "**Task**: Create a RAG system that can answer questions about a product using a knowledge base.\n",
    "\n",
    "Requirements:\n",
    "1. Create documents about a fictional product (e.g., a smart home device)\n",
    "2. Build a vector store with appropriate chunk sizes\n",
    "3. Implement a RAG chain with a custom prompt for customer support\n",
    "4. Add functionality to handle \"I don't know\" cases gracefully\n",
    "5. Include confidence scoring in responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_1_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your product knowledge base documents\n",
    "product_docs = [\n",
    "    \"\"\"SmartHome Hub - Product Overview\n",
    "    # Your product description here\"\"\",\n",
    "    \n",
    "    \"\"\"Installation Guide\n",
    "    # Installation steps here\"\"\",\n",
    "    \n",
    "    # Add more documents...\n",
    "]\n",
    "\n",
    "# TODO: Save documents to files\n",
    "# Your code here\n",
    "\n",
    "# TODO: Load and split documents\n",
    "# Your code here\n",
    "\n",
    "# TODO: Create vector store and retriever\n",
    "# Your code here\n",
    "\n",
    "# TODO: Build customer support RAG chain\n",
    "# Hint: Create a prompt that:\n",
    "# - Acts as a friendly support agent\n",
    "# - Provides step-by-step help when appropriate\n",
    "# - Admits when information isn't available\n",
    "# - Suggests contacting human support for complex issues\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# TODO: Test with customer questions\n",
    "test_questions = [\n",
    "    \"How do I set up the SmartHome Hub?\",\n",
    "    \"What's the warranty period?\",\n",
    "    \"Can it work with my existing smart lights?\"\n",
    "]\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_1_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default - try solving it yourself first!)\n",
    "\n",
    "\"\"\"\n",
    "# Create product knowledge base\n",
    "product_docs = [\n",
    "    '''SmartHome Hub - Product Overview\n",
    "    The SmartHome Hub is an all-in-one home automation controller that connects all your smart devices.\n",
    "    Features include voice control, mobile app, automation routines, and compatibility with 1000+ devices.\n",
    "    It supports WiFi, Zigbee, Z-Wave, and Bluetooth protocols. The hub has built-in security features\n",
    "    including encryption and regular security updates. Price: $199. Warranty: 2 years.''',\n",
    "    \n",
    "    '''Installation Guide\n",
    "    Step 1: Unbox your SmartHome Hub and connect the power adapter.\n",
    "    Step 2: Download the SmartHome app from App Store or Google Play.\n",
    "    Step 3: Create an account or sign in to your existing account.\n",
    "    Step 4: Follow the in-app setup wizard to connect the hub to your WiFi.\n",
    "    Step 5: The LED will turn solid green when connected successfully.\n",
    "    Step 6: Start adding your smart devices through the app.\n",
    "    Troubleshooting: If LED is red, check your WiFi password. If blinking blue, hub is in pairing mode.''',\n",
    "    \n",
    "    '''Compatible Devices\n",
    "    The SmartHome Hub works with: Philips Hue lights, LIFX bulbs, Nest thermostats, Ring doorbells,\n",
    "    August smart locks, Samsung SmartThings devices, Amazon Alexa, Google Home, Apple HomeKit (via bridge),\n",
    "    Sonos speakers, and many more. Check our website for the full compatibility list.\n",
    "    Most devices using Zigbee, Z-Wave, or WiFi protocols are supported.''',\n",
    "    \n",
    "    '''Troubleshooting Common Issues\n",
    "    Hub won't connect: Ensure you're using 2.4GHz WiFi (not 5GHz). Reset hub by holding button for 10 seconds.\n",
    "    Devices not responding: Check if device is powered on. Try removing and re-adding the device.\n",
    "    App crashes: Update to latest version. Clear app cache. Reinstall if needed.\n",
    "    Automation not working: Check time zone settings. Verify all devices in automation are online.\n",
    "    For other issues, contact support at support@smarthomehub.com or call 1-800-SMARTHUB.'''\n",
    "]\n",
    "\n",
    "# Save documents\n",
    "for i, doc in enumerate(product_docs):\n",
    "    with open(f\"product_doc_{i}.txt\", \"w\") as f:\n",
    "        f.write(doc)\n",
    "\n",
    "# Load and split\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "product_documents = []\n",
    "for i in range(len(product_docs)):\n",
    "    loader = TextLoader(f\"product_doc_{i}.txt\")\n",
    "    product_documents.extend(loader.load())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "product_splits = text_splitter.split_documents(product_documents)\n",
    "\n",
    "# Create vector store\n",
    "product_vectorstore = Chroma.from_documents(\n",
    "    documents=product_splits,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    collection_name=\"product_support\"\n",
    ")\n",
    "\n",
    "product_retriever = product_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Customer support RAG chain\n",
    "support_prompt = ChatPromptTemplate.from_template(\n",
    "    '''You are a friendly SmartHome Hub customer support assistant. \n",
    "    Use the following context to help the customer. Be helpful, clear, and professional.\n",
    "    \n",
    "    Guidelines:\n",
    "    - Provide step-by-step instructions when applicable\n",
    "    - If the information isn't in the context, politely say you'll need to check with the team\n",
    "    - For complex technical issues, suggest contacting support directly\n",
    "    - Always be encouraging and positive\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Customer Question: {question}\n",
    "    \n",
    "    Support Response:'''\n",
    ")\n",
    "\n",
    "def format_product_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "support_chain = (\n",
    "    {\"context\": product_retriever | format_product_docs, \"question\": RunnablePassthrough()}\n",
    "    | support_prompt\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the system\n",
    "test_questions = [\n",
    "    \"How do I set up the SmartHome Hub?\",\n",
    "    \"What's the warranty period?\",\n",
    "    \"Can it work with my Philips Hue lights?\",\n",
    "    \"My hub LED is red, what should I do?\",\n",
    "    \"Does it support Matter protocol?\"  # This isn't in our docs\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nCustomer: {question}\")\n",
    "    response = support_chain.invoke(question)\n",
    "    print(f\"Support: {response}\")\n",
    "    print(\"-\" * 60)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Try implementing the customer support RAG system above!\")\n",
    "print(\"The solution shows how to create a helpful, professional support assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 2: Advanced Retrieval Strategies\n",
    "\n",
    "Let's explore advanced retrieval techniques for better RAG performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi_query_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Query Retriever: Generate multiple queries for comprehensive retrieval\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create a prompt to generate multiple search queries\n",
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI assistant helping to retrieve information.\n",
    "    Generate 3 different versions of the given question to retrieve relevant documents.\n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\n",
    "    Alternative questions:\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create multi-query retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm,\n",
    "    prompt=query_prompt\n",
    ")\n",
    "\n",
    "# Test multi-query retrieval\n",
    "question = \"What are the challenges in implementing AI systems?\"\n",
    "docs = multi_query_retriever.get_relevant_documents(question)\n",
    "\n",
    "print(f\"Original question: {question}\")\n",
    "print(f\"\\nRetrieved {len(docs)} documents using multi-query approach:\")\n",
    "for i, doc in enumerate(docs[:3]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contextual_compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual Compression: Extract only relevant parts of retrieved documents\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Create compressor\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Create compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    ")\n",
    "\n",
    "# Compare regular vs compressed retrieval\n",
    "question = \"What specific techniques are used in NLP?\"\n",
    "\n",
    "print(\"Regular Retrieval:\")\n",
    "regular_docs = vectorstore.as_retriever().get_relevant_documents(question)\n",
    "print(f\"First doc ({len(regular_docs[0].page_content)} chars): {regular_docs[0].page_content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Compressed Retrieval (only relevant parts):\")\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "print(f\"First doc ({len(compressed_docs[0].page_content)} chars): {compressed_docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid_search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search: Combine semantic and keyword search\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Create BM25 retriever for keyword search\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "# Create ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vectorstore.as_retriever(search_kwargs={\"k\": 3})],\n",
    "    weights=[0.4, 0.6]  # 40% keyword, 60% semantic\n",
    ")\n",
    "\n",
    "# Test hybrid search\n",
    "question = \"CNN architecture neural networks\"\n",
    "hybrid_docs = ensemble_retriever.get_relevant_documents(question)\n",
    "\n",
    "print(f\"Hybrid search for: {question}\")\n",
    "print(f\"\\nRetrieved {len(hybrid_docs)} documents:\")\n",
    "for i, doc in enumerate(hybrid_docs[:3]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metadata_filtering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata Filtering: Target specific document categories\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Create documents with metadata\n",
    "categorized_docs = [\n",
    "    Document(\n",
    "        page_content=\"Supervised learning uses labeled data for training...\",\n",
    "        metadata={\"category\": \"ml_basics\", \"level\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Transformer architectures revolutionized NLP with attention mechanisms...\",\n",
    "        metadata={\"category\": \"deep_learning\", \"level\": \"advanced\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Ethical AI requires addressing bias in training data...\",\n",
    "        metadata={\"category\": \"ethics\", \"level\": \"intermediate\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create vector store with metadata\n",
    "metadata_vectorstore = Chroma.from_documents(\n",
    "    documents=categorized_docs,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    collection_name=\"categorized_knowledge\"\n",
    ")\n",
    "\n",
    "# Retrieve with metadata filter\n",
    "filtered_retriever = metadata_vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 2,\n",
    "        \"filter\": {\"category\": \"deep_learning\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test filtered retrieval\n",
    "question = \"How do modern NLP systems work?\"\n",
    "filtered_docs = filtered_retriever.get_relevant_documents(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\nFiltered retrieval (deep_learning category only):\")\n",
    "for doc in filtered_docs:\n",
    "    print(f\"- {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 2: Implement Advanced Retrieval\n",
    "\n",
    "Build a RAG system with advanced retrieval for a technical documentation chatbot.\n",
    "\n",
    "**Task**: Create a system that uses multiple retrieval strategies to provide accurate technical answers.\n",
    "\n",
    "Requirements:\n",
    "1. Implement multi-query retrieval for better coverage\n",
    "2. Add contextual compression for concise responses\n",
    "3. Use metadata filtering to separate beginner/advanced content\n",
    "4. Create a fallback mechanism when confidence is low\n",
    "5. Implement query routing based on question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_2_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create technical documentation with different levels\n",
    "tech_docs = [\n",
    "    # Create documents with metadata for level (beginner/advanced) and topic\n",
    "    # Your documents here\n",
    "]\n",
    "\n",
    "# TODO: Create vector store with metadata\n",
    "# Your code here\n",
    "\n",
    "# TODO: Implement multi-query retriever\n",
    "# Hint: Create a prompt that generates technical variations of the question\n",
    "# Your code here\n",
    "\n",
    "# TODO: Add contextual compression\n",
    "# Your code here\n",
    "\n",
    "# TODO: Build query router\n",
    "# Hint: Detect if question is beginner or advanced, then use appropriate filter\n",
    "def route_query(question: str) -> str:\n",
    "    # Determine question level\n",
    "    # Return \"beginner\" or \"advanced\"\n",
    "    pass\n",
    "\n",
    "# TODO: Create adaptive RAG chain\n",
    "# Should:\n",
    "# - Route queries to appropriate level\n",
    "# - Use multi-query for complex questions\n",
    "# - Compress results for clarity\n",
    "# - Handle low-confidence cases\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Test with different types of questions\n",
    "test_queries = [\n",
    "    \"What is an API?\",  # Beginner question\n",
    "    \"How do I implement OAuth 2.0 with PKCE flow?\",  # Advanced question\n",
    "    \"Explain microservices architecture trade-offs\"  # Complex question\n",
    "]\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_2_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Create technical documentation\n",
    "tech_docs = [\n",
    "    Document(\n",
    "        page_content='''API Basics: An API (Application Programming Interface) is a set of rules\n",
    "        that allows different software applications to communicate. Think of it like a menu\n",
    "        in a restaurant - it tells you what you can order and how to order it.\n",
    "        Common types include REST APIs and GraphQL APIs.''',\n",
    "        metadata={\"level\": \"beginner\", \"topic\": \"api\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content='''Advanced API Security: OAuth 2.0 with PKCE (Proof Key for Code Exchange)\n",
    "        prevents authorization code interception attacks. Implementation: Generate code_verifier\n",
    "        (random string), create code_challenge (SHA256 hash), include in authorization request,\n",
    "        send code_verifier in token exchange. Critical for mobile and SPA applications.''',\n",
    "        metadata={\"level\": \"advanced\", \"topic\": \"api_security\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content='''Microservices Architecture involves breaking applications into small,\n",
    "        independent services. Benefits: scalability, technology diversity, fault isolation.\n",
    "        Trade-offs: increased complexity, network latency, data consistency challenges.\n",
    "        Requires robust service discovery, API gateway, and monitoring.''',\n",
    "        metadata={\"level\": \"advanced\", \"topic\": \"architecture\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content='''REST API Basics: REST uses HTTP methods - GET (read), POST (create),\n",
    "        PUT (update), DELETE (remove). URLs identify resources. Status codes indicate results:\n",
    "        200 (success), 404 (not found), 500 (server error). JSON is the common data format.''',\n",
    "        metadata={\"level\": \"beginner\", \"topic\": \"api\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create vector store with metadata\n",
    "tech_vectorstore = Chroma.from_documents(\n",
    "    documents=tech_docs,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    collection_name=\"tech_docs\"\n",
    ")\n",
    "\n",
    "# Multi-query retriever with technical variations\n",
    "tech_query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template='''You are a technical documentation assistant.\n",
    "    Generate 3 different versions of this question to search technical docs:\n",
    "    - One using simpler terms\n",
    "    - One using technical terminology\n",
    "    - One focusing on practical implementation\n",
    "    \n",
    "    Original question: {question}\n",
    "    \n",
    "    Alternative questions:'''\n",
    ")\n",
    "\n",
    "multi_query_tech = MultiQueryRetriever.from_llm(\n",
    "    retriever=tech_vectorstore.as_retriever(),\n",
    "    llm=ChatOpenAI(temperature=0.2),\n",
    "    prompt=tech_query_prompt\n",
    ")\n",
    "\n",
    "# Contextual compression for technical docs\n",
    "tech_compressor = LLMChainExtractor.from_llm(\n",
    "    ChatOpenAI(temperature=0)\n",
    ")\n",
    "\n",
    "compression_tech = ContextualCompressionRetriever(\n",
    "    base_compressor=tech_compressor,\n",
    "    base_retriever=multi_query_tech\n",
    ")\n",
    "\n",
    "# Query router\n",
    "def route_query(question: str) -> str:\n",
    "    # Simple keyword-based routing (could use LLM for better classification)\n",
    "    beginner_keywords = ['what is', 'basic', 'introduction', 'explain', 'simple']\n",
    "    advanced_keywords = ['implement', 'optimize', 'architecture', 'advanced', 'pattern']\n",
    "    \n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    if any(keyword in question_lower for keyword in beginner_keywords):\n",
    "        return \"beginner\"\n",
    "    elif any(keyword in question_lower for keyword in advanced_keywords):\n",
    "        return \"advanced\"\n",
    "    else:\n",
    "        return \"all\"  # No filter\n",
    "\n",
    "# Adaptive RAG chain\n",
    "def create_adaptive_chain(level_filter=None):\n",
    "    if level_filter and level_filter != \"all\":\n",
    "        retriever = tech_vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 3, \"filter\": {\"level\": level_filter}}\n",
    "        )\n",
    "    else:\n",
    "        retriever = tech_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    # Add compression\n",
    "    compressed_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=tech_compressor,\n",
    "        base_retriever=retriever\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        '''You are a technical documentation assistant. Answer based on the context provided.\n",
    "        If confidence is low, suggest consulting official documentation.\n",
    "        \n",
    "        Context level: {level}\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer (adjust complexity based on level):'''\n",
    "    )\n",
    "    \n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": compressed_retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"level\": lambda x: level_filter or \"mixed\"\n",
    "        }\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0.2)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "# Test the system\n",
    "test_queries = [\n",
    "    \"What is an API?\",\n",
    "    \"How do I implement OAuth 2.0 with PKCE flow?\",\n",
    "    \"Explain microservices architecture trade-offs\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    level = route_query(query)\n",
    "    print(f\"\\nQuestion: {query}\")\n",
    "    print(f\"Detected Level: {level}\")\n",
    "    \n",
    "    chain = create_adaptive_chain(level)\n",
    "    answer = chain.invoke(query)\n",
    "    \n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 70)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Implement an advanced retrieval system with routing and compression!\")\n",
    "print(\"The solution demonstrates multi-query, compression, and adaptive routing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 3: RAG Evaluation and Optimization\n",
    "\n",
    "Let's learn how to evaluate and optimize RAG system performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_evaluation_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Evaluation Metrics\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, llm, retriever):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def evaluate_retrieval_relevance(self, question: str, docs: List) -> float:\n",
    "        \"\"\"Evaluate how relevant retrieved documents are to the question\"\"\"\n",
    "        relevance_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Rate the relevance of this document to the question on a scale of 0-10.\n",
    "            Question: {question}\n",
    "            Document: {document}\n",
    "            \n",
    "            Return only a number between 0-10:\"\"\"\n",
    "        )\n",
    "        \n",
    "        scores = []\n",
    "        for doc in docs:\n",
    "            chain = relevance_prompt | self.llm | StrOutputParser()\n",
    "            score_str = chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"document\": doc.page_content\n",
    "            })\n",
    "            try:\n",
    "                scores.append(float(score_str.strip()))\n",
    "            except:\n",
    "                scores.append(0)\n",
    "        \n",
    "        return np.mean(scores) if scores else 0\n",
    "    \n",
    "    def evaluate_answer_faithfulness(self, context: str, answer: str) -> float:\n",
    "        \"\"\"Check if answer is grounded in the context\"\"\"\n",
    "        faithfulness_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Is this answer fully supported by the context? Rate 0-10.\n",
    "            Context: {context}\n",
    "            Answer: {answer}\n",
    "            \n",
    "            Return only a number (10 = fully supported, 0 = not supported):\"\"\"\n",
    "        )\n",
    "        \n",
    "        chain = faithfulness_prompt | self.llm | StrOutputParser()\n",
    "        score_str = chain.invoke({\"context\": context, \"answer\": answer})\n",
    "        \n",
    "        try:\n",
    "            return float(score_str.strip())\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def evaluate_rag_pipeline(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"Comprehensive RAG evaluation\"\"\"\n",
    "        results = {\n",
    "            \"retrieval_relevance\": [],\n",
    "            \"answer_faithfulness\": [],\n",
    "            \"overall_quality\": []\n",
    "        }\n",
    "        \n",
    "        for case in test_cases:\n",
    "            question = case[\"question\"]\n",
    "            expected_topics = case.get(\"expected_topics\", [])\n",
    "            \n",
    "            # Get retrieved docs\n",
    "            docs = self.retriever.get_relevant_documents(question)\n",
    "            \n",
    "            # Evaluate retrieval\n",
    "            relevance = self.evaluate_retrieval_relevance(question, docs)\n",
    "            results[\"retrieval_relevance\"].append(relevance)\n",
    "            \n",
    "            print(f\"\\nQ: {question}\")\n",
    "            print(f\"Retrieval Relevance: {relevance:.1f}/10\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = RAGEvaluator(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Test cases for evaluation\n",
    "test_cases = [\n",
    "    {\"question\": \"What are the types of machine learning?\", \"expected_topics\": [\"supervised\", \"unsupervised\"]},\n",
    "    {\"question\": \"How does computer vision work?\", \"expected_topics\": [\"CNN\", \"image\"]}\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluator.evaluate_rag_pipeline(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Optimization Strategies\n",
    "\n",
    "class OptimizedRAG:\n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.cache = {}  # Simple cache for repeated queries\n",
    "    \n",
    "    def optimize_chunk_size(self, documents: List, sizes: List[int] = [100, 200, 500]):\n",
    "        \"\"\"Find optimal chunk size for your documents\"\"\"\n",
    "        best_size = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for size in sizes:\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=size,\n",
    "                chunk_overlap=size // 10\n",
    "            )\n",
    "            splits = splitter.split_documents(documents)\n",
    "            \n",
    "            # Evaluate retrieval quality (simplified)\n",
    "            avg_length = np.mean([len(s.page_content) for s in splits])\n",
    "            num_chunks = len(splits)\n",
    "            \n",
    "            # Balance between context and granularity\n",
    "            score = (avg_length / 500) * (100 / num_chunks)\n",
    "            \n",
    "            print(f\"Chunk size {size}: {num_chunks} chunks, avg {avg_length:.0f} chars, score: {score:.2f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_size = size\n",
    "        \n",
    "        return best_size\n",
    "    \n",
    "    def rerank_results(self, question: str, docs: List, top_k: int = 3):\n",
    "        \"\"\"Rerank retrieved documents using cross-encoder or LLM\"\"\"\n",
    "        rerank_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Score this document's relevance to the question (0-10).\n",
    "            Question: {question}\n",
    "            Document: {document}\n",
    "            Score:\"\"\"\n",
    "        )\n",
    "        \n",
    "        scored_docs = []\n",
    "        for doc in docs:\n",
    "            chain = rerank_prompt | self.llm | StrOutputParser()\n",
    "            score = chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"document\": doc.page_content[:500]\n",
    "            })\n",
    "            try:\n",
    "                scored_docs.append((float(score.strip()), doc))\n",
    "            except:\n",
    "                scored_docs.append((0, doc))\n",
    "        \n",
    "        # Sort by score and return top k\n",
    "        scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [doc for _, doc in scored_docs[:top_k]]\n",
    "    \n",
    "    def query_with_cache(self, question: str):\n",
    "        \"\"\"Cache frequently asked questions\"\"\"\n",
    "        if question in self.cache:\n",
    "            print(\"Using cached result\")\n",
    "            return self.cache[question]\n",
    "        \n",
    "        # Perform RAG\n",
    "        docs = self.vectorstore.as_retriever().get_relevant_documents(question)\n",
    "        reranked_docs = self.rerank_results(question, docs)\n",
    "        \n",
    "        # Generate answer\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "        answer = f\"Based on: {context[:200]}...\"  # Simplified\n",
    "        \n",
    "        # Cache result\n",
    "        self.cache[question] = answer\n",
    "        return answer\n",
    "\n",
    "# Create optimized RAG\n",
    "optimized_rag = OptimizedRAG(vectorstore, ChatOpenAI(temperature=0))\n",
    "\n",
    "# Test optimization techniques\n",
    "print(\"Testing chunk size optimization:\")\n",
    "best_chunk = optimized_rag.optimize_chunk_size(documents[:3])\n",
    "print(f\"\\nBest chunk size: {best_chunk}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nTesting query with caching:\")\n",
    "question = \"What is machine learning?\"\n",
    "result1 = optimized_rag.query_with_cache(question)\n",
    "result2 = optimized_rag.query_with_cache(question)  # Should use cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Performance Monitoring\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class RAGMonitor:\n",
    "    def __init__(self):\n",
    "        self.metrics = []\n",
    "    \n",
    "    def log_query(self, question: str, retrieval_time: float, \n",
    "                  generation_time: float, num_docs: int, answer_length: int):\n",
    "        \"\"\"Log performance metrics for each query\"\"\"\n",
    "        self.metrics.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"question\": question,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": retrieval_time + generation_time,\n",
    "            \"num_docs\": num_docs,\n",
    "            \"answer_length\": answer_length\n",
    "        })\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get performance statistics\"\"\"\n",
    "        if not self.metrics:\n",
    "            return {}\n",
    "        \n",
    "        retrieval_times = [m[\"retrieval_time\"] for m in self.metrics]\n",
    "        generation_times = [m[\"generation_time\"] for m in self.metrics]\n",
    "        total_times = [m[\"total_time\"] for m in self.metrics]\n",
    "        \n",
    "        return {\n",
    "            \"avg_retrieval_time\": np.mean(retrieval_times),\n",
    "            \"avg_generation_time\": np.mean(generation_times),\n",
    "            \"avg_total_time\": np.mean(total_times),\n",
    "            \"queries_processed\": len(self.metrics),\n",
    "            \"p95_total_time\": np.percentile(total_times, 95) if total_times else 0\n",
    "        }\n",
    "\n",
    "# Monitored RAG pipeline\n",
    "monitor = RAGMonitor()\n",
    "\n",
    "def monitored_rag_query(question: str, retriever, llm):\n",
    "    \"\"\"RAG query with monitoring\"\"\"\n",
    "    # Retrieval phase\n",
    "    start_retrieval = time.time()\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    retrieval_time = time.time() - start_retrieval\n",
    "    \n",
    "    # Generation phase\n",
    "    start_generation = time.time()\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Answer based on context:\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    generation_time = time.time() - start_generation\n",
    "    \n",
    "    # Log metrics\n",
    "    monitor.log_query(\n",
    "        question=question,\n",
    "        retrieval_time=retrieval_time,\n",
    "        generation_time=generation_time,\n",
    "        num_docs=len(docs),\n",
    "        answer_length=len(answer)\n",
    "    )\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with monitoring\n",
    "test_questions = [\n",
    "    \"What is deep learning?\",\n",
    "    \"Explain computer vision\",\n",
    "    \"What are ethical considerations in AI?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    answer = monitored_rag_query(\n",
    "        q, \n",
    "        vectorstore.as_retriever(),\n",
    "        ChatOpenAI(temperature=0)\n",
    "    )\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer[:100]}...\\n\")\n",
    "\n",
    "# Display performance stats\n",
    "stats = monitor.get_stats()\n",
    "print(\"\\nPerformance Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if \"time\" in key:\n",
    "        print(f\"{key}: {value:.3f}s\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 3: Build and Optimize a Production RAG System\n",
    "\n",
    "Create a production-ready RAG system with evaluation and monitoring.\n",
    "\n",
    "**Task**: Build a complete RAG system for a company knowledge base with optimization and monitoring.\n",
    "\n",
    "Requirements:\n",
    "1. Implement automatic chunk size optimization\n",
    "2. Add query result caching for common questions\n",
    "3. Build evaluation metrics for quality assurance\n",
    "4. Create performance monitoring with alerts\n",
    "5. Implement fallback strategies for poor retrieval\n",
    "6. Add user feedback collection mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_3_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your production RAG system\n",
    "\n",
    "class ProductionRAGSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize your system components\n",
    "        pass\n",
    "    \n",
    "    # TODO: Implement automatic optimization\n",
    "    def auto_optimize(self, sample_queries: List[str]):\n",
    "        # Test different configurations\n",
    "        # Find best settings\n",
    "        pass\n",
    "    \n",
    "    # TODO: Implement smart caching\n",
    "    def cached_query(self, question: str):\n",
    "        # Check cache with similarity threshold\n",
    "        # Return cached or compute new\n",
    "        pass\n",
    "    \n",
    "    # TODO: Add quality checks\n",
    "    def quality_check(self, question: str, answer: str, context: str) -> Dict:\n",
    "        # Check answer quality\n",
    "        # Return quality metrics\n",
    "        pass\n",
    "    \n",
    "    # TODO: Implement monitoring with alerts\n",
    "    def monitor_performance(self):\n",
    "        # Track latency, errors, quality\n",
    "        # Send alerts if thresholds exceeded\n",
    "        pass\n",
    "    \n",
    "    # TODO: Add fallback strategies\n",
    "    def query_with_fallback(self, question: str):\n",
    "        # Try primary retrieval\n",
    "        # If quality low, try alternative strategies\n",
    "        # If still poor, escalate to human\n",
    "        pass\n",
    "    \n",
    "    # TODO: Collect user feedback\n",
    "    def collect_feedback(self, question: str, answer: str, rating: int, comment: str = None):\n",
    "        # Store feedback\n",
    "        # Use for continuous improvement\n",
    "        pass\n",
    "\n",
    "# TODO: Create and test your system\n",
    "# Your implementation here\n",
    "\n",
    "# Test scenarios\n",
    "test_scenarios = [\n",
    "    \"Normal query\",\n",
    "    \"Repeated query (should cache)\",\n",
    "    \"Query with no good matches (should fallback)\",\n",
    "    \"Complex multi-part query\"\n",
    "]\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_3_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "import hashlib\n",
    "from collections import deque\n",
    "\n",
    "class ProductionRAGSystem:\n",
    "    def __init__(self, documents: List):\n",
    "        self.documents = documents\n",
    "        self.llm = ChatOpenAI(temperature=0)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vectorstore = None\n",
    "        self.cache = {}\n",
    "        self.performance_log = deque(maxlen=100)  # Keep last 100 queries\n",
    "        self.feedback_log = []\n",
    "        self.optimal_chunk_size = 200\n",
    "        \n",
    "        # Quality thresholds\n",
    "        self.min_relevance_score = 6.0\n",
    "        self.latency_threshold = 5.0  # seconds\n",
    "    \n",
    "    def auto_optimize(self, sample_queries: List[str]):\n",
    "        '''Test different configurations to find optimal settings'''\n",
    "        print(\"Starting auto-optimization...\")\n",
    "        \n",
    "        configurations = [\n",
    "            {\"chunk_size\": 150, \"overlap\": 30, \"k\": 3},\n",
    "            {\"chunk_size\": 200, \"overlap\": 50, \"k\": 4},\n",
    "            {\"chunk_size\": 300, \"overlap\": 75, \"k\": 5}\n",
    "        ]\n",
    "        \n",
    "        best_config = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for config in configurations:\n",
    "            # Create vector store with config\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=config[\"chunk_size\"],\n",
    "                chunk_overlap=config[\"overlap\"]\n",
    "            )\n",
    "            splits = splitter.split_documents(self.documents)\n",
    "            \n",
    "            test_vectorstore = Chroma.from_documents(\n",
    "                documents=splits,\n",
    "                embedding=self.embeddings,\n",
    "                collection_name=f\"test_{config['chunk_size']}\"\n",
    "            )\n",
    "            \n",
    "            # Evaluate with sample queries\n",
    "            scores = []\n",
    "            for query in sample_queries:\n",
    "                docs = test_vectorstore.as_retriever(\n",
    "                    search_kwargs={\"k\": config[\"k\"]}\n",
    "                ).get_relevant_documents(query)\n",
    "                \n",
    "                # Simple relevance check\n",
    "                if docs:\n",
    "                    scores.append(len(docs) / config[\"k\"])\n",
    "            \n",
    "            avg_score = np.mean(scores) if scores else 0\n",
    "            print(f\"Config {config}: Score {avg_score:.2f}\")\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_config = config\n",
    "        \n",
    "        # Apply best configuration\n",
    "        if best_config:\n",
    "            self.optimal_chunk_size = best_config[\"chunk_size\"]\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=best_config[\"chunk_size\"],\n",
    "                chunk_overlap=best_config[\"overlap\"]\n",
    "            )\n",
    "            splits = splitter.split_documents(self.documents)\n",
    "            self.vectorstore = Chroma.from_documents(\n",
    "                documents=splits,\n",
    "                embedding=self.embeddings,\n",
    "                collection_name=\"production\"\n",
    "            )\n",
    "            print(f\"\\nOptimization complete. Best config: {best_config}\")\n",
    "    \n",
    "    def _cache_key(self, question: str) -> str:\n",
    "        '''Generate cache key for question'''\n",
    "        return hashlib.md5(question.lower().encode()).hexdigest()\n",
    "    \n",
    "    def cached_query(self, question: str):\n",
    "        '''Query with intelligent caching'''\n",
    "        # Check exact match cache\n",
    "        cache_key = self._cache_key(question)\n",
    "        if cache_key in self.cache:\n",
    "            print(\"[Cache Hit] Returning cached result\")\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Check for similar questions in cache\n",
    "        for cached_q, cached_answer in self.cache.items():\n",
    "            # Simple similarity check (could use embeddings)\n",
    "            if len(set(question.lower().split()) & set(cached_q.lower().split())) > 3:\n",
    "                print(\"[Partial Cache Hit] Found similar question\")\n",
    "                return cached_answer\n",
    "        \n",
    "        # Compute new answer\n",
    "        answer = self.query_with_fallback(question)\n",
    "        self.cache[cache_key] = answer\n",
    "        \n",
    "        # Limit cache size\n",
    "        if len(self.cache) > 100:\n",
    "            # Remove oldest entries\n",
    "            oldest_key = list(self.cache.keys())[0]\n",
    "            del self.cache[oldest_key]\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def quality_check(self, question: str, answer: str, docs: List) -> Dict:\n",
    "        '''Evaluate answer quality'''\n",
    "        # Check relevance\n",
    "        relevance_prompt = ChatPromptTemplate.from_template(\n",
    "            '''Rate how well this answer addresses the question (0-10):\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            Score:'''\n",
    "        )\n",
    "        \n",
    "        chain = relevance_prompt | self.llm | StrOutputParser()\n",
    "        relevance_score = chain.invoke({\"question\": question, \"answer\": answer})\n",
    "        \n",
    "        try:\n",
    "            relevance = float(relevance_score.strip())\n",
    "        except:\n",
    "            relevance = 0\n",
    "        \n",
    "        # Check groundedness\n",
    "        context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        grounded = len(set(answer.split()) & set(context.split())) / len(answer.split())\n",
    "        \n",
    "        return {\n",
    "            \"relevance\": relevance,\n",
    "            \"groundedness\": grounded,\n",
    "            \"confidence\": (relevance / 10) * grounded,\n",
    "            \"passed\": relevance >= self.min_relevance_score\n",
    "        }\n",
    "    \n",
    "    def monitor_performance(self, query_data: Dict):\n",
    "        '''Monitor and alert on performance issues'''\n",
    "        self.performance_log.append(query_data)\n",
    "        \n",
    "        # Check recent performance\n",
    "        recent_latencies = [d[\"latency\"] for d in self.performance_log if \"latency\" in d]\n",
    "        recent_qualities = [d[\"quality\"][\"relevance\"] for d in self.performance_log if \"quality\" in d]\n",
    "        \n",
    "        # Alert conditions\n",
    "        if recent_latencies and np.mean(recent_latencies[-10:]) > self.latency_threshold:\n",
    "            print(\"⚠️  ALERT: High latency detected!\")\n",
    "            print(f\"   Average latency: {np.mean(recent_latencies[-10:]):.2f}s\")\n",
    "        \n",
    "        if recent_qualities and np.mean(recent_qualities[-10:]) < self.min_relevance_score:\n",
    "            print(\"⚠️  ALERT: Low quality responses detected!\")\n",
    "            print(f\"   Average quality: {np.mean(recent_qualities[-10:]):.1f}/10\")\n",
    "    \n",
    "    def query_with_fallback(self, question: str):\n",
    "        '''Query with multiple fallback strategies'''\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if not self.vectorstore:\n",
    "            return \"System not initialized. Please run auto_optimize first.\"\n",
    "        \n",
    "        # Strategy 1: Standard retrieval\n",
    "        retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "        \n",
    "        if docs:\n",
    "            # Generate answer\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "            prompt = ChatPromptTemplate.from_template(\n",
    "                '''Answer the question based on the context.\n",
    "                Context: {context}\n",
    "                Question: {question}\n",
    "                Answer:'''\n",
    "            )\n",
    "            \n",
    "            chain = prompt | self.llm | StrOutputParser()\n",
    "            answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "            \n",
    "            # Quality check\n",
    "            quality = self.quality_check(question, answer, docs)\n",
    "            \n",
    "            if quality[\"passed\"]:\n",
    "                # Log performance\n",
    "                self.monitor_performance({\n",
    "                    \"question\": question,\n",
    "                    \"latency\": time.time() - start_time,\n",
    "                    \"quality\": quality,\n",
    "                    \"strategy\": \"standard\"\n",
    "                })\n",
    "                return answer\n",
    "        \n",
    "        # Strategy 2: Expanded search\n",
    "        print(\"[Fallback] Trying expanded search...\")\n",
    "        retriever_expanded = self.vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "        docs = retriever_expanded.get_relevant_documents(question)\n",
    "        \n",
    "        if docs:\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs[:4]])\n",
    "            answer = f\"Based on expanded search: {context[:200]}...\"\n",
    "            return answer\n",
    "        \n",
    "        # Strategy 3: Escalate to human\n",
    "        print(\"[Fallback] No good matches found. Escalating...\")\n",
    "        return '''I couldn't find sufficient information to answer your question confidently. \n",
    "        This has been flagged for human review. In the meantime, you may want to:\n",
    "        1. Try rephrasing your question\n",
    "        2. Contact support directly at support@company.com'''\n",
    "    \n",
    "    def collect_feedback(self, question: str, answer: str, rating: int, comment: str = None):\n",
    "        '''Collect and store user feedback'''\n",
    "        feedback = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"question\": question,\n",
    "            \"answer\": answer[:200],  # Store preview\n",
    "            \"rating\": rating,\n",
    "            \"comment\": comment\n",
    "        }\n",
    "        \n",
    "        self.feedback_log.append(feedback)\n",
    "        \n",
    "        # Analyze feedback trends\n",
    "        recent_ratings = [f[\"rating\"] for f in self.feedback_log[-20:]]\n",
    "        if recent_ratings:\n",
    "            avg_rating = np.mean(recent_ratings)\n",
    "            if avg_rating < 3:\n",
    "                print(f\"⚠️  Low user satisfaction: {avg_rating:.1f}/5\")\n",
    "                print(\"   Consider retraining or updating knowledge base\")\n",
    "        \n",
    "        return f\"Thank you for your feedback! (Rating: {rating}/5)\"\n",
    "\n",
    "# Create and test the system\n",
    "print(\"Creating Production RAG System...\")\n",
    "prod_system = ProductionRAGSystem(documents)\n",
    "\n",
    "# Auto-optimize\n",
    "sample_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"What are AI ethics?\"\n",
    "]\n",
    "prod_system.auto_optimize(sample_queries)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Production System:\\n\")\n",
    "\n",
    "# Test different scenarios\n",
    "test_cases = [\n",
    "    \"What are the types of machine learning?\",\n",
    "    \"What are the types of machine learning?\",  # Repeated (should cache)\n",
    "    \"Explain quantum computing\",  # Not in knowledge base\n",
    "    \"How do CNNs work in computer vision?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_cases):\n",
    "    print(f\"\\nTest {i+1}: {question}\")\n",
    "    answer = prod_system.cached_query(question)\n",
    "    print(f\"Answer: {answer[:150]}...\")\n",
    "    \n",
    "    # Simulate user feedback\n",
    "    rating = 4 if \"couldn't find\" not in answer else 2\n",
    "    feedback_response = prod_system.collect_feedback(\n",
    "        question, answer, rating, \"Good answer\" if rating > 3 else \"Needs improvement\"\n",
    "    )\n",
    "    print(feedback_response)\n",
    "    print(\"-\" * 40)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build a complete production RAG system with optimization and monitoring!\")\n",
    "print(\"The solution includes auto-optimization, caching, quality checks, and feedback collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've learned how to build complete RAG systems. You can now:\n",
    "\n",
    "✅ Build end-to-end RAG pipelines from documents to answers\n",
    "✅ Implement advanced retrieval strategies (multi-query, compression, hybrid)\n",
    "✅ Use metadata filtering for targeted retrieval\n",
    "✅ Evaluate and optimize RAG performance\n",
    "✅ Monitor production RAG systems\n",
    "✅ Implement fallback strategies and quality checks\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Retrieval Quality Matters**: Good retrieval is crucial for good answers\n",
    "- **Multiple Strategies**: Combine different retrieval approaches for robustness\n",
    "- **Evaluation is Essential**: Measure relevance, faithfulness, and performance\n",
    "- **Optimization**: Chunk size, reranking, and caching improve results\n",
    "- **Production Considerations**: Monitor, collect feedback, and iterate\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 08**: Learn about Tools and Agents for autonomous AI systems\n",
    "- **Practice**: Build RAG systems for different domains\n",
    "- **Experiment**: Try different embedding models and vector stores\n",
    "- **Scale**: Implement distributed RAG for large knowledge bases\n",
    "\n",
    "### Additional Challenges:\n",
    "1. Implement RAG with multiple data sources (PDFs, websites, databases)\n",
    "2. Build a conversational RAG with memory\n",
    "3. Create a RAG system with real-time document updates\n",
    "4. Implement semantic caching with embedding similarity\n",
    "5. Build a multi-lingual RAG system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}