{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **Introduction to LangChain & Environment Setup**\n\n## **Learning Objectives**\nBy the end of this notebook, you will be able to:\n- Understand what LangChain is and why it's essential for building LLM applications\n- Set up your development environment with all necessary dependencies\n- Make your first LangChain API calls\n- Understand the core components of the LangChain ecosystem\n- Navigate the LangChain documentation and resources\n\n## **Why This Matters: Real-World AI Applications**\n\n**In AI Systems:**\n- LangChain provides a unified interface for working with multiple LLM providers\n- Enables rapid prototyping and production deployment of AI applications\n- Reduces boilerplate code and complexity\n\n**In RAG Pipelines:**\n- Standardizes document loading, splitting, and embedding generation\n- Provides pre-built components for vector stores and retrieval\n- Simplifies the integration of knowledge bases with LLMs\n\n**In Agentic AI:**\n- Offers tools and frameworks for building autonomous agents\n- Manages agent memory and conversation state\n- Enables complex multi-step reasoning and tool usage\n\n## **Prerequisites**\n- Python 3.10+ installed\n- Basic Python knowledge (variables, functions, loops)\n- Understanding of APIs and JSON\n- OpenAI API key (free tier works)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Setup: Install Dependencies**\n\nRun this cell first to install all required packages:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core LangChain packages\n",
    "!pip install -q langchain langchain-openai langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 1: The Problem LangChain Solves**\n\n**Concept**: Understanding why LangChain exists by comparing development with and without it.\n\n### **Example 1: Building a Simple Q&A System Without LangChain**\n\n**Problem**: Create a system that answers questions using an LLM\n**Expected Output**: Complex, vendor-specific code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport openai\nimport anthropic\n\n# **Without LangChain - vendor-specific implementations**\ndef answer_with_openai(question):\n    \"\"\"OpenAI-specific implementation\"\"\"\n    client = openai.OpenAI()\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": question}\n        ]\n    )\n    return response.choices[0].message.content\n\ndef answer_with_anthropic(question):\n    \"\"\"Anthropic-specific implementation\"\"\"\n    client = anthropic.Anthropic()\n    message = client.messages.create(\n        model=\"claude-3-5-sonnet-20241022\",\n        max_tokens=1000,\n        messages=[\n            {\"role\": \"user\", \"content\": question}\n        ]\n    )\n    return message.content[0].text\n\n# **Notice: Different APIs, different response structures, vendor lock-in**\nprint(\"Without LangChain: Multiple implementations needed!\")\n```\n\n**Why this is problematic:**\n- Different API patterns for each provider\n- Vendor lock-in makes switching providers difficult\n- No standardized way to handle prompts, memory, or tools\n- Lots of boilerplate code for common patterns\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: The Same System WITH LangChain**\n\n**Problem**: Create the same Q&A system using LangChain\n**Expected Output**: Clean, provider-agnostic code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\n\n# **With LangChain - unified interface**\ndef answer_with_langchain(question, provider=\"openai\"):\n    \"\"\"Provider-agnostic implementation\"\"\"\n    \n    # Switch providers with one line\n    if provider == \"openai\":\n        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    elif provider == \"anthropic\":\n        llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n    \n    # Same interface for all providers!\n    response = llm.invoke(question)\n    return response.content\n\n# **Example usage**\nquestion = \"What is LangChain?\"\nprint(answer_with_langchain(question))\n```\n\n**Why this works better:**\n- One interface for all LLM providers\n- Easy to switch between providers\n- Consistent response format\n- Less code, more maintainable\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: LangChain Ecosystem Components**\n\n**Problem**: Understand what components LangChain provides\n**Expected Output**: Overview of major components"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **LangChain provides these core components:**\n\n# **1. Models - Interface with LLMs**\nfrom langchain_openai import ChatOpenAI\n\n# **2. Prompts - Manage and optimize prompts**\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# **3. Chains - Connect components**\nfrom langchain_core.runnables import RunnablePassthrough\n\n# **4. Memory - Maintain conversation state**\nfrom langchain.memory import ConversationBufferMemory\n\n# **5. Document Loaders - Load various data sources**\nfrom langchain_community.document_loaders import PyPDFLoader\n\n# **6. Vector Stores - Semantic search**\nfrom langchain_community.vectorstores import Chroma\n\n# **7. Agents - Autonomous decision-making**\nfrom langchain.agents import create_react_agent\n\n# **8. Tools - Give LLMs capabilities**\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\nprint(\"LangChain Components Overview:\")\nprint(\"- Models: Interface with any LLM\")\nprint(\"- Prompts: Template and manage prompts\")\nprint(\"- Chains: Build complex workflows\")\nprint(\"- Memory: Maintain conversation context\")\nprint(\"- Document Processing: Load and split documents\")\nprint(\"- Vector Stores: Semantic search and retrieval\")\nprint(\"- Agents: Build autonomous AI systems\")\nprint(\"- Tools: Extend LLM capabilities\")\n```\n\n**Why this ecosystem matters:**\n- Pre-built components for common AI patterns\n- Everything works together seamlessly\n- Battle-tested in production\n- Active community and continuous updates\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 1: Understanding the Value of LangChain**\n\n**Practice**: Explore why LangChain is valuable for AI development\n\n### **Exercise 1: Identify the Problems**\n\n**Task**: List 3 problems you might face when building AI applications without a framework\n**Expected Output**: A list of challenges"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Think about: What would be difficult about building a chatbot from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Common problems without a framework like LangChain:**\n\nproblems = [\n    \"1. Vendor lock-in: Each LLM provider has different APIs\",\n    \"2. Boilerplate code: Lots of repeated code for common patterns\",\n    \"3. Complex integrations: Connecting documents, memory, and tools is difficult\",\n    \"4. No standardization: Every developer reinvents the wheel\",\n    \"5. Production challenges: Error handling, retries, streaming all custom-built\"\n]\n\nfor problem in problems:\n    print(problem)\n\nprint(\"\\nLangChain solves all these problems with pre-built, tested components!\")\n```\n\n**Why these matter:**\n- Real teams face these issues daily\n- LangChain's standardization saves months of development\n- Focus on your application logic, not infrastructure\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Component Matching**\n\n**Task**: Match LangChain components to real-world use cases\n**Expected Output**: Correct component for each scenario"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Match each use case to a LangChain component:\n",
    "\n",
    "use_cases = {\n",
    "    \"Building a PDF Q&A system\": \"?\",\n",
    "    \"Remembering user preferences in a chat\": \"?\",\n",
    "    \"Searching the web for current information\": \"?\",\n",
    "    \"Finding similar documents\": \"?\",\n",
    "    \"Creating dynamic prompts with variables\": \"?\"\n",
    "}\n",
    "\n",
    "# Fill in the ? with the correct component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Matching LangChain components to use cases**\n\nuse_cases = {\n    \"Building a PDF Q&A system\": \"Document Loaders + Vector Stores\",\n    \"Remembering user preferences in a chat\": \"Memory components\",\n    \"Searching the web for current information\": \"Tools (like DuckDuckGo search)\",\n    \"Finding similar documents\": \"Vector Stores + Embeddings\",\n    \"Creating dynamic prompts with variables\": \"Prompt Templates\"\n}\n\nprint(\"LangChain Component Matching:\")\nprint(\"=\" * 50)\nfor use_case, component in use_cases.items():\n    print(f\"Use Case: {use_case}\")\n    print(f\"Component: {component}\\n\")\n```\n\n**Why these matches work:**\n- Each component is purpose-built for specific AI patterns\n- Components can be combined for complex applications\n- Pre-built solutions for common use cases\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 2: Setting Up Your Environment**\n\n**Concept**: Properly configure your development environment for LangChain\n\n### **Example 1: Environment Variables Setup**\n\n**Problem**: Securely manage API keys\n**Expected Output**: Loaded environment variables"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\nfrom dotenv import load_dotenv\n\n# **Method 1: Using .env file (recommended for development)**\nload_dotenv()  # Loads from .env file\n\n# **Method 2: Direct setting (for Google Colab or testing)**\n# **Uncomment and add your key if not using .env file**\n# **os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"**\n\n# **Verify key is loaded (without revealing it)**\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif api_key:\n    # Mask the key for security\n    masked = api_key[:8] + \"...\" + api_key[-4:] if len(api_key) > 12 else \"***\"\n    print(f\"‚úÖ OpenAI API key loaded: {masked}\")\nelse:\n    print(\"‚ùå No API key found. Please set OPENAI_API_KEY\")\n    print(\"Get your key from: https://platform.openai.com/api-keys\")\n```\n\n**Why environment variables matter:**\n- Keep secrets out of code\n- Different keys for dev/prod\n- Industry best practice for security\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Verifying LangChain Installation**\n\n**Problem**: Ensure all components are properly installed\n**Expected Output**: Version information and successful imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Check LangChain installation**\nimport langchain\nimport langchain_core\nimport langchain_openai\nimport langchain_community\n\nprint(\"LangChain Installation Check:\")\nprint(\"=\" * 40)\nprint(f\"‚úÖ langchain: {langchain.__version__}\")\nprint(f\"‚úÖ langchain-core: {langchain_core.__version__}\")\nprint(f\"‚úÖ langchain-openai: {langchain_openai.__version__}\")\nprint(f\"‚úÖ langchain-community: {langchain_community.__version__}\")\n\n# **Test critical imports**\ntry:\n    from langchain_openai import ChatOpenAI\n    from langchain_core.prompts import ChatPromptTemplate\n    from langchain_core.output_parsers import StrOutputParser\n    print(\"\\n‚úÖ All critical imports successful!\")\nexcept ImportError as e:\n    print(f\"\\n‚ùå Import error: {e}\")\n    print(\"Run: pip install langchain langchain-openai langchain-community\")\n```\n\n**Why verification is important:**\n- Catch issues early\n- Ensure compatibility between packages\n- Confirm environment is ready for development\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 2: Set Up Your Environment**\n\n**Practice**: Configure your development environment\n\n### **Exercise 1: Create and Load Environment Variables**\n\n**Task**: Set up your API key securely\n**Expected Output**: Confirmation that key is loaded"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Import necessary modules and load your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport os\nfrom dotenv import load_dotenv\n\n# **Load environment variables**\nload_dotenv()\n\n# **For Google Colab users (uncomment if needed):**\n# **import getpass**\n# **os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")**\n\n# **Check if key is loaded**\nif os.getenv(\"OPENAI_API_KEY\"):\n    print(\"‚úÖ API key successfully loaded!\")\n    print(\"You're ready to use LangChain with OpenAI\")\nelse:\n    print(\"‚ö†Ô∏è No API key found\")\n    print(\"Please set OPENAI_API_KEY in your .env file or environment\")\n```\n\n**Why this works:**\n- `load_dotenv()` reads from .env file\n- `os.getenv()` safely retrieves environment variables\n- Never hardcode API keys in your code\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Verify Your Installation**\n\n**Task**: Check that all required packages are installed\n**Expected Output**: List of installed packages with versions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Import packages and print their versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Check all required packages**\npackages_to_check = [\n    (\"langchain\", \"LangChain Core\"),\n    (\"langchain_core\", \"LangChain Core Components\"),\n    (\"langchain_openai\", \"OpenAI Integration\"),\n    (\"langchain_community\", \"Community Integrations\"),\n    (\"dotenv\", \"Environment Management\")\n]\n\nprint(\"Package Installation Status:\")\nprint(\"=\" * 40)\n\nfor package_name, description in packages_to_check:\n    try:\n        package = __import__(package_name)\n        version = getattr(package, \"__version__\", \"installed\")\n        print(f\"‚úÖ {description}: {version}\")\n    except ImportError:\n        print(f\"‚ùå {description}: Not installed\")\n\nprint(\"\\nIf any packages are missing, run:\")\nprint(\"pip install langchain langchain-openai langchain-community python-dotenv\")\n```\n\n**Why this verification helps:**\n- Ensures all dependencies are present\n- Catches version mismatches early\n- Provides clear fix instructions if needed\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 3: Your First LangChain Call**\n\n**Concept**: Make your first successful API call with LangChain\n\n### **Example 1: Basic LLM Initialization**\n\n**Problem**: Create and configure an LLM instance\n**Expected Output**: Configured ChatOpenAI object"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# **Initialize the LLM with configuration**\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",      # Efficient, cost-effective model\n    temperature=0.7,          # Balance between creativity and consistency\n    max_tokens=150,          # Limit response length\n    timeout=30,              # API timeout in seconds\n    max_retries=2,           # Retry on failure\n)\n\n# **Display configuration**\nprint(\"LLM Configuration:\")\nprint(f\"Model: {llm.model_name}\")\nprint(f\"Temperature: {llm.temperature}\")\nprint(f\"Max Tokens: {llm.max_tokens}\")\nprint(\"\\n‚úÖ LLM initialized and ready!\")\n```\n\n**Why these settings matter:**\n- **model**: gpt-4o-mini is fast and affordable\n- **temperature**: 0 = deterministic, 1 = creative\n- **max_tokens**: Controls response length and cost\n- **timeout/retries**: Production reliability\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Your First API Call**\n\n**Problem**: Make a simple call to the LLM\n**Expected Output**: AI-generated response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# **Initialize LLM**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\n# **Make your first call!**\nresponse = llm.invoke(\"Hello LangChain! Tell me a fun fact about AI in one sentence.\")\n\n# **Display the response**\nprint(\"üéâ Your First LangChain Response:\")\nprint(\"=\" * 50)\nprint(response.content)\nprint(\"=\" * 50)\nprint(\"\\nüöÄ Congratulations! You've made your first LangChain call!\")\n```\n\n**Why this is significant:**\n- You've successfully connected to an LLM\n- The same code works with any LLM provider\n- Foundation for all future LangChain applications\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Understanding the Response Object**\n\n**Problem**: Explore what the LLM returns\n**Expected Output**: Response object details"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# **Initialize and call LLM**\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nresponse = llm.invoke(\"What is Python?\")\n\n# **Explore the response object**\nprint(\"Response Object Analysis:\")\nprint(\"=\" * 50)\nprint(f\"Type: {type(response)}\")\nprint(f\"\\nContent: {response.content[:100]}...\")\nprint(f\"\\nResponse metadata:\")\nprint(f\"- Token usage: {response.response_metadata.get('token_usage', 'N/A')}\")\nprint(f\"- Model: {response.response_metadata.get('model_name', 'N/A')}\")\nprint(f\"- Finish reason: {response.response_metadata.get('finish_reason', 'N/A')}\")\n\n# **Access just the text content**\ntext_content = response.content\nprint(f\"\\nJust the text (first 200 chars):\")\nprint(text_content[:200])\n```\n\n**Why understanding responses matters:**\n- Access token usage for cost tracking\n- Check finish_reason for truncation\n- Extract metadata for logging/debugging\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 3: Make Your First LangChain Calls**\n\n**Practice**: Create and use your own LLM instances\n\n### **Exercise 1: Initialize Your LLM**\n\n**Task**: Create a ChatOpenAI instance with custom settings\n**Expected Output**: Configured LLM ready to use"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a ChatOpenAI instance with:\n",
    "# - model: gpt-4o-mini\n",
    "# - temperature: 0.5\n",
    "# - max_tokens: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# **Create your LLM instance**\nmy_llm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.5,      # Balanced creativity\n    max_tokens=100       # Keep responses concise\n)\n\n# **Verify configuration**\nprint(\"Your LLM Configuration:\")\nprint(f\"‚úÖ Model: {my_llm.model_name}\")\nprint(f\"‚úÖ Temperature: {my_llm.temperature}\")\nprint(f\"‚úÖ Max Tokens: {my_llm.max_tokens}\")\nprint(\"\\nYour LLM is ready to use!\")\n```\n\n**Why these settings:**\n- Temperature 0.5 balances consistency with creativity\n- Max tokens 100 keeps responses focused\n- gpt-4o-mini is perfect for learning\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Make Your First Call**\n\n**Task**: Ask the LLM to explain LangChain in simple terms\n**Expected Output**: Simple explanation of LangChain"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Use your LLM to get an explanation of LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# **Initialize your LLM**\nmy_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n\n# **Make your first call**\nprompt = \"Explain LangChain to me like I'm a beginner programmer in 2-3 sentences.\"\nresponse = my_llm.invoke(prompt)\n\n# **Display the response**\nprint(\"üéì LangChain Explained Simply:\")\nprint(\"=\" * 50)\nprint(response.content)\nprint(\"=\" * 50)\nprint(\"\\n‚úÖ Success! You've used LangChain to get AI-generated content!\")\n```\n\n**What just happened:**\n- You sent a prompt to an LLM\n- LangChain handled all the API complexity\n- You received structured response data\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 3: Experiment with Parameters**\n\n**Task**: Compare outputs with different temperature settings\n**Expected Output**: See how temperature affects responses"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create two LLMs with temperature 0 and 1\n",
    "# Ask both the same creative question\n",
    "# Compare the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# **Create two LLMs with different temperatures**\ndeterministic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\ncreative_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n\n# **Same prompt for both**\nprompt = \"Invent a creative name for a AI-powered coffee shop.\"\n\n# **Get responses**\ndeterministic_response = deterministic_llm.invoke(prompt)\ncreative_response = creative_llm.invoke(prompt)\n\n# **Compare outputs**\nprint(\"Temperature Comparison:\")\nprint(\"=\" * 50)\nprint(f\"Temperature 0 (Deterministic): {deterministic_response.content}\")\nprint()\nprint(f\"Temperature 1 (Creative): {creative_response.content}\")\nprint(\"=\" * 50)\nprint(\"\\nüí° Notice how temperature affects creativity and consistency!\")\n```\n\n**Key insight:**\n- Temperature 0: Consistent, predictable outputs\n- Temperature 1: More creative, varied outputs\n- Choose based on your use case needs\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Optional Extra Practice**\n\n**Challenge yourself with these exercises that combine all concepts**\n\n### **Challenge 1: Build a Simple Q&A Function**\n\n**Task**: Create a reusable function that answers questions\n**Expected Output**: A function that takes questions and returns answers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\nimport os\n\ndef create_qa_assistant(model=\"gpt-4o-mini\", temperature=0.7):\n    \"\"\"Create a Q&A assistant using LangChain\"\"\"\n    \n    # Initialize LLM\n    llm = ChatOpenAI(model=model, temperature=temperature)\n    \n    def ask_question(question):\n        \"\"\"Ask a question and get an answer\"\"\"\n        try:\n            # Add context to make answers more helpful\n            enhanced_prompt = f\"\"\"Please provide a clear, helpful answer to this question:\n            \n            Question: {question}\n            \n            Answer in a friendly, informative way.\"\"\"\n            \n            response = llm.invoke(enhanced_prompt)\n            return response.content\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n    \n    return ask_question\n\n# **Create and test the assistant**\nqa = create_qa_assistant(temperature=0.7)\n\n# **Test with different questions**\nquestions = [\n    \"What is machine learning?\",\n    \"How do I get started with Python?\",\n    \"What are the benefits of using LangChain?\"\n]\n\nfor q in questions:\n    print(f\"Q: {q}\")\n    print(f\"A: {qa(q)[:200]}...\\n\")  # Show first 200 chars\n```\n\n**Why this pattern is useful:**\n- Encapsulates LLM logic\n- Reusable across your application\n- Easy to test and maintain\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Challenge 2: Multi-Provider Comparison**\n\n**Task**: Create a function that can use different LLM providers\n**Expected Output**: Same interface for different providers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import ChatOpenAI\n# **from langchain_anthropic import ChatAnthropic  # If you have Anthropic key**\n# **from langchain_google_genai import ChatGoogleGenerativeAI  # If you have Google key**\n\ndef create_multi_provider_llm(provider=\"openai\", model=None, temperature=0.7):\n    \"\"\"Create LLM instance for different providers\"\"\"\n    \n    if provider == \"openai\":\n        model = model or \"gpt-4o-mini\"\n        return ChatOpenAI(model=model, temperature=temperature)\n    \n    # Uncomment if you have other API keys:\n    # elif provider == \"anthropic\":\n    #     model = model or \"claude-3-5-sonnet-20241022\"\n    #     return ChatAnthropic(model=model, temperature=temperature)\n    \n    # elif provider == \"google\":\n    #     model = model or \"gemini-2.5-flash\"\n    #     return ChatGoogleGenerativeAI(model=model, temperature=temperature)\n    \n    else:\n        raise ValueError(f\"Unknown provider: {provider}\")\n\n# **Test with available providers**\nproviders_to_test = [\"openai\"]  # Add others if you have keys\n\nprompt = \"What makes Python a great programming language? (Answer in one sentence)\"\n\nfor provider in providers_to_test:\n    try:\n        llm = create_multi_provider_llm(provider)\n        response = llm.invoke(prompt)\n        print(f\"{provider.upper()} Response:\")\n        print(f\"{response.content}\\n\")\n    except Exception as e:\n        print(f\"{provider.upper()} Error: {e}\\n\")\n\nprint(\"üí° Same interface, different providers - that's the power of LangChain!\")\n```\n\n**Why multi-provider support matters:**\n- Avoid vendor lock-in\n- Use best model for each task\n- Fallback options for reliability\n- Cost optimization\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Summary & Next Steps**\n\n### **What You've Learned**\n‚úÖ What LangChain is and why it's valuable  \n‚úÖ How to set up your development environment  \n‚úÖ Making your first LangChain API calls  \n‚úÖ Understanding LLM parameters and responses  \n‚úÖ The core components of the LangChain ecosystem  \n\n### **Key Takeaways**\n1. **LangChain simplifies AI development** by providing a unified interface\n2. **Environment setup is crucial** - always use environment variables for API keys\n3. **Temperature controls creativity** - adjust based on your use case\n4. **Same code, multiple providers** - avoid vendor lock-in\n5. **Foundation for complex apps** - these basics enable RAG, agents, and more\n\n### **What's Next?**\nIn the next notebook (`01_basic_llm_calls.ipynb`), you'll learn:\n- Working with different message types (System, Human, AI)\n- Building multi-turn conversations\n- Streaming responses for better UX\n- Error handling and retries\n- Cost optimization techniques\n\n### **Resources**\n- [LangChain Documentation](https://python.langchain.com/)\n- [OpenAI API Keys](https://platform.openai.com/api-keys)\n- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n- [Community Discord](https://discord.gg/langchain)\n\n---\n\nüéâ **Congratulations!** You've completed your introduction to LangChain! You're now ready to build AI-powered applications."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}