{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Handling in Python\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this section, you will be able to:\n",
    "- Read and write text files using Python's built-in file operations\n",
    "- Use context managers (with statement) for safe file handling\n",
    "- Construct cross-platform file paths using os.path.join\n",
    "- Handle file-related errors gracefully\n",
    "- Apply file operations to real-world AI/RAG/Agentic AI workflows\n",
    "\n",
    "## Why This Matters: Real-World AI/RAG/Agentic Applications\n",
    "\n",
    "**In AI Systems:**\n",
    "- Load training data and datasets from disk\n",
    "- Save model configurations, checkpoints, and predictions\n",
    "- Process batch files for inference pipelines\n",
    "- Log training metrics and experimental results\n",
    "\n",
    "**In RAG Pipelines:**\n",
    "- Load documents from various sources (PDFs, text files, markdown)\n",
    "- Cache embeddings and vector representations to disk\n",
    "- Save retrieved context and conversation history\n",
    "- Build document indexes from file directories\n",
    "- Store preprocessed chunks for faster retrieval\n",
    "\n",
    "**In Agentic AI:**\n",
    "- Read tool configurations and API credentials\n",
    "- Save agent conversation logs and decision traces\n",
    "- Load prompt templates from files\n",
    "- Store action results and intermediate outputs\n",
    "- Manage file-based state persistence across agent runs\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python syntax (variables, strings)\n",
    "- Understanding of functions\n",
    "- Basic knowledge of error handling (try/except)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Activity 1\n",
    "**Concept**: Basic file writing and reading operations\n",
    "\n",
    "### Example 1: Writing to a File\n",
    "\n",
    "**Problem**: Create a text file and write a simple message to it\n",
    "\n",
    "**Expected Output**: A file named `hello.txt` containing \"Hello, World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Open file in write mode ('w')\n",
    "# 'w' mode creates a new file or overwrites existing content\n",
    "file = open('hello.txt', 'w')\n",
    "\n",
    "# Write content to the file\n",
    "file.write('Hello, World!')\n",
    "\n",
    "# IMPORTANT: Always close the file to save changes\n",
    "file.close()\n",
    "\n",
    "print(\"File 'hello.txt' created successfully!\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `open(filename, mode)` opens a file connection\n",
    "- Mode `'w'` means \"write\" - creates file if it doesn't exist, overwrites if it does\n",
    "- `.write()` method writes string content to the file\n",
    "- `.close()` is crucial - it saves changes and frees system resources\n",
    "- Without closing, data might not be written to disk!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Reading from a File\n",
    "\n",
    "**Problem**: Read and display the content of the file we just created\n",
    "\n",
    "**Expected Output**: `\"Hello, World!\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Open file in read mode ('r')\n",
    "file = open('hello.txt', 'r')\n",
    "\n",
    "# Read the entire file content\n",
    "content = file.read()\n",
    "\n",
    "# Close the file\n",
    "file.close()\n",
    "\n",
    "print(\"File content:\", content)\n",
    "# Output: File content: Hello, World!\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Mode `'r'` means \"read\" - opens file for reading only\n",
    "- `.read()` reads the entire file content as a string\n",
    "- Always close files after reading to free resources\n",
    "- If file doesn't exist, you'll get a `FileNotFoundError`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Appending to a File\n",
    "\n",
    "**Problem**: Add a new line to the existing file without overwriting\n",
    "\n",
    "**Expected Output**: File contains both original content and new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Open file in append mode ('a')\n",
    "file = open('hello.txt', 'a')\n",
    "\n",
    "# Add a new line (\\n for newline character)\n",
    "file.write('\\nThis is a new line!')\n",
    "\n",
    "file.close()\n",
    "\n",
    "# Read and display the updated content\n",
    "file = open('hello.txt', 'r')\n",
    "content = file.read()\n",
    "file.close()\n",
    "\n",
    "print(\"Updated content:\")\n",
    "print(content)\n",
    "# Output:\n",
    "# Hello, World!\n",
    "# This is a new line!\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Mode `'a'` means \"append\" - adds content to the end without deleting existing data\n",
    "- `'\\n'` is the newline character - starts content on a new line\n",
    "- Unlike `'w'` mode, `'a'` mode preserves existing content\n",
    "- Perfect for logging or adding entries to files\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 1\n",
    "**Practice**: Basic file writing and reading operations\n",
    "\n",
    "### Exercise 1: Create a Shopping List\n",
    "\n",
    "**Task**: Write a list of items to a file named `shopping_list.txt`\n",
    "\n",
    "**Given**: `items = ['milk', 'bread', 'eggs', 'cheese']`\n",
    "\n",
    "**Expected Output**: File containing each item on a separate line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "items = ['milk', 'bread', 'eggs', 'cheese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "items = ['milk', 'bread', 'eggs', 'cheese']\n",
    "\n",
    "# Open file for writing\n",
    "file = open('shopping_list.txt', 'w')\n",
    "\n",
    "# Write each item on a new line\n",
    "for item in items:\n",
    "    file.write(item + '\\n')  # Add newline after each item\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(\"Shopping list saved to shopping_list.txt\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Loop through the list to write each item\n",
    "- Add `'\\n'` after each item to place them on separate lines\n",
    "- This creates a readable, line-by-line format\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Read and Count Lines\n",
    "\n",
    "**Task**: Read the shopping list file and count how many items it contains\n",
    "\n",
    "**Expected Output**: `\"Total items: 4\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Open file for reading\n",
    "file = open('shopping_list.txt', 'r')\n",
    "\n",
    "# Read all lines into a list\n",
    "lines = file.readlines()  # readlines() returns a list of all lines\n",
    "\n",
    "file.close()\n",
    "\n",
    "# Count non-empty lines\n",
    "count = len([line for line in lines if line.strip()])  # strip() removes whitespace\n",
    "\n",
    "print(f\"Total items: {count}\")\n",
    "# Output: Total items: 4\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `.readlines()` returns a list where each element is a line from the file\n",
    "- `.strip()` removes leading/trailing whitespace (including `'\\n'`)\n",
    "- List comprehension filters out any empty lines\n",
    "- `len()` gives us the count of items\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Add Item to Shopping List\n",
    "\n",
    "**Task**: Append \"butter\" to the existing shopping list\n",
    "\n",
    "**Expected Output**: File now contains 5 items total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Open file in append mode\n",
    "file = open('shopping_list.txt', 'a')\n",
    "\n",
    "# Add new item\n",
    "file.write('butter\\n')\n",
    "\n",
    "file.close()\n",
    "\n",
    "# Verify by reading the file\n",
    "file = open('shopping_list.txt', 'r')\n",
    "content = file.read()\n",
    "file.close()\n",
    "\n",
    "print(\"Updated shopping list:\")\n",
    "print(content)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Append mode `'a'` adds content without removing existing items\n",
    "- Including `'\\n'` ensures proper line formatting\n",
    "- Reading afterward verifies the append operation worked\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 2\n",
    "**Concept**: Context managers for safe file handling (the `with` statement)\n",
    "\n",
    "### Example 1: Using `with` for Reading\n",
    "\n",
    "**Problem**: Read a file safely without worrying about closing it\n",
    "\n",
    "**Expected Output**: File content displayed, file automatically closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Using 'with' statement - file closes automatically!\n",
    "with open('hello.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(\"Content:\", content)\n",
    "\n",
    "# File is automatically closed here, even if an error occurs\n",
    "print(\"File closed:\", file.closed)  # Should print True\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- The `with` statement is a context manager - handles resource cleanup automatically\n",
    "- No need to call `.close()` explicitly\n",
    "- File closes automatically when the `with` block ends\n",
    "- Even if an error occurs inside the block, file still closes properly\n",
    "- This is the **Pythonic way** and **best practice** for file handling\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Using `with` for Writing\n",
    "\n",
    "**Problem**: Write data to a file safely\n",
    "\n",
    "**Expected Output**: Data written and file automatically closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Write data using context manager\n",
    "data = [\"Line 1\", \"Line 2\", \"Line 3\"]\n",
    "\n",
    "with open('output.txt', 'w') as file:\n",
    "    for line in data:\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "print(\"File written and closed automatically\")\n",
    "\n",
    "# Verify content\n",
    "with open('output.txt', 'r') as file:\n",
    "    print(file.read())\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `with` ensures file is properly closed and changes are saved\n",
    "- Prevents data loss from forgetting to close files\n",
    "- More concise and cleaner code\n",
    "- Industry standard for file operations\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Reading Line by Line (Memory Efficient)\n",
    "\n",
    "**Problem**: Read a large file without loading everything into memory\n",
    "\n",
    "**Expected Output**: Process each line individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Create a sample file first\n",
    "with open('numbers.txt', 'w') as file:\n",
    "    for i in range(1, 11):\n",
    "        file.write(f\"Line {i}\\n\")\n",
    "\n",
    "# Read and process line by line - memory efficient!\n",
    "line_count = 0\n",
    "with open('numbers.txt', 'r') as file:\n",
    "    for line in file:  # Iterates one line at a time\n",
    "        print(line.strip())  # strip() removes the newline character\n",
    "        line_count += 1\n",
    "\n",
    "print(f\"\\nProcessed {line_count} lines\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Iterating directly over the file object reads one line at a time\n",
    "- Very memory efficient - only one line in memory at once\n",
    "- Perfect for large files (GB+ size)\n",
    "- Essential for processing large datasets in AI/ML workflows\n",
    "- `.strip()` removes trailing newline characters\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 2\n",
    "**Practice**: Context managers for safe file handling\n",
    "\n",
    "### Exercise 1: Log Messages with Context Manager\n",
    "\n",
    "**Task**: Create a log file that records three timestamped messages using `with` statement\n",
    "\n",
    "**Expected Output**: File `app.log` with three log entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Use 'with' to create a log file with these messages:\n",
    "messages = [\n",
    "    \"Application started\",\n",
    "    \"User logged in\",\n",
    "    \"Data processed successfully\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    \"Application started\",\n",
    "    \"User logged in\",\n",
    "    \"Data processed successfully\"\n",
    "]\n",
    "\n",
    "# Write log entries using context manager\n",
    "with open('app.log', 'w') as file:\n",
    "    for i, message in enumerate(messages, 1):\n",
    "        file.write(f\"[Log {i}] {message}\\n\")\n",
    "\n",
    "print(\"Log file created\")\n",
    "\n",
    "# Read and display the log\n",
    "with open('app.log', 'r') as file:\n",
    "    print(\"\\nLog contents:\")\n",
    "    print(file.read())\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `enumerate(messages, 1)` provides both index and value, starting from 1\n",
    "- F-string formats each log entry with a number\n",
    "- `with` ensures file is properly closed after writing\n",
    "- Second `with` block safely reads back the log\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Process File Line by Line\n",
    "\n",
    "**Task**: Read `numbers.txt` and count how many lines contain the word \"Line\"\n",
    "\n",
    "**Expected Output**: Count of matching lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# First create numbers.txt with some content, then count lines containing \"Line\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Create sample file\n",
    "with open('numbers.txt', 'w') as file:\n",
    "    file.write(\"Line 1\\n\")\n",
    "    file.write(\"Line 2\\n\")\n",
    "    file.write(\"Something else\\n\")\n",
    "    file.write(\"Line 3\\n\")\n",
    "    file.write(\"Another thing\\n\")\n",
    "\n",
    "# Count lines containing \"Line\"\n",
    "count = 0\n",
    "with open('numbers.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        if \"Line\" in line:\n",
    "            count += 1\n",
    "\n",
    "print(f\"Lines containing 'Line': {count}\")\n",
    "# Output: Lines containing 'Line': 3\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Iterate line by line for memory efficiency\n",
    "- Use `in` operator to check if substring exists\n",
    "- Counter increments for each match\n",
    "- File automatically closes when done\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Multiple Operations with Context Manager\n",
    "\n",
    "**Task**: Read from one file, transform the content (uppercase), and write to another file\n",
    "\n",
    "**Expected Output**: New file with uppercase content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Read from 'hello.txt', convert to uppercase, write to 'hello_upper.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Read from source file\n",
    "with open('hello.txt', 'r') as source:\n",
    "    content = source.read()\n",
    "\n",
    "# Transform content to uppercase\n",
    "uppercase_content = content.upper()\n",
    "\n",
    "# Write to destination file\n",
    "with open('hello_upper.txt', 'w') as dest:\n",
    "    dest.write(uppercase_content)\n",
    "\n",
    "print(\"Content transformed and saved\")\n",
    "\n",
    "# Verify\n",
    "with open('hello_upper.txt', 'r') as file:\n",
    "    print(\"New content:\", file.read())\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Separate `with` blocks for reading and writing\n",
    "- Each file operation is safely handled\n",
    "- Transform happens between read and write\n",
    "- Pattern useful for data processing pipelines\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 3\n",
    "**Concept**: Safe path handling with `os.path.join` for cross-platform compatibility\n",
    "\n",
    "### Example 1: Understanding Path Issues\n",
    "\n",
    "**Problem**: Demonstrate why hardcoded paths can cause problems\n",
    "\n",
    "**Expected Output**: Understanding of platform-specific path separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# BAD: Hardcoded Windows-style path\n",
    "# This will FAIL on Mac/Linux!\n",
    "windows_path = \"data\\\\files\\\\input.txt\"  # Uses backslashes\n",
    "\n",
    "# BAD: Hardcoded Unix-style path\n",
    "# This might work but isn't portable\n",
    "unix_path = \"data/files/input.txt\"  # Uses forward slashes\n",
    "\n",
    "# GOOD: Using os.path.join - works everywhere!\n",
    "cross_platform_path = os.path.join(\"data\", \"files\", \"input.txt\")\n",
    "\n",
    "print(\"Windows path:\", windows_path)\n",
    "print(\"Unix path:\", unix_path)\n",
    "print(\"Cross-platform path:\", cross_platform_path)\n",
    "print(\"\\nYour system uses:\", os.sep)  # Shows your system's separator\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Windows uses backslash `\\` as path separator\n",
    "- Mac/Linux use forward slash `/`\n",
    "- `os.path.join()` automatically uses the correct separator for your OS\n",
    "- Makes your code portable across all platforms\n",
    "- Essential for production code that runs on different systems\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Creating Nested Directory Paths\n",
    "\n",
    "**Problem**: Build a path to a file in nested directories\n",
    "\n",
    "**Expected Output**: Properly formatted path for your operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Build path components\n",
    "project_dir = \"my_project\"\n",
    "data_dir = \"data\"\n",
    "subdirectory = \"raw\"\n",
    "filename = \"dataset.txt\"\n",
    "\n",
    "# Combine into complete path\n",
    "file_path = os.path.join(project_dir, data_dir, subdirectory, filename)\n",
    "\n",
    "print(\"Complete path:\", file_path)\n",
    "# On Windows: my_project\\data\\raw\\dataset.txt\n",
    "# On Mac/Linux: my_project/data/raw/dataset.txt\n",
    "\n",
    "# Create the directory structure\n",
    "directory = os.path.join(project_dir, data_dir, subdirectory)\n",
    "os.makedirs(directory, exist_ok=True)  # exist_ok=True prevents error if exists\n",
    "\n",
    "# Now write to that path\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(\"Sample data for ML training\")\n",
    "\n",
    "print(f\"File created at: {file_path}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `os.path.join()` accepts multiple path components\n",
    "- `os.makedirs()` creates all necessary parent directories\n",
    "- `exist_ok=True` prevents errors if directory already exists\n",
    "- Pattern is essential for organizing project files and datasets\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Working with Absolute and Relative Paths\n",
    "\n",
    "**Problem**: Convert between relative and absolute paths safely\n",
    "\n",
    "**Expected Output**: Full absolute paths to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Get current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current directory:\", current_dir)\n",
    "\n",
    "# Create a relative path\n",
    "relative_path = os.path.join(\"data\", \"output.txt\")\n",
    "print(\"\\nRelative path:\", relative_path)\n",
    "\n",
    "# Convert to absolute path\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "print(\"Absolute path:\", absolute_path)\n",
    "\n",
    "# Check if path exists\n",
    "print(\"\\nPath exists?\", os.path.exists(absolute_path))\n",
    "\n",
    "# Get directory name from a path\n",
    "directory = os.path.dirname(absolute_path)\n",
    "print(\"Directory:\", directory)\n",
    "\n",
    "# Get just the filename\n",
    "filename = os.path.basename(absolute_path)\n",
    "print(\"Filename:\", filename)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `os.getcwd()` returns current working directory\n",
    "- `os.path.abspath()` converts relative paths to absolute\n",
    "- `os.path.exists()` checks if a path exists\n",
    "- `os.path.dirname()` extracts directory from full path\n",
    "- `os.path.basename()` extracts filename from full path\n",
    "- These utilities are crucial for robust file handling\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 3\n",
    "**Practice**: Safe path handling with os.path.join\n",
    "\n",
    "### Exercise 1: Create Project Structure\n",
    "\n",
    "**Task**: Create a directory structure and save a configuration file in it\n",
    "\n",
    "**Structure**:\n",
    "```\n",
    "ml_project/\n",
    "  config/\n",
    "    settings.txt\n",
    "```\n",
    "\n",
    "**Expected Output**: File created at correct nested location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Define path components\n",
    "project = \"ml_project\"\n",
    "config_dir = \"config\"\n",
    "config_file = \"settings.txt\"\n",
    "\n",
    "# Build directory path\n",
    "dir_path = os.path.join(project, config_dir)\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Build complete file path\n",
    "file_path = os.path.join(dir_path, config_file)\n",
    "\n",
    "# Write configuration\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(\"learning_rate=0.001\\n\")\n",
    "    file.write(\"batch_size=32\\n\")\n",
    "    file.write(\"epochs=100\\n\")\n",
    "\n",
    "print(f\"Configuration saved to: {file_path}\")\n",
    "print(f\"Absolute path: {os.path.abspath(file_path)}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `os.path.join()` builds paths safely\n",
    "- `os.makedirs()` creates nested directories\n",
    "- `exist_ok=True` prevents errors on re-runs\n",
    "- Pattern mirrors real ML project organization\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: List Files in Directory\n",
    "\n",
    "**Task**: Create several files in a directory, then list all `.txt` files\n",
    "\n",
    "**Expected Output**: List of text files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Create a documents directory\n",
    "docs_dir = \"documents\"\n",
    "os.makedirs(docs_dir, exist_ok=True)\n",
    "\n",
    "# Create several files\n",
    "files_to_create = [\"note1.txt\", \"note2.txt\", \"data.csv\", \"readme.txt\"]\n",
    "\n",
    "for filename in files_to_create:\n",
    "    file_path = os.path.join(docs_dir, filename)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(f\"Content of {filename}\")\n",
    "\n",
    "print(\"Files created successfully\\n\")\n",
    "\n",
    "# List all .txt files\n",
    "print(\"Text files in directory:\")\n",
    "for filename in os.listdir(docs_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        full_path = os.path.join(docs_dir, filename)\n",
    "        print(f\"  - {filename} (size: {os.path.getsize(full_path)} bytes)\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `os.listdir()` returns all files/folders in a directory\n",
    "- `.endswith('.txt')` filters for text files only\n",
    "- `os.path.getsize()` returns file size in bytes\n",
    "- Always use `os.path.join()` when working with directory listings\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Safe File Path Checking\n",
    "\n",
    "**Task**: Write a function that safely checks if a file exists before reading it\n",
    "\n",
    "**Expected Output**: Function that handles missing files gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os\n",
    "\n",
    "def safe_read_file(directory, filename):\n",
    "    # Build path and check existence, then read\n",
    "    pass\n",
    "\n",
    "# Test with existing and non-existing files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "def safe_read_file(directory, filename):\n",
    "    \"\"\"Safely read a file with proper path handling and error checking.\"\"\"\n",
    "    \n",
    "    # Build the complete path\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"Error: File '{file_path}' does not exist\"\n",
    "    \n",
    "    # Check if it's actually a file (not a directory)\n",
    "    if not os.path.isfile(file_path):\n",
    "        return f\"Error: '{file_path}' is not a file\"\n",
    "    \n",
    "    # Read and return content\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "# Test with existing file\n",
    "result1 = safe_read_file(\"documents\", \"note1.txt\")\n",
    "print(\"Reading existing file:\")\n",
    "print(result1)\n",
    "\n",
    "# Test with non-existing file\n",
    "result2 = safe_read_file(\"documents\", \"missing.txt\")\n",
    "print(\"\\nReading non-existing file:\")\n",
    "print(result2)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `os.path.exists()` checks if path exists\n",
    "- `os.path.isfile()` verifies it's a file, not a directory\n",
    "- Try/except catches any unexpected errors\n",
    "- Returns meaningful error messages\n",
    "- This defensive programming prevents crashes in production\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 4\n",
    "**Concept**: Real-world AI/RAG/Agentic applications with file handling\n",
    "\n",
    "### Example 1: Building a Document Loader for RAG\n",
    "\n",
    "**Problem**: Load multiple documents from a directory for RAG pipeline\n",
    "\n",
    "**Expected Output**: Dictionary mapping filenames to content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "def load_documents_for_rag(directory):\n",
    "    \"\"\"\n",
    "    Load all text files from a directory for RAG indexing.\n",
    "    Returns a dictionary: {filename: content}\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' not found\")\n",
    "        return documents\n",
    "    \n",
    "    # Iterate through all files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Only process text files\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            # Read file content\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    documents[filename] = content\n",
    "                    print(f\"Loaded: {filename} ({len(content)} characters)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Create sample documents\n",
    "rag_docs = \"rag_documents\"\n",
    "os.makedirs(rag_docs, exist_ok=True)\n",
    "\n",
    "# Sample AI-related documents\n",
    "with open(os.path.join(rag_docs, \"intro_to_rag.txt\"), 'w') as f:\n",
    "    f.write(\"RAG (Retrieval-Augmented Generation) combines retrieval with LLMs.\")\n",
    "\n",
    "with open(os.path.join(rag_docs, \"embeddings.txt\"), 'w') as f:\n",
    "    f.write(\"Embeddings are vector representations of text for semantic search.\")\n",
    "\n",
    "with open(os.path.join(rag_docs, \"agents.txt\"), 'w') as f:\n",
    "    f.write(\"AI agents use tools and reasoning to accomplish complex tasks.\")\n",
    "\n",
    "# Load documents\n",
    "print(\"Loading documents for RAG pipeline:\\n\")\n",
    "docs = load_documents_for_rag(rag_docs)\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(docs)}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Pattern used in real RAG systems to load knowledge base\n",
    "- `encoding='utf-8'` handles international characters\n",
    "- Error handling prevents one bad file from breaking everything\n",
    "- Returns structured data ready for embedding generation\n",
    "- This is the first step in building a RAG pipeline\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Caching AI Agent Results\n",
    "\n",
    "**Problem**: Save agent outputs to cache file for faster retrieval\n",
    "\n",
    "**Expected Output**: Cache system that saves and loads agent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "\n",
    "class AgentCache:\n",
    "    \"\"\"Cache system for AI agent results.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=\"agent_cache\"):\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    def save_result(self, query, result):\n",
    "        \"\"\"Save an agent result to cache.\"\"\"\n",
    "        # Create a simple filename from query (sanitized)\n",
    "        filename = query.replace(\" \", \"_\")[:50] + \".json\"\n",
    "        file_path = os.path.join(self.cache_dir, filename)\n",
    "        \n",
    "        # Save as JSON\n",
    "        cache_data = {\n",
    "            \"query\": query,\n",
    "            \"result\": result\n",
    "        }\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(cache_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Cached: {filename}\")\n",
    "    \n",
    "    def load_result(self, query):\n",
    "        \"\"\"Load a cached result if it exists.\"\"\"\n",
    "        filename = query.replace(\" \", \"_\")[:50] + \".json\"\n",
    "        file_path = os.path.join(self.cache_dir, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                cache_data = json.load(f)\n",
    "            print(f\"Cache hit: {filename}\")\n",
    "            return cache_data[\"result\"]\n",
    "        \n",
    "        print(f\"Cache miss: {query}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "cache = AgentCache()\n",
    "\n",
    "# Simulate agent processing a query\n",
    "query1 = \"What is machine learning\"\n",
    "result1 = \"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
    "\n",
    "# Save to cache\n",
    "cache.save_result(query1, result1)\n",
    "\n",
    "# Later, try to load from cache\n",
    "print(\"\\nAttempting to load from cache:\")\n",
    "cached_result = cache.load_result(query1)\n",
    "print(f\"Result: {cached_result}\")\n",
    "\n",
    "# Try with query not in cache\n",
    "print(\"\\nTrying non-cached query:\")\n",
    "cache.load_result(\"What is deep learning\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Caching saves expensive API calls to LLMs\n",
    "- JSON format is human-readable and easily parseable\n",
    "- `os.path.exists()` checks cache before processing\n",
    "- Real agentic systems use similar patterns to reduce costs\n",
    "- Cache directory organization keeps files manageable\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Logging Training Metrics\n",
    "\n",
    "**Problem**: Append training metrics to a log file during ML training\n",
    "\n",
    "**Expected Output**: Log file with metrics from multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def log_training_metrics(log_dir, epoch, loss, accuracy):\n",
    "    \"\"\"\n",
    "    Append training metrics to a log file.\n",
    "    Used during ML model training to track progress.\n",
    "    \"\"\"\n",
    "    # Ensure log directory exists\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Create log file path\n",
    "    log_file = os.path.join(log_dir, \"training.log\")\n",
    "    \n",
    "    # Get current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Format log entry\n",
    "    log_entry = f\"[{timestamp}] Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.2%}\\n\"\n",
    "    \n",
    "    # Append to log file\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(log_entry)\n",
    "    \n",
    "    print(f\"Logged: Epoch {epoch}\")\n",
    "\n",
    "# Simulate training loop\n",
    "print(\"Simulating model training...\\n\")\n",
    "\n",
    "# Create logs directory\n",
    "logs_dir = os.path.join(\"ml_project\", \"logs\")\n",
    "\n",
    "# Simulate 5 training epochs\n",
    "import random\n",
    "for epoch in range(1, 6):\n",
    "    # Simulate improving metrics\n",
    "    loss = 2.0 / (epoch + 0.5)\n",
    "    accuracy = 0.5 + (epoch * 0.08)\n",
    "    \n",
    "    log_training_metrics(logs_dir, epoch, loss, accuracy)\n",
    "\n",
    "# Read and display the log\n",
    "print(\"\\nTraining log contents:\")\n",
    "log_path = os.path.join(logs_dir, \"training.log\")\n",
    "with open(log_path, 'r') as f:\n",
    "    print(f.read())\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Append mode `'a'` adds metrics without overwriting previous epochs\n",
    "- Timestamps help track training duration\n",
    "- Formatted output is human-readable\n",
    "- Pattern used in real ML frameworks (TensorFlow, PyTorch)\n",
    "- Logs are essential for debugging and comparing experiments\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 4\n",
    "**Practice**: Real-world AI/RAG/Agentic applications\n",
    "\n",
    "### Exercise 1: Document Chunker for RAG\n",
    "\n",
    "**Task**: Read a large document and split it into chunks, saving each chunk as a separate file\n",
    "\n",
    "**Scenario**: In RAG systems, large documents are split into smaller chunks for embedding\n",
    "\n",
    "**Expected Output**: Multiple chunk files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os\n",
    "\n",
    "def chunk_document(input_file, output_dir, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Split a document into chunks and save each chunk.\n",
    "    chunk_size: number of characters per chunk\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Create a sample document first, then chunk it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "def chunk_document(input_file, output_dir, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Split a document into chunks for RAG processing.\n",
    "    chunk_size: number of characters per chunk\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the document\n",
    "    with open(input_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(content), chunk_size):\n",
    "        chunk = content[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Save each chunk\n",
    "    for idx, chunk in enumerate(chunks, 1):\n",
    "        chunk_file = os.path.join(output_dir, f\"chunk_{idx:03d}.txt\")\n",
    "        with open(chunk_file, 'w') as f:\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks in '{output_dir}'\")\n",
    "    return len(chunks)\n",
    "\n",
    "# Create a sample document\n",
    "sample_doc = \"sample_document.txt\"\n",
    "with open(sample_doc, 'w') as f:\n",
    "    f.write(\n",
    "        \"Retrieval-Augmented Generation (RAG) is a technique that combines \"\n",
    "        \"information retrieval with language generation. It retrieves relevant \"\n",
    "        \"documents from a knowledge base and uses them to generate accurate responses. \"\n",
    "        \"This approach helps AI systems provide more factual and grounded answers.\"\n",
    "    )\n",
    "\n",
    "# Chunk the document\n",
    "chunk_count = chunk_document(sample_doc, \"chunks_output\", chunk_size=100)\n",
    "\n",
    "# Display first chunk as example\n",
    "print(\"\\nFirst chunk content:\")\n",
    "with open(os.path.join(\"chunks_output\", \"chunk_001.txt\"), 'r') as f:\n",
    "    print(f.read())\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Chunking is essential for RAG - LLMs have token limits\n",
    "- Fixed-size chunks are simple (real systems use semantic chunking)\n",
    "- Each chunk becomes a separate embedding in vector database\n",
    "- Numbered filenames maintain order\n",
    "- `f\"{idx:03d}\"` formats numbers with leading zeros (001, 002, etc.)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Conversation History Manager\n",
    "\n",
    "**Task**: Build a system to save and load conversation history for an AI agent\n",
    "\n",
    "**Expected Output**: Functions to append messages and retrieve full conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_message(conversation_id, role, message):\n",
    "    \"\"\"Save a conversation message (role: 'user' or 'assistant').\"\"\"\n",
    "    pass\n",
    "\n",
    "def load_conversation(conversation_id):\n",
    "    \"\"\"Load all messages in a conversation.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "CONVERSATIONS_DIR = \"conversations\"\n",
    "\n",
    "def save_message(conversation_id, role, message):\n",
    "    \"\"\"\n",
    "    Save a conversation message.\n",
    "    role: 'user' or 'assistant'\n",
    "    \"\"\"\n",
    "    # Create conversations directory\n",
    "    os.makedirs(CONVERSATIONS_DIR, exist_ok=True)\n",
    "    \n",
    "    # Build file path\n",
    "    file_path = os.path.join(CONVERSATIONS_DIR, f\"{conversation_id}.jsonl\")\n",
    "    \n",
    "    # Create message entry\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"role\": role,\n",
    "        \"message\": message\n",
    "    }\n",
    "    \n",
    "    # Append to file (JSONL format - one JSON per line)\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "def load_conversation(conversation_id):\n",
    "    \"\"\"Load all messages in a conversation.\"\"\"\n",
    "    file_path = os.path.join(CONVERSATIONS_DIR, f\"{conversation_id}.jsonl\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        return []\n",
    "    \n",
    "    messages = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                messages.append(json.loads(line))\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Simulate a conversation\n",
    "conv_id = \"chat_001\"\n",
    "\n",
    "save_message(conv_id, \"user\", \"What is RAG?\")\n",
    "save_message(conv_id, \"assistant\", \"RAG stands for Retrieval-Augmented Generation...\")\n",
    "save_message(conv_id, \"user\", \"How does it work?\")\n",
    "save_message(conv_id, \"assistant\", \"It combines document retrieval with LLMs...\")\n",
    "\n",
    "print(\"Conversation saved\\n\")\n",
    "\n",
    "# Load and display conversation\n",
    "print(\"Loading conversation history:\\n\")\n",
    "history = load_conversation(conv_id)\n",
    "\n",
    "for msg in history:\n",
    "    print(f\"[{msg['role'].upper()}]: {msg['message']}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- JSONL format (JSON Lines) is efficient for append-only logs\n",
    "- Each message is a separate line - easy to parse and stream\n",
    "- Timestamps help with conversation analysis\n",
    "- Pattern used in real chatbots and AI assistants\n",
    "- Conversation history enables context-aware responses\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Model Checkpoint Manager\n",
    "\n",
    "**Task**: Save and load model checkpoints (simulated) during training\n",
    "\n",
    "**Expected Output**: System that saves checkpoints and can restore from the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, epoch, metrics):\n",
    "    \"\"\"Save model checkpoint with metrics.\"\"\"\n",
    "    pass\n",
    "\n",
    "def find_best_checkpoint(checkpoint_dir, metric='accuracy'):\n",
    "    \"\"\"Find checkpoint with best metric value.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, epoch, metrics):\n",
    "    \"\"\"\n",
    "    Save model checkpoint with training metrics.\n",
    "    In real systems, this would also save model weights.\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Create checkpoint data\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.json\")\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "    \n",
    "    print(f\"Checkpoint saved: epoch {epoch}\")\n",
    "\n",
    "def find_best_checkpoint(checkpoint_dir, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Find the checkpoint with the best metric value.\n",
    "    Higher values are better.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        print(\"No checkpoints found\")\n",
    "        return None\n",
    "    \n",
    "    best_checkpoint = None\n",
    "    best_value = -float('inf')\n",
    "    \n",
    "    # Scan all checkpoint files\n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(checkpoint_dir, filename)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "            \n",
    "            # Check if this checkpoint is better\n",
    "            value = checkpoint['metrics'].get(metric, -float('inf'))\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_checkpoint = checkpoint\n",
    "    \n",
    "    return best_checkpoint\n",
    "\n",
    "# Simulate training with checkpoints\n",
    "print(\"Simulating training with checkpoints\\n\")\n",
    "\n",
    "checkpoint_dir = os.path.join(\"ml_project\", \"checkpoints\")\n",
    "\n",
    "# Simulate 5 epochs\n",
    "for epoch in range(1, 6):\n",
    "    metrics = {\n",
    "        \"loss\": 2.0 / (epoch + 0.5),\n",
    "        \"accuracy\": 0.5 + (epoch * 0.08),\n",
    "        \"val_loss\": 2.2 / (epoch + 0.5)\n",
    "    }\n",
    "    save_checkpoint(checkpoint_dir, epoch, metrics)\n",
    "\n",
    "# Find best checkpoint\n",
    "print(\"\\nFinding best checkpoint...\")\n",
    "best = find_best_checkpoint(checkpoint_dir, metric='accuracy')\n",
    "\n",
    "if best:\n",
    "    print(f\"\\nBest checkpoint found:\")\n",
    "    print(f\"  Epoch: {best['epoch']}\")\n",
    "    print(f\"  Metrics: {best['metrics']}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Checkpointing prevents data loss if training crashes\n",
    "- Saving at each epoch allows resuming from any point\n",
    "- Finding best checkpoint helps select optimal model\n",
    "- Real ML frameworks (PyTorch, TensorFlow) use similar patterns\n",
    "- JSON makes checkpoints human-readable and debuggable\n",
    "- In production, you'd also save model weights (not just metrics)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional Extra Practice\n",
    "**Challenge yourself with these integrated problems**\n",
    "\n",
    "### Challenge 1: Error-Resilient File Processor\n",
    "\n",
    "**Task**: Create a function that processes multiple files, handling errors gracefully\n",
    "\n",
    "**Requirements**:\n",
    "- Read all `.txt` files from a directory\n",
    "- Count words in each file\n",
    "- Handle missing files and read errors\n",
    "- Return a summary report\n",
    "\n",
    "**Expected Output**: Dictionary with file statistics and error log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "def process_files_with_error_handling(directory):\n",
    "    \"\"\"\n",
    "    Process all text files in directory with comprehensive error handling.\n",
    "    Returns statistics and error log.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"files_processed\": [],\n",
    "        \"errors\": [],\n",
    "        \"total_words\": 0,\n",
    "        \"total_files\": 0\n",
    "    }\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        results[\"errors\"].append(f\"Directory '{directory}' not found\")\n",
    "        return results\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Count words\n",
    "            word_count = len(content.split())\n",
    "            \n",
    "            # Record success\n",
    "            results[\"files_processed\"].append({\n",
    "                \"filename\": filename,\n",
    "                \"word_count\": word_count\n",
    "            })\n",
    "            results[\"total_words\"] += word_count\n",
    "            results[\"total_files\"] += 1\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            results[\"errors\"].append(f\"File not found: {filename}\")\n",
    "        except UnicodeDecodeError:\n",
    "            results[\"errors\"].append(f\"Encoding error: {filename}\")\n",
    "        except Exception as e:\n",
    "            results[\"errors\"].append(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the function\n",
    "results = process_files_with_error_handling(\"rag_documents\")\n",
    "\n",
    "print(\"=== File Processing Report ===\")\n",
    "print(f\"\\nFiles processed: {results['total_files']}\")\n",
    "print(f\"Total words: {results['total_words']}\")\n",
    "\n",
    "print(\"\\nDetails:\")\n",
    "for file_info in results[\"files_processed\"]:\n",
    "    print(f\"  {file_info['filename']}: {file_info['word_count']} words\")\n",
    "\n",
    "if results[\"errors\"]:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for error in results[\"errors\"]:\n",
    "        print(f\"  - {error}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Comprehensive error handling prevents crashes\n",
    "- Specific exception types allow targeted error messages\n",
    "- Results dictionary provides detailed report\n",
    "- Pattern essential for production data pipelines\n",
    "- Handles encoding issues common in real-world files\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: RAG Document Index Builder\n",
    "\n",
    "**Task**: Build a complete document indexing system for RAG\n",
    "\n",
    "**Requirements**:\n",
    "- Load documents from nested directories\n",
    "- Create metadata for each document (path, size, word count)\n",
    "- Save index as JSON file\n",
    "- Provide search function to find documents\n",
    "\n",
    "**Expected Output**: Complete indexing system with search capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "\n",
    "class DocumentIndexer:\n",
    "    \"\"\"Simple document indexer for RAG systems.\"\"\"\n",
    "    \n",
    "    def __init__(self, index_file=\"document_index.json\"):\n",
    "        self.index_file = index_file\n",
    "        self.index = []\n",
    "    \n",
    "    def build_index(self, root_directory):\n",
    "        \"\"\"Recursively index all text files in directory tree.\"\"\"\n",
    "        print(f\"Building index from: {root_directory}\\n\")\n",
    "        \n",
    "        # Walk through directory tree\n",
    "        for dirpath, dirnames, filenames in os.walk(root_directory):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.txt'):\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    \n",
    "                    try:\n",
    "                        # Read file\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                        \n",
    "                        # Create metadata\n",
    "                        metadata = {\n",
    "                            \"filename\": filename,\n",
    "                            \"path\": file_path,\n",
    "                            \"size_bytes\": os.path.getsize(file_path),\n",
    "                            \"word_count\": len(content.split()),\n",
    "                            \"preview\": content[:100]  # First 100 chars\n",
    "                        }\n",
    "                        \n",
    "                        self.index.append(metadata)\n",
    "                        print(f\"Indexed: {filename}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error indexing {filename}: {e}\")\n",
    "        \n",
    "        print(f\"\\nTotal documents indexed: {len(self.index)}\")\n",
    "    \n",
    "    def save_index(self):\n",
    "        \"\"\"Save index to JSON file.\"\"\"\n",
    "        with open(self.index_file, 'w') as f:\n",
    "            json.dump(self.index, f, indent=2)\n",
    "        print(f\"Index saved to: {self.index_file}\")\n",
    "    \n",
    "    def load_index(self):\n",
    "        \"\"\"Load index from JSON file.\"\"\"\n",
    "        if os.path.exists(self.index_file):\n",
    "            with open(self.index_file, 'r') as f:\n",
    "                self.index = json.load(f)\n",
    "            print(f\"Index loaded: {len(self.index)} documents\")\n",
    "        else:\n",
    "            print(\"No index file found\")\n",
    "    \n",
    "    def search(self, keyword):\n",
    "        \"\"\"Search for documents containing keyword.\"\"\"\n",
    "        results = []\n",
    "        keyword_lower = keyword.lower()\n",
    "        \n",
    "        for doc in self.index:\n",
    "            # Search in filename and preview\n",
    "            if (keyword_lower in doc['filename'].lower() or \n",
    "                keyword_lower in doc['preview'].lower()):\n",
    "                results.append(doc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "indexer = DocumentIndexer()\n",
    "\n",
    "# Build index from rag_documents directory\n",
    "indexer.build_index(\"rag_documents\")\n",
    "\n",
    "# Save index\n",
    "indexer.save_index()\n",
    "\n",
    "# Search example\n",
    "print(\"\\n=== Search Results ===\")\n",
    "results = indexer.search(\"RAG\")\n",
    "print(f\"Found {len(results)} documents containing 'RAG':\\n\")\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"File: {doc['filename']}\")\n",
    "    print(f\"Words: {doc['word_count']}\")\n",
    "    print(f\"Preview: {doc['preview'][:50]}...\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `os.walk()` recursively traverses directory trees\n",
    "- Metadata enables efficient document retrieval without re-reading files\n",
    "- JSON index is portable and human-readable\n",
    "- Search functionality provides basic retrieval capability\n",
    "- Real RAG systems extend this with vector embeddings and semantic search\n",
    "- Pattern is foundation for document management in AI applications\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "- Basic file operations (read, write, append)\n",
    "- Context managers (`with` statement) for safe file handling\n",
    "- Cross-platform path handling with `os.path.join`\n",
    "- Error handling for robust file operations\n",
    "- Real-world patterns for AI/RAG/Agentic systems\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. Always use `with` statement for file operations\n",
    "2. Always use `os.path.join()` for cross-platform paths\n",
    "3. Handle errors gracefully with try/except\n",
    "4. Use appropriate file modes (`'r'`, `'w'`, `'a'`)\n",
    "5. Consider memory efficiency for large files (line-by-line reading)\n",
    "\n",
    "**Next Steps**:\n",
    "- Learn about JSON and CSV file handling\n",
    "- Explore file operations in pandas for data analysis\n",
    "- Study binary file handling for images and models\n",
    "- Practice with real datasets and API responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
