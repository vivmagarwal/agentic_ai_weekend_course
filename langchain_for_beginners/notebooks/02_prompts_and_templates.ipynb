{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts and Templates in LangChain\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Create reusable prompt templates with variables\n",
    "- Use ChatPromptTemplate for structured conversations\n",
    "- Implement few-shot prompting for better results\n",
    "- Work with MessagesPlaceholder for dynamic content\n",
    "- Build prompt libraries for different use cases\n",
    "- Apply advanced prompt engineering techniques\n",
    "\n",
    "## Why This Matters: Production-Ready AI Applications\n",
    "\n",
    "**In Enterprise Applications:**\n",
    "- Prompt templates ensure consistency across teams\n",
    "- Variables enable dynamic, personalized responses\n",
    "- Version control for prompts becomes possible\n",
    "\n",
    "**In AI Products:**\n",
    "- Few-shot learning improves accuracy without fine-tuning\n",
    "- Templates make prompts maintainable and testable\n",
    "- Separation of prompt logic from application code\n",
    "\n",
    "**In Development:**\n",
    "- Rapid iteration on prompt designs\n",
    "- A/B testing different prompt strategies\n",
    "- Reusable components across projects\n",
    "\n",
    "## Prerequisites\n",
    "- Completed notebooks 00 and 01\n",
    "- Understanding of basic LLM calls\n",
    "- Familiarity with Python string formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Dependencies\n",
    "\n",
    "Run this cell first to set up your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai python-dotenv\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    FewShotPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Verify setup\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ Environment ready! Let's work with prompts and templates.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please set your OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 1: Basic Prompt Templates\n",
    "\n",
    "**Concept**: Prompt templates allow you to create reusable prompts with variable substitution, making your code more maintainable and flexible.\n",
    "\n",
    "### Example 1: Simple PromptTemplate\n",
    "\n",
    "**Problem**: Create a reusable prompt with variables\n",
    "**Expected Output**: Dynamic prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a simple template\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"audience\"],\n",
    "    template=\"\"\"Explain {topic} in simple terms suitable for {audience}.\n",
    "    \n",
    "    Make sure your explanation:\n",
    "    - Uses appropriate language for the audience\n",
    "    - Includes relevant examples\n",
    "    - Is engaging and clear\"\"\"\n",
    ")\n",
    "\n",
    "# Format the template with different values\n",
    "prompt1 = template.format(topic=\"machine learning\", audience=\"5th graders\")\n",
    "prompt2 = template.format(topic=\"machine learning\", audience=\"executives\")\n",
    "\n",
    "print(\"Prompt for 5th graders:\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt1)\n",
    "print(\"\\nPrompt for executives:\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt2)\n",
    "\n",
    "# Use with LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "response = llm.invoke(prompt1)\n",
    "print(\"\\nLLM Response (for 5th graders):\")\n",
    "print(\"=\" * 50)\n",
    "print(response.content[:300] + \"...\")\n",
    "```\n",
    "\n",
    "**Why templates are powerful:**\n",
    "- Reuse the same structure with different inputs\n",
    "- Maintain consistency across your application\n",
    "- Easy to update prompts in one place\n",
    "- Variables make prompts dynamic\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Template with Validation\n",
    "\n",
    "**Problem**: Create templates with input validation and formatting\n",
    "**Expected Output**: Safe, validated prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Template with partial variables and validation\n",
    "def create_code_review_template():\n",
    "    \"\"\"Create a code review template with validation\"\"\"\n",
    "    \n",
    "    template = PromptTemplate(\n",
    "        input_variables=[\"language\", \"code\", \"focus_areas\"],\n",
    "        template=\"\"\"You are an expert {language} code reviewer.\n",
    "        \n",
    "Please review the following code:\n",
    "\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Focus on these areas:\n",
    "{focus_areas}\n",
    "\n",
    "Provide:\n",
    "1. Issues found (if any)\n",
    "2. Suggestions for improvement\n",
    "3. Best practices to follow\n",
    "4. Overall assessment\"\"\",\n",
    "        validate_template=True  # Ensure all variables are used\n",
    "    )\n",
    "    \n",
    "    return template\n",
    "\n",
    "# Create template with validation\n",
    "review_template = create_code_review_template()\n",
    "\n",
    "# Example with Python code\n",
    "python_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    sum = 0\n",
    "    for n in numbers:\n",
    "        sum = sum + n\n",
    "    average = sum / len(numbers)\n",
    "    return average\n",
    "\"\"\"\n",
    "\n",
    "# Format with specific focus areas\n",
    "review_prompt = review_template.format(\n",
    "    language=\"Python\",\n",
    "    code=python_code,\n",
    "    focus_areas=\"- Error handling\\n- Variable naming\\n- Pythonic style\\n- Performance\"\n",
    ")\n",
    "\n",
    "print(\"Generated Code Review Prompt:\")\n",
    "print(\"=\" * 50)\n",
    "print(review_prompt[:400] + \"...\")\n",
    "\n",
    "# Use with LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "review = llm.invoke(review_prompt)\n",
    "print(\"\\nCode Review Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(review.content[:500] + \"...\")\n",
    "```\n",
    "\n",
    "**Key benefits of validation:**\n",
    "- Catches missing variables early\n",
    "- Ensures template consistency\n",
    "- Prevents runtime errors\n",
    "- Makes templates more maintainable\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Partial Templates\n",
    "\n",
    "**Problem**: Create templates with some variables pre-filled\n",
    "**Expected Output**: Partially configured templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "\n",
    "# Template with both dynamic and partial variables\n",
    "base_template = PromptTemplate(\n",
    "    input_variables=[\"task\", \"context\"],\n",
    "    partial_variables={\"date\": datetime.now().strftime(\"%Y-%m-%d\"), \"version\": \"1.0\"},\n",
    "    template=\"\"\"[Report Date: {date}]\n",
    "[Template Version: {version}]\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Please provide a detailed analysis including:\n",
    "- Current situation assessment\n",
    "- Recommended actions\n",
    "- Expected outcomes\n",
    "- Risk factors\"\"\"\n",
    ")\n",
    "\n",
    "# Create specialized versions with partial variables\n",
    "marketing_template = base_template.partial(\n",
    "    context=\"Marketing department quarterly review\"\n",
    ")\n",
    "\n",
    "engineering_template = base_template.partial(\n",
    "    context=\"Engineering team sprint retrospective\"\n",
    ")\n",
    "\n",
    "# Now only need to provide 'task' variable\n",
    "marketing_prompt = marketing_template.format(\n",
    "    task=\"Analyze social media campaign performance\"\n",
    ")\n",
    "\n",
    "engineering_prompt = engineering_template.format(\n",
    "    task=\"Review deployment pipeline efficiency\"\n",
    ")\n",
    "\n",
    "print(\"Marketing Template Output:\")\n",
    "print(\"=\" * 50)\n",
    "print(marketing_prompt)\n",
    "print(\"\\nEngineering Template Output:\")\n",
    "print(\"=\" * 50)\n",
    "print(engineering_prompt)\n",
    "\n",
    "print(\"\\nüí° Partial templates allow creating specialized versions!\")\n",
    "```\n",
    "\n",
    "**Partial template use cases:**\n",
    "- Department-specific templates\n",
    "- Templates with timestamps\n",
    "- Configuration-based templates\n",
    "- Multi-tenant applications\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 1: Practice with Basic Templates\n",
    "\n",
    "**Practice Focus**: Create and use various prompt templates\n",
    "\n",
    "### Exercise 1: Create a Product Description Template\n",
    "\n",
    "**Task**: Build a template for generating product descriptions\n",
    "**Expected Output**: Dynamic product descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a PromptTemplate with variables:\n",
    "# - product_name\n",
    "# - features (list of features)\n",
    "# - target_audience\n",
    "# Generate descriptions for 2 different products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create product description template\n",
    "product_template = PromptTemplate(\n",
    "    input_variables=[\"product_name\", \"features\", \"target_audience\"],\n",
    "    template=\"\"\"Create an engaging product description for {product_name}.\n",
    "\n",
    "Key Features:\n",
    "{features}\n",
    "\n",
    "Target Audience: {target_audience}\n",
    "\n",
    "Write a compelling 3-paragraph description that:\n",
    "1. Hooks the reader with benefits\n",
    "2. Highlights the key features\n",
    "3. Includes a call to action\n",
    "\n",
    "Use language appropriate for the target audience.\"\"\"\n",
    ")\n",
    "\n",
    "# Test with different products\n",
    "products = [\n",
    "    {\n",
    "        \"product_name\": \"SmartFit Pro Watch\",\n",
    "        \"features\": \"\"\"- Heart rate monitoring\n",
    "- GPS tracking\n",
    "- 7-day battery life\n",
    "- Water resistant to 50m\n",
    "- Sleep tracking\"\"\",\n",
    "        \"target_audience\": \"fitness enthusiasts aged 25-45\"\n",
    "    },\n",
    "    {\n",
    "        \"product_name\": \"KiddoLearn Tablet\",\n",
    "        \"features\": \"\"\"- Parental controls\n",
    "- Educational apps pre-installed\n",
    "- Shatterproof screen\n",
    "- 10-hour battery\n",
    "- Age-appropriate content filtering\"\"\",\n",
    "        \"target_audience\": \"parents of children aged 5-10\"\n",
    "    }\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n",
    "\n",
    "for product in products:\n",
    "    # Format the template\n",
    "    prompt = product_template.format(**product)\n",
    "    \n",
    "    # Generate description\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(f\"\\nüì¶ {product['product_name']} Description:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(response.content[:400] + \"...\\n\")\n",
    "\n",
    "print(\"‚úÖ Templates make it easy to generate consistent product descriptions!\")\n",
    "```\n",
    "\n",
    "**What you learned:**\n",
    "- Templates ensure consistent structure\n",
    "- Variables make content dynamic\n",
    "- Same template works for different products\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Template with Validation\n",
    "\n",
    "**Task**: Create a template that validates inputs\n",
    "**Expected Output**: Error messages for invalid inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a template for email generation that:\n",
    "# - Validates that all required variables are present\n",
    "# - Has variables: recipient_name, sender_name, subject, main_content\n",
    "# - Test with both valid and invalid inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def create_email_template():\n",
    "    \"\"\"Create an email template with validation\"\"\"\n",
    "    \n",
    "    template = PromptTemplate(\n",
    "        input_variables=[\"recipient_name\", \"sender_name\", \"subject\", \"main_content\"],\n",
    "        template=\"\"\"Draft a professional email with the following details:\n",
    "\n",
    "To: {recipient_name}\n",
    "From: {sender_name}\n",
    "Subject: {subject}\n",
    "\n",
    "Main Content:\n",
    "{main_content}\n",
    "\n",
    "Format the email with:\n",
    "- Appropriate greeting\n",
    "- Clear and concise body\n",
    "- Professional closing\n",
    "- Signature with {sender_name}\"\"\",\n",
    "        validate_template=True\n",
    "    )\n",
    "    return template\n",
    "\n",
    "# Create the template\n",
    "email_template = create_email_template()\n",
    "\n",
    "# Test with valid inputs\n",
    "try:\n",
    "    valid_email = email_template.format(\n",
    "        recipient_name=\"John Smith\",\n",
    "        sender_name=\"Sarah Johnson\",\n",
    "        subject=\"Project Update\",\n",
    "        main_content=\"Update on Q4 deliverables and timeline adjustments\"\n",
    "    )\n",
    "    print(\"‚úÖ Valid template formatting succeeded!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(valid_email[:200] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test with missing variable (will fail)\n",
    "print(\"\\nTesting with missing variable:\")\n",
    "try:\n",
    "    invalid_email = email_template.format(\n",
    "        recipient_name=\"John Smith\",\n",
    "        sender_name=\"Sarah Johnson\",\n",
    "        # Missing: subject and main_content\n",
    "    )\n",
    "except KeyError as e:\n",
    "    print(f\"‚ùå Validation caught missing variable: {e}\")\n",
    "    print(\"This is good - template validation works!\")\n",
    "\n",
    "print(\"\\nüí° Template validation helps catch errors early!\")\n",
    "```\n",
    "\n",
    "**Key takeaway:**\n",
    "- Validation prevents runtime errors\n",
    "- Catches missing variables immediately\n",
    "- Makes debugging easier\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 2: ChatPromptTemplate for Conversations\n",
    "\n",
    "**Concept**: ChatPromptTemplate is specifically designed for chat models, providing structured conversation templates.\n",
    "\n",
    "### Example 1: Basic ChatPromptTemplate\n",
    "\n",
    "**Problem**: Create structured chat conversations\n",
    "**Expected Output**: Properly formatted chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a chat prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {role} expert. Your tone should be {tone}.\"),\n",
    "    (\"human\", \"Explain {concept} to me.\"),\n",
    "    (\"ai\", \"I'd be happy to explain {concept}. Let me break it down for you.\"),\n",
    "    (\"human\", \"{follow_up_question}\")\n",
    "])\n",
    "\n",
    "# Format with specific values\n",
    "messages = chat_template.format_messages(\n",
    "    role=\"data science\",\n",
    "    tone=\"friendly and encouraging\",\n",
    "    concept=\"neural networks\",\n",
    "    follow_up_question=\"Can you give me a real-world example?\"\n",
    ")\n",
    "\n",
    "print(\"Generated Chat Messages:\")\n",
    "print(\"=\" * 50)\n",
    "for msg in messages:\n",
    "    print(f\"{msg.__class__.__name__}: {msg.content[:100]}...\")\n",
    "\n",
    "# Use with LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"\\nLLM Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.content[:400] + \"...\")\n",
    "```\n",
    "\n",
    "**Why ChatPromptTemplate is powerful:**\n",
    "- Maintains proper message structure\n",
    "- Clear role separation\n",
    "- Supports conversation flow\n",
    "- Type-safe message handling\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Dynamic Conversations with MessagesPlaceholder\n",
    "\n",
    "**Problem**: Create templates that can include dynamic conversation history\n",
    "**Expected Output**: Flexible conversation templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Template with dynamic message history\n",
    "prompt_with_history = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful programming tutor. \n",
    "    Remember what the student has told you and build upon previous explanations.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{current_question}\")\n",
    "])\n",
    "\n",
    "# Example conversation history\n",
    "history = [\n",
    "    HumanMessage(content=\"I'm learning Python and struggling with loops.\"),\n",
    "    AIMessage(content=\"I understand you're having trouble with loops. They're fundamental in Python. Let me help you understand them better.\"),\n",
    "    HumanMessage(content=\"I specifically don't understand the difference between 'for' and 'while' loops.\"),\n",
    "    AIMessage(content=\"Great question! 'for' loops are used when you know how many times to iterate, while 'while' loops continue until a condition becomes false.\")\n",
    "]\n",
    "\n",
    "# Format with history and new question\n",
    "messages = prompt_with_history.format_messages(\n",
    "    chat_history=history,\n",
    "    current_question=\"Can you show me an example where I would choose 'while' over 'for'?\"\n",
    ")\n",
    "\n",
    "print(\"Conversation with Dynamic History:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total messages in conversation: {len(messages)}\")\n",
    "print(f\"\\nLast 3 messages:\")\n",
    "for msg in messages[-3:]:\n",
    "    msg_type = msg.__class__.__name__\n",
    "    content_preview = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n",
    "    print(f\"  {msg_type}: {content_preview}\")\n",
    "\n",
    "# Get response\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"\\nAI Response (with context):\")\n",
    "print(\"=\" * 50)\n",
    "print(response.content[:400] + \"...\")\n",
    "\n",
    "print(\"\\nüí° MessagesPlaceholder enables dynamic conversation management!\")\n",
    "```\n",
    "\n",
    "**MessagesPlaceholder benefits:**\n",
    "- Inject conversation history dynamically\n",
    "- Flexible conversation management\n",
    "- Maintains context across turns\n",
    "- Essential for chatbots\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Multi-Modal Templates\n",
    "\n",
    "**Problem**: Create templates that combine different message types\n",
    "**Expected Output**: Complex conversation structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Complex template with multiple components\n",
    "multi_modal_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a {assistant_type} assistant.\n",
    "    Your expertise: {expertise_areas}\n",
    "    Communication style: {communication_style}\"\"\"),\n",
    "    (\"human\", \"Here's my situation: {user_context}\"),\n",
    "    (\"ai\", \"I understand your situation. Let me help you with that.\"),\n",
    "    MessagesPlaceholder(variable_name=\"examples\", optional=True),\n",
    "    (\"human\", \"My specific question: {question}\"),\n",
    "    (\"system\", \"Remember to {special_instructions}\")\n",
    "])\n",
    "\n",
    "# Create a customer service scenario\n",
    "cs_messages = multi_modal_template.format_messages(\n",
    "    assistant_type=\"customer service\",\n",
    "    expertise_areas=\"product returns, warranty claims, technical support\",\n",
    "    communication_style=\"empathetic and solution-focused\",\n",
    "    user_context=\"I bought a laptop 2 months ago and the screen stopped working\",\n",
    "    examples=[],  # Could include example interactions\n",
    "    question=\"What are my options for getting this fixed?\",\n",
    "    special_instructions=\"provide clear options with pros and cons\"\n",
    ")\n",
    "\n",
    "# Create a technical support scenario\n",
    "tech_messages = multi_modal_template.format_messages(\n",
    "    assistant_type=\"technical support\",\n",
    "    expertise_areas=\"Python, debugging, performance optimization\",\n",
    "    communication_style=\"technical but accessible\",\n",
    "    user_context=\"My Python script is running very slowly with large datasets\",\n",
    "    examples=[],\n",
    "    question=\"How can I profile and optimize my code?\",\n",
    "    special_instructions=\"include code examples and step-by-step instructions\"\n",
    ")\n",
    "\n",
    "print(\"Multi-Modal Template Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Customer service response\n",
    "print(\"\\nüõçÔ∏è Customer Service Response:\")\n",
    "cs_response = llm.invoke(cs_messages)\n",
    "print(cs_response.content[:300] + \"...\")\n",
    "\n",
    "# Technical support response\n",
    "print(\"\\nüíª Technical Support Response:\")\n",
    "tech_response = llm.invoke(tech_messages)\n",
    "print(tech_response.content[:300] + \"...\")\n",
    "\n",
    "print(\"\\n‚úÖ Same template, different personalities and expertise!\")\n",
    "```\n",
    "\n",
    "**Multi-modal template advantages:**\n",
    "- Combine system, human, and AI messages\n",
    "- Optional components with MessagesPlaceholder\n",
    "- Highly customizable conversations\n",
    "- Reusable across different domains\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 2: Practice with ChatPromptTemplate\n",
    "\n",
    "**Practice Focus**: Build conversation templates for different scenarios\n",
    "\n",
    "### Exercise 1: Create a Teaching Assistant Template\n",
    "\n",
    "**Task**: Build a template for a teaching assistant that remembers context\n",
    "**Expected Output**: Context-aware tutoring responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a ChatPromptTemplate that:\n",
    "# - Has a system message defining the teaching assistant role\n",
    "# - Uses MessagesPlaceholder for conversation history\n",
    "# - Takes subject and difficulty_level as variables\n",
    "# - Test with a math question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create teaching assistant template\n",
    "teaching_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a {subject} teaching assistant.\n",
    "    Difficulty level: {difficulty_level}\n",
    "    \n",
    "    Your approach:\n",
    "    - Break down complex concepts into simple steps\n",
    "    - Use examples relevant to the difficulty level\n",
    "    - Check understanding with questions\n",
    "    - Be encouraging and patient\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"conversation_history\"),\n",
    "    (\"human\", \"{student_question}\")\n",
    "])\n",
    "\n",
    "# Simulate a learning session\n",
    "conversation_history = [\n",
    "    HumanMessage(content=\"I'm struggling with algebra.\"),\n",
    "    AIMessage(content=\"I'm here to help you with algebra! What specific topic are you finding challenging?\")\n",
    "]\n",
    "\n",
    "# Format the template\n",
    "messages = teaching_template.format_messages(\n",
    "    subject=\"Mathematics\",\n",
    "    difficulty_level=\"High School\",\n",
    "    conversation_history=conversation_history,\n",
    "    student_question=\"Can you explain how to solve quadratic equations?\"\n",
    ")\n",
    "\n",
    "# Get response\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"üéì Teaching Assistant Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.content[:500] + \"...\")\n",
    "\n",
    "# Add to history and ask follow-up\n",
    "conversation_history.extend([\n",
    "    HumanMessage(content=\"Can you explain how to solve quadratic equations?\"),\n",
    "    AIMessage(content=response.content)\n",
    "])\n",
    "\n",
    "# Follow-up question\n",
    "follow_up_messages = teaching_template.format_messages(\n",
    "    subject=\"Mathematics\",\n",
    "    difficulty_level=\"High School\",\n",
    "    conversation_history=conversation_history,\n",
    "    student_question=\"What's the quadratic formula?\"\n",
    ")\n",
    "\n",
    "follow_up_response = llm.invoke(follow_up_messages)\n",
    "print(\"\\nüìö Follow-up Response (with context):\")\n",
    "print(follow_up_response.content[:300] + \"...\")\n",
    "\n",
    "print(\"\\n‚úÖ The assistant remembers the conversation context!\")\n",
    "```\n",
    "\n",
    "**What you learned:**\n",
    "- ChatPromptTemplate structures conversations\n",
    "- MessagesPlaceholder enables history\n",
    "- Context improves response relevance\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 3: Few-Shot Prompting\n",
    "\n",
    "**Concept**: Few-shot prompting teaches the LLM by example, improving accuracy for specific tasks without fine-tuning.\n",
    "\n",
    "### Example 1: Basic Few-Shot Template\n",
    "\n",
    "**Problem**: Teach the LLM a specific format through examples\n",
    "**Expected Output**: Responses following the demonstrated pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define examples of the pattern we want\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"The movie was fantastic! Best film I've seen all year.\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"keywords\": \"fantastic, best\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"It was okay, nothing special but not terrible.\",\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"keywords\": \"okay, nothing special\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Completely disappointed. Waste of time and money.\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"confidence\": \"high\",\n",
    "        \"keywords\": \"disappointed, waste\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I'm not sure how I feel about it. It had good and bad parts.\",\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"confidence\": \"low\",\n",
    "        \"keywords\": \"not sure, good and bad\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create template for examples\n",
    "example_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"sentiment\", \"confidence\", \"keywords\"],\n",
    "    template=\"\"\"Input: {input}\n",
    "Sentiment: {sentiment}\n",
    "Confidence: {confidence}\n",
    "Keywords: {keywords}\"\"\"\n",
    ")\n",
    "\n",
    "# Create few-shot template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_template,\n",
    "    prefix=\"Analyze the sentiment of text with confidence level and key words.\\n\",\n",
    "    suffix=\"\\nInput: {input}\\nSentiment:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# Test with new inputs\n",
    "test_inputs = [\n",
    "    \"This product exceeded all my expectations! Absolutely love it!\",\n",
    "    \"The service was slow and the food was cold.\"\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Few-Shot Sentiment Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_input in test_inputs:\n",
    "    prompt = few_shot_prompt.format(input=test_input)\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(f\"\\nüìù Input: {test_input}\")\n",
    "    print(f\"ü§ñ Analysis:\\n{response.content}\")\n",
    "\n",
    "print(\"\\nüí° Few-shot learning teaches consistent output format!\")\n",
    "```\n",
    "\n",
    "**Why few-shot prompting works:**\n",
    "- LLM learns from examples\n",
    "- Ensures consistent output format\n",
    "- No fine-tuning required\n",
    "- Improves accuracy for specific tasks\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Dynamic Example Selection\n",
    "\n",
    "**Problem**: Select relevant examples based on input\n",
    "**Expected Output**: Context-appropriate examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "\n",
    "# Examples for different types of writing\n",
    "writing_examples = [\n",
    "    {\"style\": \"formal\", \"input\": \"Hello\", \"output\": \"Good day. I hope this message finds you well.\"},\n",
    "    {\"style\": \"formal\", \"input\": \"Thanks\", \"output\": \"I would like to express my sincere gratitude.\"},\n",
    "    {\"style\": \"casual\", \"input\": \"Hello\", \"output\": \"Hey there! How's it going?\"},\n",
    "    {\"style\": \"casual\", \"input\": \"Thanks\", \"output\": \"Thanks a bunch! Really appreciate it!\"},\n",
    "    {\"style\": \"professional\", \"input\": \"Hello\", \"output\": \"Hello, I hope you're having a productive day.\"},\n",
    "    {\"style\": \"professional\", \"input\": \"Thanks\", \"output\": \"Thank you for your assistance with this matter.\"},\n",
    "]\n",
    "\n",
    "# Example template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"style\", \"input\", \"output\"],\n",
    "    template=\"Style: {style}\\nInput: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "# Dynamic selector (selects based on length to fit token limits)\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=writing_examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100,  # Maximum length in words\n",
    ")\n",
    "\n",
    "# Create dynamic few-shot template\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Transform text into different writing styles based on examples:\\n\",\n",
    "    suffix=\"\\nStyle: {style}\\nInput: {input}\\nOutput:\",\n",
    "    input_variables=[\"style\", \"input\"]\n",
    ")\n",
    "\n",
    "# Test with different styles\n",
    "test_cases = [\n",
    "    {\"style\": \"formal\", \"input\": \"I disagree\"},\n",
    "    {\"style\": \"casual\", \"input\": \"I disagree\"},\n",
    "    {\"style\": \"professional\", \"input\": \"I disagree\"}\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "print(\"Dynamic Few-Shot Style Transfer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test in test_cases:\n",
    "    prompt = dynamic_prompt.format(**test)\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(f\"\\nüé® Style: {test['style']}\")\n",
    "    print(f\"üì• Input: {test['input']}\")\n",
    "    print(f\"üì§ Output: {response.content}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dynamic example selection optimizes token usage!\")\n",
    "```\n",
    "\n",
    "**Dynamic selection benefits:**\n",
    "- Fits within token limits\n",
    "- Selects most relevant examples\n",
    "- Optimizes performance\n",
    "- Reduces costs\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 3: Practice Few-Shot Prompting\n",
    "\n",
    "**Practice Focus**: Create few-shot templates for specific tasks\n",
    "\n",
    "### Exercise 1: Build a Data Extraction Template\n",
    "\n",
    "**Task**: Create a few-shot template for extracting information\n",
    "**Expected Output**: Structured data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a few-shot template that extracts:\n",
    "# - Product name\n",
    "# - Price\n",
    "# - Rating\n",
    "# From product review text\n",
    "# Provide 3 examples and test with new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Examples of product review extraction\n",
    "extraction_examples = [\n",
    "    {\n",
    "        \"text\": \"I bought the UltraSound Pro headphones for $199 and they're amazing! Definitely worth 5 stars.\",\n",
    "        \"product\": \"UltraSound Pro headphones\",\n",
    "        \"price\": \"$199\",\n",
    "        \"rating\": \"5 stars\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The QuickBlend mixer at $89 is okay. Works fine but nothing special. 3 out of 5.\",\n",
    "        \"product\": \"QuickBlend mixer\",\n",
    "        \"price\": \"$89\",\n",
    "        \"rating\": \"3 out of 5\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Waste of money! The TechGadget X for $450 broke after a week. 1 star!\",\n",
    "        \"product\": \"TechGadget X\",\n",
    "        \"price\": \"$450\",\n",
    "        \"rating\": \"1 star\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Template for examples\n",
    "example_template = PromptTemplate(\n",
    "    input_variables=[\"text\", \"product\", \"price\", \"rating\"],\n",
    "    template=\"\"\"Review: {text}\n",
    "Extracted Data:\n",
    "- Product: {product}\n",
    "- Price: {price}\n",
    "- Rating: {rating}\"\"\"\n",
    ")\n",
    "\n",
    "# Create few-shot template\n",
    "extraction_prompt = FewShotPromptTemplate(\n",
    "    examples=extraction_examples,\n",
    "    example_prompt=example_template,\n",
    "    prefix=\"Extract product information from reviews:\\n\",\n",
    "    suffix=\"\\nReview: {text}\\nExtracted Data:\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Test with new reviews\n",
    "test_reviews = [\n",
    "    \"Just got the SmartWatch Ultra for $299. It's fantastic! Easy 5 star rating from me.\",\n",
    "    \"The CoffeMaker Pro costs $159 and makes decent coffee. I'd give it 4 stars overall.\"\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Product Information Extraction:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for review in test_reviews:\n",
    "    prompt = extraction_prompt.format(text=review)\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(f\"\\nüìù Review: {review}\")\n",
    "    print(f\"üìä Extracted:\\n{response.content}\")\n",
    "\n",
    "print(\"\\n‚úÖ Few-shot prompting enables consistent data extraction!\")\n",
    "```\n",
    "\n",
    "**What you learned:**\n",
    "- Few-shot examples teach extraction patterns\n",
    "- Consistent output format\n",
    "- Works for various data types\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional Extra Practice\n",
    "\n",
    "### Challenge 1: Build a Multi-Purpose Template System\n",
    "\n",
    "**Task**: Create a template factory for different business documents\n",
    "**Expected Output**: Reusable template system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "class DocumentType(Enum):\n",
    "    EMAIL = \"email\"\n",
    "    REPORT = \"report\"\n",
    "    PROPOSAL = \"proposal\"\n",
    "    MEMO = \"memo\"\n",
    "\n",
    "class TemplateFactory:\n",
    "    \"\"\"Factory for creating different document templates\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = {\n",
    "            DocumentType.EMAIL: self._create_email_template(),\n",
    "            DocumentType.REPORT: self._create_report_template(),\n",
    "            DocumentType.PROPOSAL: self._create_proposal_template(),\n",
    "            DocumentType.MEMO: self._create_memo_template()\n",
    "        }\n",
    "    \n",
    "    def _create_email_template(self) -> ChatPromptTemplate:\n",
    "        return ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a professional email writer.\"),\n",
    "            (\"human\", \"\"\"Write an email with:\n",
    "            To: {recipient}\n",
    "            Subject: {subject}\n",
    "            Purpose: {purpose}\n",
    "            Tone: {tone}\"\"\")\n",
    "        ])\n",
    "    \n",
    "    def _create_report_template(self) -> ChatPromptTemplate:\n",
    "        return ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a business analyst creating professional reports.\"),\n",
    "            (\"human\", \"\"\"Create a report on:\n",
    "            Topic: {topic}\n",
    "            Data: {data}\n",
    "            Recommendations needed: {recommendations}\n",
    "            Length: {length} pages\"\"\")\n",
    "        ])\n",
    "    \n",
    "    def _create_proposal_template(self) -> ChatPromptTemplate:\n",
    "        return ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a business development expert.\"),\n",
    "            (\"human\", \"\"\"Create a proposal for:\n",
    "            Client: {client}\n",
    "            Service/Product: {offering}\n",
    "            Budget: {budget}\n",
    "            Timeline: {timeline}\"\"\")\n",
    "        ])\n",
    "    \n",
    "    def _create_memo_template(self) -> ChatPromptTemplate:\n",
    "        return ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are writing internal company communications.\"),\n",
    "            (\"human\", \"\"\"Write a memo about:\n",
    "            Topic: {topic}\n",
    "            Audience: {audience}\n",
    "            Action items: {actions}\n",
    "            Urgency: {urgency}\"\"\")\n",
    "        ])\n",
    "    \n",
    "    def generate_document(self, doc_type: DocumentType, **kwargs) -> str:\n",
    "        \"\"\"Generate a document using the appropriate template\"\"\"\n",
    "        template = self.templates[doc_type]\n",
    "        messages = template.format_messages(**kwargs)\n",
    "        \n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "        response = llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "# Use the template factory\n",
    "factory = TemplateFactory()\n",
    "\n",
    "# Generate different documents\n",
    "print(\"Template Factory System:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Email\n",
    "email = factory.generate_document(\n",
    "    DocumentType.EMAIL,\n",
    "    recipient=\"team@company.com\",\n",
    "    subject=\"Project Update\",\n",
    "    purpose=\"inform about milestone completion\",\n",
    "    tone=\"professional but friendly\"\n",
    ")\n",
    "print(\"\\nüìß Email:\")\n",
    "print(email[:300] + \"...\")\n",
    "\n",
    "# Memo\n",
    "memo = factory.generate_document(\n",
    "    DocumentType.MEMO,\n",
    "    topic=\"New Remote Work Policy\",\n",
    "    audience=\"all employees\",\n",
    "    actions=\"review policy, submit feedback by Friday\",\n",
    "    urgency=\"high\"\n",
    ")\n",
    "print(\"\\nüìù Memo:\")\n",
    "print(memo[:300] + \"...\")\n",
    "\n",
    "print(\"\\n‚úÖ Template factory enables scalable document generation!\")\n",
    "```\n",
    "\n",
    "**Factory pattern benefits:**\n",
    "- Centralized template management\n",
    "- Easy to add new document types\n",
    "- Consistent interface\n",
    "- Reusable across applications\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "‚úÖ Creating reusable PromptTemplates with variables  \n",
    "‚úÖ Using ChatPromptTemplate for structured conversations  \n",
    "‚úÖ Working with MessagesPlaceholder for dynamic content  \n",
    "‚úÖ Implementing few-shot prompting for better results  \n",
    "‚úÖ Building template libraries and factories  \n",
    "‚úÖ Partial templates and validation techniques  \n",
    "\n",
    "### Key Takeaways\n",
    "1. **Templates ensure consistency** - Maintain quality across your application\n",
    "2. **Variables make prompts dynamic** - One template, many uses\n",
    "3. **Few-shot learning is powerful** - Teach by example without fine-tuning\n",
    "4. **ChatPromptTemplate for chat models** - Proper message structure matters\n",
    "5. **MessagesPlaceholder enables flexibility** - Dynamic conversation management\n",
    "\n",
    "### What's Next?\n",
    "In the next notebook (`03_chains_and_lcel.ipynb`), you'll learn:\n",
    "- Building chains with LangChain Expression Language (LCEL)\n",
    "- The pipe operator for component composition\n",
    "- Creating complex workflows\n",
    "- Parallel and sequential execution\n",
    "- Error handling in chains\n",
    "\n",
    "### Resources\n",
    "- [LangChain Prompts Documentation](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
    "- [Few-Shot Prompting Guide](https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples)\n",
    "- [ChatPromptTemplate API](https://python.langchain.com/docs/modules/model_io/prompts/message_prompts)\n",
    "- [Prompt Engineering Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You've mastered prompts and templates in LangChain! You can now build maintainable, reusable prompt systems for production applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}