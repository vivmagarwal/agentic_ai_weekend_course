{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain & Environment Setup\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand what LangChain is and why it's essential for building LLM applications\n",
    "- Set up your development environment with all necessary dependencies\n",
    "- Make your first LangChain API calls\n",
    "- Understand the core components of the LangChain ecosystem\n",
    "- Navigate the LangChain documentation and resources\n",
    "\n",
    "## Why This Matters: Real-World AI Applications\n",
    "\n",
    "**In AI Systems:**\n",
    "- LangChain provides a unified interface for working with multiple LLM providers\n",
    "- Enables rapid prototyping and production deployment of AI applications\n",
    "- Reduces boilerplate code and complexity\n",
    "\n",
    "**In RAG Pipelines:**\n",
    "- Standardizes document loading, splitting, and embedding generation\n",
    "- Provides pre-built components for vector stores and retrieval\n",
    "- Simplifies the integration of knowledge bases with LLMs\n",
    "\n",
    "**In Agentic AI:**\n",
    "- Offers tools and frameworks for building autonomous agents\n",
    "- Manages agent memory and conversation state\n",
    "- Enables complex multi-step reasoning and tool usage\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10+ installed\n",
    "- Basic Python knowledge (variables, functions, loops)\n",
    "- Understanding of APIs and JSON\n",
    "- OpenAI API key (free tier works)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies\n",
    "\n",
    "Run this cell first to install all required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core LangChain packages\n",
    "!pip install -q langchain langchain-openai langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 1: The Problem LangChain Solves\n",
    "\n",
    "**Concept**: Understanding why LangChain exists by comparing development with and without it.\n",
    "\n",
    "### Example 1: Building a Simple Q&A System Without LangChain\n",
    "\n",
    "**Problem**: Create a system that answers questions using an LLM\n",
    "**Expected Output**: Complex, vendor-specific code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import anthropic\n",
    "\n",
    "# Without LangChain - vendor-specific implementations\n",
    "def answer_with_openai(question):\n",
    "    \"\"\"OpenAI-specific implementation\"\"\"\n",
    "    client = openai.OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def answer_with_anthropic(question):\n",
    "    \"\"\"Anthropic-specific implementation\"\"\"\n",
    "    client = anthropic.Anthropic()\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1000,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "# Notice: Different APIs, different response structures, vendor lock-in\n",
    "print(\"Without LangChain: Multiple implementations needed!\")\n",
    "```\n",
    "\n",
    "**Why this is problematic:**\n",
    "- Different API patterns for each provider\n",
    "- Vendor lock-in makes switching providers difficult\n",
    "- No standardized way to handle prompts, memory, or tools\n",
    "- Lots of boilerplate code for common patterns\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: The Same System WITH LangChain\n",
    "\n",
    "**Problem**: Create the same Q&A system using LangChain\n",
    "**Expected Output**: Clean, provider-agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# With LangChain - unified interface\n",
    "def answer_with_langchain(question, provider=\"openai\"):\n",
    "    \"\"\"Provider-agnostic implementation\"\"\"\n",
    "    \n",
    "    # Switch providers with one line\n",
    "    if provider == \"openai\":\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    elif provider == \"anthropic\":\n",
    "        llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n",
    "    \n",
    "    # Same interface for all providers!\n",
    "    response = llm.invoke(question)\n",
    "    return response.content\n",
    "\n",
    "# Example usage\n",
    "question = \"What is LangChain?\"\n",
    "print(answer_with_langchain(question))\n",
    "```\n",
    "\n",
    "**Why this works better:**\n",
    "- One interface for all LLM providers\n",
    "- Easy to switch between providers\n",
    "- Consistent response format\n",
    "- Less code, more maintainable\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: LangChain Ecosystem Components\n",
    "\n",
    "**Problem**: Understand what components LangChain provides\n",
    "**Expected Output**: Overview of major components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# LangChain provides these core components:\n",
    "\n",
    "# 1. Models - Interface with LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 2. Prompts - Manage and optimize prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 3. Chains - Connect components\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 4. Memory - Maintain conversation state\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 5. Document Loaders - Load various data sources\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 6. Vector Stores - Semantic search\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 7. Agents - Autonomous decision-making\n",
    "from langchain.agents import create_react_agent\n",
    "\n",
    "# 8. Tools - Give LLMs capabilities\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "print(\"LangChain Components Overview:\")\n",
    "print(\"- Models: Interface with any LLM\")\n",
    "print(\"- Prompts: Template and manage prompts\")\n",
    "print(\"- Chains: Build complex workflows\")\n",
    "print(\"- Memory: Maintain conversation context\")\n",
    "print(\"- Document Processing: Load and split documents\")\n",
    "print(\"- Vector Stores: Semantic search and retrieval\")\n",
    "print(\"- Agents: Build autonomous AI systems\")\n",
    "print(\"- Tools: Extend LLM capabilities\")\n",
    "```\n",
    "\n",
    "**Why this ecosystem matters:**\n",
    "- Pre-built components for common AI patterns\n",
    "- Everything works together seamlessly\n",
    "- Battle-tested in production\n",
    "- Active community and continuous updates\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 1: Understanding the Value of LangChain\n",
    "\n",
    "**Practice**: Explore why LangChain is valuable for AI development\n",
    "\n",
    "### Exercise 1: Identify the Problems\n",
    "\n",
    "**Task**: List 3 problems you might face when building AI applications without a framework\n",
    "**Expected Output**: A list of challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Think about: What would be difficult about building a chatbot from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Common problems without a framework like LangChain:\n",
    "\n",
    "problems = [\n",
    "    \"1. Vendor lock-in: Each LLM provider has different APIs\",\n",
    "    \"2. Boilerplate code: Lots of repeated code for common patterns\",\n",
    "    \"3. Complex integrations: Connecting documents, memory, and tools is difficult\",\n",
    "    \"4. No standardization: Every developer reinvents the wheel\",\n",
    "    \"5. Production challenges: Error handling, retries, streaming all custom-built\"\n",
    "]\n",
    "\n",
    "for problem in problems:\n",
    "    print(problem)\n",
    "\n",
    "print(\"\\nLangChain solves all these problems with pre-built, tested components!\")\n",
    "```\n",
    "\n",
    "**Why these matter:**\n",
    "- Real teams face these issues daily\n",
    "- LangChain's standardization saves months of development\n",
    "- Focus on your application logic, not infrastructure\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Component Matching\n",
    "\n",
    "**Task**: Match LangChain components to real-world use cases\n",
    "**Expected Output**: Correct component for each scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Match each use case to a LangChain component:\n",
    "\n",
    "use_cases = {\n",
    "    \"Building a PDF Q&A system\": \"?\",\n",
    "    \"Remembering user preferences in a chat\": \"?\",\n",
    "    \"Searching the web for current information\": \"?\",\n",
    "    \"Finding similar documents\": \"?\",\n",
    "    \"Creating dynamic prompts with variables\": \"?\"\n",
    "}\n",
    "\n",
    "# Fill in the ? with the correct component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Matching LangChain components to use cases\n",
    "\n",
    "use_cases = {\n",
    "    \"Building a PDF Q&A system\": \"Document Loaders + Vector Stores\",\n",
    "    \"Remembering user preferences in a chat\": \"Memory components\",\n",
    "    \"Searching the web for current information\": \"Tools (like DuckDuckGo search)\",\n",
    "    \"Finding similar documents\": \"Vector Stores + Embeddings\",\n",
    "    \"Creating dynamic prompts with variables\": \"Prompt Templates\"\n",
    "}\n",
    "\n",
    "print(\"LangChain Component Matching:\")\n",
    "print(\"=\" * 50)\n",
    "for use_case, component in use_cases.items():\n",
    "    print(f\"Use Case: {use_case}\")\n",
    "    print(f\"Component: {component}\\n\")\n",
    "```\n",
    "\n",
    "**Why these matches work:**\n",
    "- Each component is purpose-built for specific AI patterns\n",
    "- Components can be combined for complex applications\n",
    "- Pre-built solutions for common use cases\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 2: Setting Up Your Environment\n",
    "\n",
    "**Concept**: Properly configure your development environment for LangChain\n",
    "\n",
    "### Example 1: Environment Variables Setup\n",
    "\n",
    "**Problem**: Securely manage API keys\n",
    "**Expected Output**: Loaded environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Method 1: Using .env file (recommended for development)\n",
    "load_dotenv()  # Loads from .env file\n",
    "\n",
    "# Method 2: Direct setting (for Google Colab or testing)\n",
    "# Uncomment and add your key if not using .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Verify key is loaded (without revealing it)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    # Mask the key for security\n",
    "    masked = api_key[:8] + \"...\" + api_key[-4:] if len(api_key) > 12 else \"***\"\n",
    "    print(f\"✅ OpenAI API key loaded: {masked}\")\n",
    "else:\n",
    "    print(\"❌ No API key found. Please set OPENAI_API_KEY\")\n",
    "    print(\"Get your key from: https://platform.openai.com/api-keys\")\n",
    "```\n",
    "\n",
    "**Why environment variables matter:**\n",
    "- Keep secrets out of code\n",
    "- Different keys for dev/prod\n",
    "- Industry best practice for security\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Verifying LangChain Installation\n",
    "\n",
    "**Problem**: Ensure all components are properly installed\n",
    "**Expected Output**: Version information and successful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Check LangChain installation\n",
    "import langchain\n",
    "import langchain_core\n",
    "import langchain_openai\n",
    "import langchain_community\n",
    "\n",
    "print(\"LangChain Installation Check:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"✅ langchain: {langchain.__version__}\")\n",
    "print(f\"✅ langchain-core: {langchain_core.__version__}\")\n",
    "print(f\"✅ langchain-openai: {langchain_openai.__version__}\")\n",
    "print(f\"✅ langchain-community: {langchain_community.__version__}\")\n",
    "\n",
    "# Test critical imports\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    print(\"\\n✅ All critical imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n❌ Import error: {e}\")\n",
    "    print(\"Run: pip install langchain langchain-openai langchain-community\")\n",
    "```\n",
    "\n",
    "**Why verification is important:**\n",
    "- Catch issues early\n",
    "- Ensure compatibility between packages\n",
    "- Confirm environment is ready for development\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 2: Set Up Your Environment\n",
    "\n",
    "**Practice**: Configure your development environment\n",
    "\n",
    "### Exercise 1: Create and Load Environment Variables\n",
    "\n",
    "**Task**: Set up your API key securely\n",
    "**Expected Output**: Confirmation that key is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Import necessary modules and load your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# For Google Colab users (uncomment if needed):\n",
    "# import getpass\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "# Check if key is loaded\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"✅ API key successfully loaded!\")\n",
    "    print(\"You're ready to use LangChain with OpenAI\")\n",
    "else:\n",
    "    print(\"⚠️ No API key found\")\n",
    "    print(\"Please set OPENAI_API_KEY in your .env file or environment\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `load_dotenv()` reads from .env file\n",
    "- `os.getenv()` safely retrieves environment variables\n",
    "- Never hardcode API keys in your code\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Verify Your Installation\n",
    "\n",
    "**Task**: Check that all required packages are installed\n",
    "**Expected Output**: List of installed packages with versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Import packages and print their versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Check all required packages\n",
    "packages_to_check = [\n",
    "    (\"langchain\", \"LangChain Core\"),\n",
    "    (\"langchain_core\", \"LangChain Core Components\"),\n",
    "    (\"langchain_openai\", \"OpenAI Integration\"),\n",
    "    (\"langchain_community\", \"Community Integrations\"),\n",
    "    (\"dotenv\", \"Environment Management\")\n",
    "]\n",
    "\n",
    "print(\"Package Installation Status:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for package_name, description in packages_to_check:\n",
    "    try:\n",
    "        package = __import__(package_name)\n",
    "        version = getattr(package, \"__version__\", \"installed\")\n",
    "        print(f\"✅ {description}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {description}: Not installed\")\n",
    "\n",
    "print(\"\\nIf any packages are missing, run:\")\n",
    "print(\"pip install langchain langchain-openai langchain-community python-dotenv\")\n",
    "```\n",
    "\n",
    "**Why this verification helps:**\n",
    "- Ensures all dependencies are present\n",
    "- Catches version mismatches early\n",
    "- Provides clear fix instructions if needed\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 3: Your First LangChain Call\n",
    "\n",
    "**Concept**: Make your first successful API call with LangChain\n",
    "\n",
    "### Example 1: Basic LLM Initialization\n",
    "\n",
    "**Problem**: Create and configure an LLM instance\n",
    "**Expected Output**: Configured ChatOpenAI object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM with configuration\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",      # Efficient, cost-effective model\n",
    "    temperature=0.7,          # Balance between creativity and consistency\n",
    "    max_tokens=150,          # Limit response length\n",
    "    timeout=30,              # API timeout in seconds\n",
    "    max_retries=2,           # Retry on failure\n",
    ")\n",
    "\n",
    "# Display configuration\n",
    "print(\"LLM Configuration:\")\n",
    "print(f\"Model: {llm.model_name}\")\n",
    "print(f\"Temperature: {llm.temperature}\")\n",
    "print(f\"Max Tokens: {llm.max_tokens}\")\n",
    "print(\"\\n✅ LLM initialized and ready!\")\n",
    "```\n",
    "\n",
    "**Why these settings matter:**\n",
    "- **model**: gpt-4o-mini is fast and affordable\n",
    "- **temperature**: 0 = deterministic, 1 = creative\n",
    "- **max_tokens**: Controls response length and cost\n",
    "- **timeout/retries**: Production reliability\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Your First API Call\n",
    "\n",
    "**Problem**: Make a simple call to the LLM\n",
    "**Expected Output**: AI-generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Make your first call!\n",
    "response = llm.invoke(\"Hello LangChain! Tell me a fun fact about AI in one sentence.\")\n",
    "\n",
    "# Display the response\n",
    "print(\"🎉 Your First LangChain Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.content)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n🚀 Congratulations! You've made your first LangChain call!\")\n",
    "```\n",
    "\n",
    "**Why this is significant:**\n",
    "- You've successfully connected to an LLM\n",
    "- The same code works with any LLM provider\n",
    "- Foundation for all future LangChain applications\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Understanding the Response Object\n",
    "\n",
    "**Problem**: Explore what the LLM returns\n",
    "**Expected Output**: Response object details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize and call LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "response = llm.invoke(\"What is Python?\")\n",
    "\n",
    "# Explore the response object\n",
    "print(\"Response Object Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Type: {type(response)}\")\n",
    "print(f\"\\nContent: {response.content[:100]}...\")\n",
    "print(f\"\\nResponse metadata:\")\n",
    "print(f\"- Token usage: {response.response_metadata.get('token_usage', 'N/A')}\")\n",
    "print(f\"- Model: {response.response_metadata.get('model_name', 'N/A')}\")\n",
    "print(f\"- Finish reason: {response.response_metadata.get('finish_reason', 'N/A')}\")\n",
    "\n",
    "# Access just the text content\n",
    "text_content = response.content\n",
    "print(f\"\\nJust the text (first 200 chars):\")\n",
    "print(text_content[:200])\n",
    "```\n",
    "\n",
    "**Why understanding responses matters:**\n",
    "- Access token usage for cost tracking\n",
    "- Check finish_reason for truncation\n",
    "- Extract metadata for logging/debugging\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 3: Make Your First LangChain Calls\n",
    "\n",
    "**Practice**: Create and use your own LLM instances\n",
    "\n",
    "### Exercise 1: Initialize Your LLM\n",
    "\n",
    "**Task**: Create a ChatOpenAI instance with custom settings\n",
    "**Expected Output**: Configured LLM ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a ChatOpenAI instance with:\n",
    "# - model: gpt-4o-mini\n",
    "# - temperature: 0.5\n",
    "# - max_tokens: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create your LLM instance\n",
    "my_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,      # Balanced creativity\n",
    "    max_tokens=100       # Keep responses concise\n",
    ")\n",
    "\n",
    "# Verify configuration\n",
    "print(\"Your LLM Configuration:\")\n",
    "print(f\"✅ Model: {my_llm.model_name}\")\n",
    "print(f\"✅ Temperature: {my_llm.temperature}\")\n",
    "print(f\"✅ Max Tokens: {my_llm.max_tokens}\")\n",
    "print(\"\\nYour LLM is ready to use!\")\n",
    "```\n",
    "\n",
    "**Why these settings:**\n",
    "- Temperature 0.5 balances consistency with creativity\n",
    "- Max tokens 100 keeps responses focused\n",
    "- gpt-4o-mini is perfect for learning\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Make Your First Call\n",
    "\n",
    "**Task**: Ask the LLM to explain LangChain in simple terms\n",
    "**Expected Output**: Simple explanation of LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Use your LLM to get an explanation of LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize your LLM\n",
    "my_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "# Make your first call\n",
    "prompt = \"Explain LangChain to me like I'm a beginner programmer in 2-3 sentences.\"\n",
    "response = my_llm.invoke(prompt)\n",
    "\n",
    "# Display the response\n",
    "print(\"🎓 LangChain Explained Simply:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.content)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n✅ Success! You've used LangChain to get AI-generated content!\")\n",
    "```\n",
    "\n",
    "**What just happened:**\n",
    "- You sent a prompt to an LLM\n",
    "- LangChain handled all the API complexity\n",
    "- You received structured response data\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Experiment with Parameters\n",
    "\n",
    "**Task**: Compare outputs with different temperature settings\n",
    "**Expected Output**: See how temperature affects responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create two LLMs with temperature 0 and 1\n",
    "# Ask both the same creative question\n",
    "# Compare the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create two LLMs with different temperatures\n",
    "deterministic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "creative_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n",
    "\n",
    "# Same prompt for both\n",
    "prompt = \"Invent a creative name for a AI-powered coffee shop.\"\n",
    "\n",
    "# Get responses\n",
    "deterministic_response = deterministic_llm.invoke(prompt)\n",
    "creative_response = creative_llm.invoke(prompt)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"Temperature Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Temperature 0 (Deterministic): {deterministic_response.content}\")\n",
    "print()\n",
    "print(f\"Temperature 1 (Creative): {creative_response.content}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n💡 Notice how temperature affects creativity and consistency!\")\n",
    "```\n",
    "\n",
    "**Key insight:**\n",
    "- Temperature 0: Consistent, predictable outputs\n",
    "- Temperature 1: More creative, varied outputs\n",
    "- Choose based on your use case needs\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional Extra Practice\n",
    "\n",
    "**Challenge yourself with these exercises that combine all concepts**\n",
    "\n",
    "### Challenge 1: Build a Simple Q&A Function\n",
    "\n",
    "**Task**: Create a reusable function that answers questions\n",
    "**Expected Output**: A function that takes questions and returns answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "def create_qa_assistant(model=\"gpt-4o-mini\", temperature=0.7):\n",
    "    \"\"\"Create a Q&A assistant using LangChain\"\"\"\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "    \n",
    "    def ask_question(question):\n",
    "        \"\"\"Ask a question and get an answer\"\"\"\n",
    "        try:\n",
    "            # Add context to make answers more helpful\n",
    "            enhanced_prompt = f\"\"\"Please provide a clear, helpful answer to this question:\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer in a friendly, informative way.\"\"\"\n",
    "            \n",
    "            response = llm.invoke(enhanced_prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    return ask_question\n",
    "\n",
    "# Create and test the assistant\n",
    "qa = create_qa_assistant(temperature=0.7)\n",
    "\n",
    "# Test with different questions\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do I get started with Python?\",\n",
    "    \"What are the benefits of using LangChain?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {qa(q)[:200]}...\\n\")  # Show first 200 chars\n",
    "```\n",
    "\n",
    "**Why this pattern is useful:**\n",
    "- Encapsulates LLM logic\n",
    "- Reusable across your application\n",
    "- Easy to test and maintain\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Multi-Provider Comparison\n",
    "\n",
    "**Task**: Create a function that can use different LLM providers\n",
    "**Expected Output**: Same interface for different providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_anthropic import ChatAnthropic  # If you have Anthropic key\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI  # If you have Google key\n",
    "\n",
    "def create_multi_provider_llm(provider=\"openai\", model=None, temperature=0.7):\n",
    "    \"\"\"Create LLM instance for different providers\"\"\"\n",
    "    \n",
    "    if provider == \"openai\":\n",
    "        model = model or \"gpt-4o-mini\"\n",
    "        return ChatOpenAI(model=model, temperature=temperature)\n",
    "    \n",
    "    # Uncomment if you have other API keys:\n",
    "    # elif provider == \"anthropic\":\n",
    "    #     model = model or \"claude-3-5-sonnet-20241022\"\n",
    "    #     return ChatAnthropic(model=model, temperature=temperature)\n",
    "    \n",
    "    # elif provider == \"google\":\n",
    "    #     model = model or \"gemini-1.5-flash\"\n",
    "    #     return ChatGoogleGenerativeAI(model=model, temperature=temperature)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "# Test with available providers\n",
    "providers_to_test = [\"openai\"]  # Add others if you have keys\n",
    "\n",
    "prompt = \"What makes Python a great programming language? (Answer in one sentence)\"\n",
    "\n",
    "for provider in providers_to_test:\n",
    "    try:\n",
    "        llm = create_multi_provider_llm(provider)\n",
    "        response = llm.invoke(prompt)\n",
    "        print(f\"{provider.upper()} Response:\")\n",
    "        print(f\"{response.content}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"{provider.upper()} Error: {e}\\n\")\n",
    "\n",
    "print(\"💡 Same interface, different providers - that's the power of LangChain!\")\n",
    "```\n",
    "\n",
    "**Why multi-provider support matters:**\n",
    "- Avoid vendor lock-in\n",
    "- Use best model for each task\n",
    "- Fallback options for reliability\n",
    "- Cost optimization\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "✅ What LangChain is and why it's valuable  \n",
    "✅ How to set up your development environment  \n",
    "✅ Making your first LangChain API calls  \n",
    "✅ Understanding LLM parameters and responses  \n",
    "✅ The core components of the LangChain ecosystem  \n",
    "\n",
    "### Key Takeaways\n",
    "1. **LangChain simplifies AI development** by providing a unified interface\n",
    "2. **Environment setup is crucial** - always use environment variables for API keys\n",
    "3. **Temperature controls creativity** - adjust based on your use case\n",
    "4. **Same code, multiple providers** - avoid vendor lock-in\n",
    "5. **Foundation for complex apps** - these basics enable RAG, agents, and more\n",
    "\n",
    "### What's Next?\n",
    "In the next notebook (`01_basic_llm_calls.ipynb`), you'll learn:\n",
    "- Working with different message types (System, Human, AI)\n",
    "- Building multi-turn conversations\n",
    "- Streaming responses for better UX\n",
    "- Error handling and retries\n",
    "- Cost optimization techniques\n",
    "\n",
    "### Resources\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [OpenAI API Keys](https://platform.openai.com/api-keys)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "- [Community Discord](https://discord.gg/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "🎉 **Congratulations!** You've completed your introduction to LangChain! You're now ready to build AI-powered applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}