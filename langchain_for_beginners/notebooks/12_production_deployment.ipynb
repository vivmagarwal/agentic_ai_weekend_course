{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **Notebook 12: Production Deployment**\n\n## **Learning Objectives**\nBy the end of this notebook, you will be able to:\n1. Deploy LangChain applications as APIs\n2. Containerize LangChain applications\n3. Implement production error handling and monitoring\n4. Apply security best practices\n5. Optimize performance and costs\n\n## **Prerequisites**\n- Completion of notebooks 00-11\n- Understanding of REST APIs\n- Basic knowledge of Docker (helpful but not required)\n\n## **Setup**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai fastapi uvicorn pydantic python-dotenv prometheus-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel, Field\n",
    "import time\n",
    "from functools import wraps\n",
    "import hashlib\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set OpenAI API key (replace with your key)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Part 1: Deploying LangChain as an API**\n\n### **Instructor Activity 1: Building a Production-Ready FastAPI Application**\n\nLet me show you how to create a production-ready API service with proper request/response models, error handling, and rate limiting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request and Response Models\n",
    "class ChatRequest(BaseModel):\n",
    "    \"\"\"Request model for chat endpoint\"\"\"\n",
    "    message: str = Field(..., description=\"User message\")\n",
    "    session_id: str = Field(..., description=\"Session ID for conversation tracking\")\n",
    "    temperature: float = Field(0.7, ge=0, le=2, description=\"Model temperature\")\n",
    "    max_tokens: Optional[int] = Field(None, description=\"Max tokens in response\")\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    \"\"\"Response model for chat endpoint\"\"\"\n",
    "    response: str\n",
    "    session_id: str\n",
    "    tokens_used: int\n",
    "    cost: float\n",
    "    timestamp: str\n",
    "\n",
    "class HealthCheckResponse(BaseModel):\n",
    "    \"\"\"Health check response\"\"\"\n",
    "    status: str\n",
    "    model_loaded: bool\n",
    "    timestamp: str\n",
    "\n",
    "# Rate Limiter\n",
    "class RateLimiter:\n",
    "    \"\"\"Simple in-memory rate limiter\"\"\"\n",
    "    def __init__(self, max_requests: int = 10, window_seconds: int = 60):\n",
    "        self.max_requests = max_requests\n",
    "        self.window_seconds = window_seconds\n",
    "        self.requests = {}\n",
    "    \n",
    "    def is_allowed(self, key: str) -> bool:\n",
    "        now = time.time()\n",
    "        \n",
    "        # Clean old requests\n",
    "        if key in self.requests:\n",
    "            self.requests[key] = [\n",
    "                timestamp for timestamp in self.requests[key]\n",
    "                if now - timestamp < self.window_seconds\n",
    "            ]\n",
    "        else:\n",
    "            self.requests[key] = []\n",
    "        \n",
    "        # Check if allowed\n",
    "        if len(self.requests[key]) < self.max_requests:\n",
    "            self.requests[key].append(now)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Production LangChain Service\n",
    "class LangChainService:\n",
    "    \"\"\"Production-ready LangChain service\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.sessions = {}\n",
    "        self.rate_limiter = RateLimiter(max_requests=10, window_seconds=60)\n",
    "        self.initialize_model()\n",
    "    \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize the model with error handling\"\"\"\n",
    "        try:\n",
    "            self.llm = ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=0.7,\n",
    "                max_retries=3,\n",
    "                request_timeout=30\n",
    "            )\n",
    "            logger.info(\"Model initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_or_create_session(self, session_id: str) -> ConversationBufferMemory:\n",
    "        \"\"\"Get or create a conversation session\"\"\"\n",
    "        if session_id not in self.sessions:\n",
    "            self.sessions[session_id] = ConversationBufferMemory(\n",
    "                return_messages=True\n",
    "            )\n",
    "        return self.sessions[session_id]\n",
    "    \n",
    "    async def process_chat(self, request: ChatRequest) -> ChatResponse:\n",
    "        \"\"\"Process chat request with full error handling\"\"\"\n",
    "        \n",
    "        # Rate limiting\n",
    "        if not self.rate_limiter.is_allowed(request.session_id):\n",
    "            raise Exception(\"Rate limit exceeded\")\n",
    "        \n",
    "        # Get session memory\n",
    "        memory = self.get_or_create_session(request.session_id)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful assistant.\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # Create chain\n",
    "        chain = prompt | self.llm\n",
    "        \n",
    "        # Process with token tracking\n",
    "        with get_openai_callback() as cb:\n",
    "            response = chain.invoke({\n",
    "                \"input\": request.message,\n",
    "                \"history\": memory.chat_memory.messages\n",
    "            })\n",
    "            \n",
    "            # Update memory\n",
    "            memory.chat_memory.add_user_message(request.message)\n",
    "            memory.chat_memory.add_ai_message(response.content)\n",
    "            \n",
    "            return ChatResponse(\n",
    "                response=response.content,\n",
    "                session_id=request.session_id,\n",
    "                tokens_used=cb.total_tokens,\n",
    "                cost=cb.total_cost,\n",
    "                timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "\n",
    "# Create FastAPI app (save as app.py)\n",
    "api_code = '''\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Production API\",\n",
    "    version=\"1.0.0\",\n",
    "    description=\"Production-ready LangChain API\"\n",
    ")\n",
    "\n",
    "# CORS configuration\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Initialize service\n",
    "service = LangChainService()\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthCheckResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return HealthCheckResponse(\n",
    "        status=\"healthy\",\n",
    "        model_loaded=service.llm is not None,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"Main chat endpoint\"\"\"\n",
    "    try:\n",
    "        return await service.process_chat(request)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/sessions/{session_id}/history\")\n",
    "async def get_session_history(session_id: str):\n",
    "    \"\"\"Get conversation history for a session\"\"\"\n",
    "    if session_id in service.sessions:\n",
    "        messages = service.sessions[session_id].chat_memory.messages\n",
    "        return {\n",
    "            \"session_id\": session_id,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"human\" if isinstance(m, HumanMessage) else \"ai\",\n",
    "                 \"content\": m.content}\n",
    "                for m in messages\n",
    "            ]\n",
    "        }\n",
    "    raise HTTPException(status_code=404, detail=\"Session not found\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "print(\"FastAPI application created!\")\n",
    "print(\"\\nTo run: python app.py\")\n",
    "print(\"API will be available at: http://localhost:8000\")\n",
    "print(\"Documentation at: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Learner Activity 1: Create Your Own API Service**\n\nBuild a production API that includes:\n1. Multiple endpoints (completion, summarization, translation)\n2. API key authentication\n3. Request validation and error handling"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a multi-endpoint API service\n",
    "# Requirements:\n",
    "# 1. Add authentication with API keys\n",
    "# 2. Create endpoints for different tasks\n",
    "# 3. Implement proper error handling\n",
    "# 4. Add request logging\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION (Hidden - Run this cell to see the solution)\n",
    "solution_1 = '''\n",
    "from fastapi import Header, HTTPException\n",
    "from typing import Optional\n",
    "import secrets\n",
    "\n",
    "# Authentication\n",
    "class APIKeyManager:\n",
    "    \"\"\"Manage API keys\"\"\"\n",
    "    def __init__(self):\n",
    "        # In production, store in database\n",
    "        self.api_keys = {\n",
    "            \"demo-key-123\": {\"name\": \"Demo User\", \"tier\": \"free\"},\n",
    "            \"premium-key-456\": {\"name\": \"Premium User\", \"tier\": \"premium\"}\n",
    "        }\n",
    "    \n",
    "    def validate_key(self, api_key: str) -> Dict:\n",
    "        if api_key in self.api_keys:\n",
    "            return self.api_keys[api_key]\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n",
    "\n",
    "# Request Models\n",
    "class CompletionRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 100\n",
    "    temperature: float = 0.7\n",
    "\n",
    "class SummarizationRequest(BaseModel):\n",
    "    text: str\n",
    "    max_length: int = 100\n",
    "    style: str = \"bullet_points\"  # bullet_points, paragraph, key_points\n",
    "\n",
    "class TranslationRequest(BaseModel):\n",
    "    text: str\n",
    "    source_language: str = \"auto\"\n",
    "    target_language: str\n",
    "\n",
    "# Enhanced Service\n",
    "class MultiServiceAPI:\n",
    "    \"\"\"Multi-endpoint LangChain service\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        self.key_manager = APIKeyManager()\n",
    "        self.request_log = []\n",
    "    \n",
    "    def log_request(self, endpoint: str, user: str, tokens: int):\n",
    "        \"\"\"Log API requests\"\"\"\n",
    "        self.request_log.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"endpoint\": endpoint,\n",
    "            \"user\": user,\n",
    "            \"tokens\": tokens\n",
    "        })\n",
    "    \n",
    "    async def complete_text(self, request: CompletionRequest, api_key: str):\n",
    "        \"\"\"Text completion endpoint\"\"\"\n",
    "        user = self.key_manager.validate_key(api_key)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\"{prompt}\")\n",
    "        chain = prompt | self.llm\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            result = chain.invoke({\"prompt\": request.prompt})\n",
    "            self.log_request(\"completion\", user[\"name\"], cb.total_tokens)\n",
    "            \n",
    "            return {\n",
    "                \"completion\": result.content,\n",
    "                \"tokens_used\": cb.total_tokens,\n",
    "                \"cost\": cb.total_cost\n",
    "            }\n",
    "    \n",
    "    async def summarize_text(self, request: SummarizationRequest, api_key: str):\n",
    "        \"\"\"Text summarization endpoint\"\"\"\n",
    "        user = self.key_manager.validate_key(api_key)\n",
    "        \n",
    "        # Different prompts based on style\n",
    "        prompts = {\n",
    "            \"bullet_points\": \"Summarize in bullet points:\\\\n{text}\",\n",
    "            \"paragraph\": \"Summarize in a paragraph:\\\\n{text}\",\n",
    "            \"key_points\": \"Extract key points:\\\\n{text}\"\n",
    "        }\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            prompts.get(request.style, prompts[\"bullet_points\"])\n",
    "        )\n",
    "        chain = prompt | self.llm\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            result = chain.invoke({\"text\": request.text})\n",
    "            self.log_request(\"summarization\", user[\"name\"], cb.total_tokens)\n",
    "            \n",
    "            return {\n",
    "                \"summary\": result.content,\n",
    "                \"style\": request.style,\n",
    "                \"tokens_used\": cb.total_tokens\n",
    "            }\n",
    "    \n",
    "    async def translate_text(self, request: TranslationRequest, api_key: str):\n",
    "        \"\"\"Text translation endpoint\"\"\"\n",
    "        user = self.key_manager.validate_key(api_key)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"Translate the following text to {target_language}:\\\\n{text}\"\n",
    "        )\n",
    "        chain = prompt | self.llm\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            result = chain.invoke({\n",
    "                \"text\": request.text,\n",
    "                \"target_language\": request.target_language\n",
    "            })\n",
    "            self.log_request(\"translation\", user[\"name\"], cb.total_tokens)\n",
    "            \n",
    "            return {\n",
    "                \"translation\": result.content,\n",
    "                \"source_language\": request.source_language,\n",
    "                \"target_language\": request.target_language,\n",
    "                \"tokens_used\": cb.total_tokens\n",
    "            }\n",
    "\n",
    "# FastAPI endpoints\n",
    "api = MultiServiceAPI()\n",
    "\n",
    "@app.post(\"/complete\")\n",
    "async def complete(\n",
    "    request: CompletionRequest,\n",
    "    api_key: str = Header(..., alias=\"X-API-Key\")\n",
    "):\n",
    "    try:\n",
    "        return await api.complete_text(request, api_key)\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Completion error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "@app.post(\"/summarize\")\n",
    "async def summarize(\n",
    "    request: SummarizationRequest,\n",
    "    api_key: str = Header(..., alias=\"X-API-Key\")\n",
    "):\n",
    "    try:\n",
    "        return await api.summarize_text(request, api_key)\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Summarization error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "@app.post(\"/translate\")\n",
    "async def translate(\n",
    "    request: TranslationRequest,\n",
    "    api_key: str = Header(..., alias=\"X-API-Key\")\n",
    "):\n",
    "    try:\n",
    "        return await api.translate_text(request, api_key)\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Translation error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "@app.get(\"/usage\")\n",
    "async def get_usage(\n",
    "    api_key: str = Header(..., alias=\"X-API-Key\")\n",
    "):\n",
    "    \"\"\"Get API usage statistics\"\"\"\n",
    "    user = api.key_manager.validate_key(api_key)\n",
    "    user_logs = [log for log in api.request_log if log[\"user\"] == user[\"name\"]]\n",
    "    \n",
    "    return {\n",
    "        \"user\": user[\"name\"],\n",
    "        \"tier\": user[\"tier\"],\n",
    "        \"total_requests\": len(user_logs),\n",
    "        \"total_tokens\": sum(log[\"tokens\"] for log in user_logs),\n",
    "        \"recent_requests\": user_logs[-10:]  # Last 10 requests\n",
    "    }\n",
    "\n",
    "print(\"Multi-endpoint API with authentication created!\")\n",
    "'''\n",
    "\n",
    "print(solution_1 if input(\"Show solution? (y/n): \").lower() == 'y' else \"Keep trying!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Part 2: Containerization and Deployment**\n\n### **Instructor Activity 2: Docker Deployment with Best Practices**\n\nLet me demonstrate how to containerize your LangChain application with production best practices."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile\n",
    "dockerfile_content = '''\n",
    "# Multi-stage build for smaller image\n",
    "FROM python:3.9-slim as builder\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    gcc \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --user --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Production stage\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd -m -u 1000 appuser\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy dependencies from builder\n",
    "COPY --from=builder /root/.local /home/appuser/.local\n",
    "\n",
    "# Copy application code\n",
    "COPY --chown=appuser:appuser . .\n",
    "\n",
    "# Switch to non-root user\n",
    "USER appuser\n",
    "\n",
    "# Add user local bin to PATH\n",
    "ENV PATH=/home/appuser/.local/bin:$PATH\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "    CMD python -c \"import requests; requests.get('http://localhost:8000/health')\"\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "# Create docker-compose.yml\n",
    "docker_compose_content = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  langchain-api:\n",
    "    build: .\n",
    "    container_name: langchain-prod\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
    "      - LOG_LEVEL=INFO\n",
    "      - MAX_WORKERS=4\n",
    "    volumes:\n",
    "      - ./logs:/app/logs\n",
    "      - ./data:/app/data\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - langchain-network\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          cpus: '2'\n",
    "          memory: 2G\n",
    "        reservations:\n",
    "          cpus: '1'\n",
    "          memory: 1G\n",
    "\n",
    "  redis:\n",
    "    image: redis:alpine\n",
    "    container_name: langchain-redis\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis-data:/data\n",
    "    networks:\n",
    "      - langchain-network\n",
    "    command: redis-server --appendonly yes\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus\n",
    "    container_name: langchain-prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "      - prometheus-data:/prometheus\n",
    "    networks:\n",
    "      - langchain-network\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "\n",
    "networks:\n",
    "  langchain-network:\n",
    "    driver: bridge\n",
    "\n",
    "volumes:\n",
    "  redis-data:\n",
    "  prometheus-data:\n",
    "'''\n",
    "\n",
    "# Create requirements.txt\n",
    "requirements_content = '''\n",
    "langchain==0.1.0\n",
    "langchain-openai==0.0.5\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "pydantic==2.5.0\n",
    "python-dotenv==1.0.0\n",
    "redis==5.0.1\n",
    "prometheus-client==0.19.0\n",
    "httpx==0.25.2\n",
    "python-multipart==0.0.6\n",
    "'''\n",
    "\n",
    "# Environment configuration\n",
    "class ProductionConfig:\n",
    "    \"\"\"Production configuration management\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.load_environment()\n",
    "    \n",
    "    def load_environment(self):\n",
    "        \"\"\"Load environment variables with validation\"\"\"\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Required variables\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is required\")\n",
    "        \n",
    "        # Optional with defaults\n",
    "        self.log_level = os.getenv(\"LOG_LEVEL\", \"INFO\")\n",
    "        self.max_workers = int(os.getenv(\"MAX_WORKERS\", \"4\"))\n",
    "        self.redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "        self.enable_metrics = os.getenv(\"ENABLE_METRICS\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    def get_logging_config(self):\n",
    "        \"\"\"Get logging configuration\"\"\"\n",
    "        return {\n",
    "            \"version\": 1,\n",
    "            \"disable_existing_loggers\": False,\n",
    "            \"formatters\": {\n",
    "                \"default\": {\n",
    "                    \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "                },\n",
    "                \"json\": {\n",
    "                    \"format\": \"%(asctime)s %(name)s %(levelname)s %(message)s\",\n",
    "                    \"class\": \"pythonjsonlogger.jsonlogger.JsonFormatter\"\n",
    "                }\n",
    "            },\n",
    "            \"handlers\": {\n",
    "                \"console\": {\n",
    "                    \"class\": \"logging.StreamHandler\",\n",
    "                    \"level\": self.log_level,\n",
    "                    \"formatter\": \"default\",\n",
    "                    \"stream\": \"ext://sys.stdout\"\n",
    "                },\n",
    "                \"file\": {\n",
    "                    \"class\": \"logging.handlers.RotatingFileHandler\",\n",
    "                    \"level\": self.log_level,\n",
    "                    \"formatter\": \"json\",\n",
    "                    \"filename\": \"logs/app.log\",\n",
    "                    \"maxBytes\": 10485760,  # 10MB\n",
    "                    \"backupCount\": 5\n",
    "                }\n",
    "            },\n",
    "            \"root\": {\n",
    "                \"level\": self.log_level,\n",
    "                \"handlers\": [\"console\", \"file\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"Docker configuration created!\")\n",
    "print(\"\\nTo build and run:\")\n",
    "print(\"1. docker-compose build\")\n",
    "print(\"2. docker-compose up -d\")\n",
    "print(\"3. docker-compose logs -f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Learner Activity 2: Build a Complete Deployment Pipeline**\n\nCreate a deployment pipeline with:\n1. Multi-stage Docker build\n2. Environment-specific configurations\n3. Kubernetes deployment manifests"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create Kubernetes deployment configuration\n",
    "# Requirements:\n",
    "# 1. Create deployment.yaml with proper resource limits\n",
    "# 2. Create service.yaml for load balancing\n",
    "# 3. Create configmap.yaml for configuration\n",
    "# 4. Create secret.yaml for sensitive data\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION (Hidden - Run this cell to see the solution)\n",
    "solution_2 = '''\n",
    "# Kubernetes Deployment Configuration\n",
    "\n",
    "# deployment.yaml\n",
    "deployment_yaml = \"\"\"\\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: langchain-api\n",
    "  labels:\n",
    "    app: langchain\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: langchain\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: langchain\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: langchain-api\n",
    "        image: langchain-api:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: OPENAI_API_KEY\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: langchain-secrets\n",
    "              key: openai-api-key\n",
    "        - name: LOG_LEVEL\n",
    "          valueFrom:\n",
    "            configMapKeyRef:\n",
    "              name: langchain-config\n",
    "              key: log.level\n",
    "        - name: REDIS_URL\n",
    "          value: redis://redis-service:6379\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "\"\"\"\n",
    "\n",
    "# service.yaml\n",
    "service_yaml = \"\"\"\\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: langchain-service\n",
    "  labels:\n",
    "    app: langchain\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: langchain\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  sessionAffinity: ClientIP\n",
    "  sessionAffinityConfig:\n",
    "    clientIP:\n",
    "      timeoutSeconds: 3600\n",
    "\"\"\"\n",
    "\n",
    "# configmap.yaml\n",
    "configmap_yaml = \"\"\"\\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: langchain-config\n",
    "data:\n",
    "  log.level: \"INFO\"\n",
    "  max.workers: \"4\"\n",
    "  rate.limit: \"100\"\n",
    "  cache.ttl: \"3600\"\n",
    "\"\"\"\n",
    "\n",
    "# secret.yaml\n",
    "secret_yaml = \"\"\"\\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: langchain-secrets\n",
    "type: Opaque\n",
    "data:\n",
    "  # Base64 encoded values\n",
    "  openai-api-key: <base64-encoded-api-key>\n",
    "  database-url: <base64-encoded-db-url>\n",
    "\"\"\"\n",
    "\n",
    "# hpa.yaml (Horizontal Pod Autoscaler)\n",
    "hpa_yaml = \"\"\"\\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: langchain-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: langchain-api\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "\"\"\"\n",
    "\n",
    "# ingress.yaml\n",
    "ingress_yaml = \"\"\"\\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: langchain-ingress\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
    "    cert-manager.io/cluster-issuer: letsencrypt-prod\n",
    "spec:\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - api.yourdomain.com\n",
    "    secretName: langchain-tls\n",
    "  rules:\n",
    "  - host: api.yourdomain.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: langchain-service\n",
    "            port:\n",
    "              number: 80\n",
    "\"\"\"\n",
    "\n",
    "# Helm values.yaml for more advanced deployment\n",
    "helm_values = \"\"\"\\n",
    "replicaCount: 3\n",
    "\n",
    "image:\n",
    "  repository: your-registry/langchain-api\n",
    "  pullPolicy: IfNotPresent\n",
    "  tag: \"1.0.0\"\n",
    "\n",
    "service:\n",
    "  type: ClusterIP\n",
    "  port: 80\n",
    "\n",
    "ingress:\n",
    "  enabled: true\n",
    "  className: nginx\n",
    "  hosts:\n",
    "    - host: api.yourdomain.com\n",
    "      paths:\n",
    "        - path: /\n",
    "          pathType: Prefix\n",
    "  tls:\n",
    "    - secretName: langchain-tls\n",
    "      hosts:\n",
    "        - api.yourdomain.com\n",
    "\n",
    "resources:\n",
    "  limits:\n",
    "    cpu: 500m\n",
    "    memory: 1Gi\n",
    "  requests:\n",
    "    cpu: 250m\n",
    "    memory: 512Mi\n",
    "\n",
    "autoscaling:\n",
    "  enabled: true\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  targetCPUUtilizationPercentage: 70\n",
    "\n",
    "env:\n",
    "  - name: LOG_LEVEL\n",
    "    value: \"INFO\"\n",
    "  - name: REDIS_URL\n",
    "    value: \"redis://redis:6379\"\n",
    "\n",
    "secrets:\n",
    "  - name: OPENAI_API_KEY\n",
    "    valueFrom:\n",
    "      secretKeyRef:\n",
    "        name: langchain-secrets\n",
    "        key: openai-api-key\n",
    "\"\"\"\n",
    "\n",
    "print(\"Kubernetes deployment configuration created!\")\n",
    "print(\"\\\\nTo deploy:\")\n",
    "print(\"1. kubectl apply -f configmap.yaml\")\n",
    "print(\"2. kubectl apply -f secret.yaml\")\n",
    "print(\"3. kubectl apply -f deployment.yaml\")\n",
    "print(\"4. kubectl apply -f service.yaml\")\n",
    "print(\"5. kubectl apply -f hpa.yaml\")\n",
    "print(\"6. kubectl apply -f ingress.yaml\")\n",
    "'''\n",
    "\n",
    "print(solution_2 if input(\"Show solution? (y/n): \").lower() == 'y' else \"Keep trying!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Part 3: Monitoring, Security, and Optimization**\n\n### **Instructor Activity 3: Production Monitoring and Security**\n\nLet me show you how to implement comprehensive monitoring, security, and optimization for production."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "import asyncio\n",
    "from typing import Callable\n",
    "from functools import lru_cache\n",
    "import jwt\n",
    "\n",
    "# Metrics\n",
    "request_count = Counter('langchain_requests_total', 'Total requests', ['endpoint', 'status'])\n",
    "request_duration = Histogram('langchain_request_duration_seconds', 'Request duration', ['endpoint'])\n",
    "active_sessions = Gauge('langchain_active_sessions', 'Active chat sessions')\n",
    "token_usage = Counter('langchain_tokens_total', 'Total tokens used', ['model', 'operation'])\n",
    "\n",
    "# Security Manager\n",
    "class SecurityManager:\n",
    "    \"\"\"Handle security concerns\"\"\"\n",
    "    \n",
    "    def __init__(self, secret_key: str):\n",
    "        self.secret_key = secret_key\n",
    "        self.blocked_ips = set()\n",
    "        self.rate_limits = {}\n",
    "    \n",
    "    def generate_jwt(self, user_id: str, tier: str) -> str:\n",
    "        \"\"\"Generate JWT token\"\"\"\n",
    "        payload = {\n",
    "            \"user_id\": user_id,\n",
    "            \"tier\": tier,\n",
    "            \"exp\": datetime.utcnow() + timedelta(hours=24),\n",
    "            \"iat\": datetime.utcnow()\n",
    "        }\n",
    "        return jwt.encode(payload, self.secret_key, algorithm=\"HS256\")\n",
    "    \n",
    "    def verify_jwt(self, token: str) -> Dict:\n",
    "        \"\"\"Verify JWT token\"\"\"\n",
    "        try:\n",
    "            return jwt.decode(token, self.secret_key, algorithms=[\"HS256\"])\n",
    "        except jwt.ExpiredSignatureError:\n",
    "            raise HTTPException(status_code=401, detail=\"Token expired\")\n",
    "        except jwt.InvalidTokenError:\n",
    "            raise HTTPException(status_code=401, detail=\"Invalid token\")\n",
    "    \n",
    "    def sanitize_input(self, text: str) -> str:\n",
    "        \"\"\"Sanitize user input\"\"\"\n",
    "        # Remove potential injection patterns\n",
    "        dangerous_patterns = [\n",
    "            r\"<script.*?>.*?</script>\",\n",
    "            r\"javascript:\",\n",
    "            r\"on\\w+=\",\n",
    "            r\"eval\\(\",\n",
    "            r\"exec\\(\"\n",
    "        ]\n",
    "        \n",
    "        import re\n",
    "        for pattern in dangerous_patterns:\n",
    "            text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def check_content_safety(self, text: str) -> bool:\n",
    "        \"\"\"Check content for safety issues\"\"\"\n",
    "        # Implement content moderation\n",
    "        # In production, use OpenAI's moderation API or similar\n",
    "        return True\n",
    "\n",
    "# Caching Layer\n",
    "class CacheManager:\n",
    "    \"\"\"Manage response caching\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_url: str = None):\n",
    "        self.cache = {}\n",
    "        self.ttl = 3600  # 1 hour\n",
    "        \n",
    "        # Use Redis in production\n",
    "        if redis_url:\n",
    "            import redis\n",
    "            self.redis_client = redis.from_url(redis_url)\n",
    "        else:\n",
    "            self.redis_client = None\n",
    "    \n",
    "    def get_cache_key(self, prompt: str, params: Dict) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        content = f\"{prompt}:{json.dumps(params, sort_keys=True)}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    async def get(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Get cached response\"\"\"\n",
    "        if self.redis_client:\n",
    "            value = self.redis_client.get(key)\n",
    "            return value.decode() if value else None\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    async def set(self, key: str, value: str, ttl: int = None):\n",
    "        \"\"\"Set cached response\"\"\"\n",
    "        ttl = ttl or self.ttl\n",
    "        if self.redis_client:\n",
    "            self.redis_client.setex(key, ttl, value)\n",
    "        else:\n",
    "            self.cache[key] = value\n",
    "\n",
    "# Performance Optimizer\n",
    "class PerformanceOptimizer:\n",
    "    \"\"\"Optimize LangChain performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_pool = {}\n",
    "        self.batch_queue = []\n",
    "        self.batch_size = 10\n",
    "        self.batch_timeout = 1.0\n",
    "    \n",
    "    @lru_cache(maxsize=5)\n",
    "    def get_model(self, model_name: str, temperature: float):\n",
    "        \"\"\"Get cached model instance\"\"\"\n",
    "        return ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_retries=3\n",
    "        )\n",
    "    \n",
    "    async def batch_process(self, requests: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process requests in batches\"\"\"\n",
    "        # Batch similar requests together\n",
    "        batches = {}\n",
    "        for req in requests:\n",
    "            key = f\"{req['model']}:{req['temperature']}\"\n",
    "            if key not in batches:\n",
    "                batches[key] = []\n",
    "            batches[key].append(req)\n",
    "        \n",
    "        results = []\n",
    "        for key, batch in batches.items():\n",
    "            model_name, temp = key.split(\":\")\n",
    "            model = self.get_model(model_name, float(temp))\n",
    "            \n",
    "            # Process batch\n",
    "            batch_results = await asyncio.gather(*[\n",
    "                self.process_single(model, req) for req in batch\n",
    "            ])\n",
    "            results.extend(batch_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def process_single(self, model, request):\n",
    "        \"\"\"Process single request\"\"\"\n",
    "        # Implementation here\n",
    "        pass\n",
    "\n",
    "# Complete Production System\n",
    "class ProductionLangChain:\n",
    "    \"\"\"Production-ready LangChain system\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProductionConfig):\n",
    "        self.config = config\n",
    "        self.security = SecurityManager(secret_key=\"your-secret-key\")\n",
    "        self.cache = CacheManager(redis_url=config.redis_url)\n",
    "        self.optimizer = PerformanceOptimizer()\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    \n",
    "    async def process_request(self, request: Dict) -> Dict:\n",
    "        \"\"\"Process request with all production features\"\"\"\n",
    "        \n",
    "        # Start metrics\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Security checks\n",
    "            sanitized_input = self.security.sanitize_input(request[\"message\"])\n",
    "            if not self.security.check_content_safety(sanitized_input):\n",
    "                raise ValueError(\"Content safety check failed\")\n",
    "            \n",
    "            # Check cache\n",
    "            cache_key = self.cache.get_cache_key(\n",
    "                sanitized_input, \n",
    "                {\"temperature\": request.get(\"temperature\", 0.7)}\n",
    "            )\n",
    "            cached_response = await self.cache.get(cache_key)\n",
    "            \n",
    "            if cached_response:\n",
    "                logger.info(\"Cache hit\")\n",
    "                return json.loads(cached_response)\n",
    "            \n",
    "            # Process with LangChain\n",
    "            with get_openai_callback() as cb:\n",
    "                response = await self.llm.ainvoke(sanitized_input)\n",
    "                \n",
    "                # Update metrics\n",
    "                token_usage.labels(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    operation=\"chat\"\n",
    "                ).inc(cb.total_tokens)\n",
    "                \n",
    "                result = {\n",
    "                    \"response\": response.content,\n",
    "                    \"tokens\": cb.total_tokens,\n",
    "                    \"cost\": cb.total_cost,\n",
    "                    \"cached\": False\n",
    "                }\n",
    "                \n",
    "                # Cache response\n",
    "                await self.cache.set(cache_key, json.dumps(result))\n",
    "                \n",
    "                return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log error\n",
    "            logger.error(f\"Request processing error: {e}\")\n",
    "            request_count.labels(endpoint=\"chat\", status=\"error\").inc()\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            # Record metrics\n",
    "            duration = time.time() - start_time\n",
    "            request_duration.labels(endpoint=\"chat\").observe(duration)\n",
    "            request_count.labels(endpoint=\"chat\", status=\"success\").inc()\n",
    "\n",
    "# Cost Optimization\n",
    "class CostOptimizer:\n",
    "    \"\"\"Optimize API costs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_costs = {\n",
    "            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002},\n",
    "            \"gpt-3.5-turbo-16k\": {\"input\": 0.003, \"output\": 0.004}\n",
    "        }\n",
    "        self.usage_history = []\n",
    "    \n",
    "    def select_model(self, task_complexity: str, max_cost: float) -> str:\n",
    "        \"\"\"Select appropriate model based on task and budget\"\"\"\n",
    "        if task_complexity == \"simple\" and max_cost < 0.01:\n",
    "            return \"gpt-3.5-turbo\"\n",
    "        elif task_complexity == \"complex\" and max_cost > 0.1:\n",
    "            return \"gpt-4\"\n",
    "        else:\n",
    "            return \"gpt-3.5-turbo-16k\"\n",
    "    \n",
    "    def estimate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Estimate cost for a request\"\"\"\n",
    "        costs = self.model_costs.get(model, self.model_costs[\"gpt-3.5-turbo\"])\n",
    "        input_cost = (input_tokens / 1000) * costs[\"input\"]\n",
    "        output_cost = (output_tokens / 1000) * costs[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def get_usage_report(self) -> Dict:\n",
    "        \"\"\"Generate usage and cost report\"\"\"\n",
    "        total_cost = sum(item[\"cost\"] for item in self.usage_history)\n",
    "        total_tokens = sum(item[\"tokens\"] for item in self.usage_history)\n",
    "        \n",
    "        return {\n",
    "            \"total_cost\": total_cost,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"average_cost_per_request\": total_cost / len(self.usage_history) if self.usage_history else 0,\n",
    "            \"requests_count\": len(self.usage_history)\n",
    "        }\n",
    "\n",
    "print(\"Production monitoring and security system created!\")\n",
    "print(\"\\nKey features implemented:\")\n",
    "print(\"- JWT authentication\")\n",
    "print(\"- Input sanitization\")\n",
    "print(\"- Response caching\")\n",
    "print(\"- Prometheus metrics\")\n",
    "print(\"- Cost optimization\")\n",
    "print(\"- Performance monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Learner Activity 3: Build a Complete Production System**\n\nCreate a production-ready system with:\n1. Complete observability (logs, metrics, traces)\n2. A/B testing capability\n3. Automatic failover and circuit breakers\n4. Cost tracking and optimization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a complete production system\n",
    "# Requirements:\n",
    "# 1. Implement distributed tracing\n",
    "# 2. Add A/B testing for model selection\n",
    "# 3. Implement circuit breakers\n",
    "# 4. Create cost tracking dashboard\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION (Hidden - Run this cell to see the solution)\n",
    "solution_3 = '''\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.exporter.jaeger import JaegerExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "import random\n",
    "from enum import Enum\n",
    "\n",
    "# Circuit Breaker States\n",
    "class CircuitState(Enum):\n",
    "    CLOSED = \"closed\"\n",
    "    OPEN = \"open\"\n",
    "    HALF_OPEN = \"half_open\"\n",
    "\n",
    "# Circuit Breaker\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker for fault tolerance\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 5, timeout: int = 60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = CircuitState.CLOSED\n",
    "    \n",
    "    def call(self, func: Callable, *args, **kwargs):\n",
    "        \"\"\"Execute function with circuit breaker\"\"\"\n",
    "        if self.state == CircuitState.OPEN:\n",
    "            if time.time() - self.last_failure_time > self.timeout:\n",
    "                self.state = CircuitState.HALF_OPEN\n",
    "            else:\n",
    "                raise Exception(\"Circuit breaker is OPEN\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            if self.state == CircuitState.HALF_OPEN:\n",
    "                self.state = CircuitState.CLOSED\n",
    "                self.failure_count = 0\n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = time.time()\n",
    "            \n",
    "            if self.failure_count >= self.failure_threshold:\n",
    "                self.state = CircuitState.OPEN\n",
    "            \n",
    "            raise e\n",
    "\n",
    "# A/B Testing Manager\n",
    "class ABTestManager:\n",
    "    \"\"\"Manage A/B testing for models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def create_experiment(self, name: str, variants: Dict[str, Dict]):\n",
    "        \"\"\"Create new A/B test\"\"\"\n",
    "        self.experiments[name] = {\n",
    "            \"variants\": variants,\n",
    "            \"results\": {v: {\"count\": 0, \"success\": 0, \"tokens\": 0, \"cost\": 0} \n",
    "                       for v in variants}\n",
    "        }\n",
    "    \n",
    "    def select_variant(self, experiment_name: str, user_id: str = None) -> str:\n",
    "        \"\"\"Select variant for user\"\"\"\n",
    "        if user_id:\n",
    "            # Consistent hashing for user\n",
    "            hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n",
    "            variants = list(self.experiments[experiment_name][\"variants\"].keys())\n",
    "            return variants[hash_value % len(variants)]\n",
    "        else:\n",
    "            # Random selection\n",
    "            return random.choice(list(self.experiments[experiment_name][\"variants\"].keys()))\n",
    "    \n",
    "    def record_result(self, experiment: str, variant: str, success: bool, \n",
    "                     tokens: int, cost: float):\n",
    "        \"\"\"Record experiment result\"\"\"\n",
    "        results = self.experiments[experiment][\"results\"][variant]\n",
    "        results[\"count\"] += 1\n",
    "        if success:\n",
    "            results[\"success\"] += 1\n",
    "        results[\"tokens\"] += tokens\n",
    "        results[\"cost\"] += cost\n",
    "    \n",
    "    def get_results(self, experiment: str) -> Dict:\n",
    "        \"\"\"Get experiment results with statistical significance\"\"\"\n",
    "        results = self.experiments[experiment][\"results\"]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        for variant, data in results.items():\n",
    "            if data[\"count\"] > 0:\n",
    "                data[\"success_rate\"] = data[\"success\"] / data[\"count\"]\n",
    "                data[\"avg_tokens\"] = data[\"tokens\"] / data[\"count\"]\n",
    "                data[\"avg_cost\"] = data[\"cost\"] / data[\"count\"]\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Distributed Tracing\n",
    "class TracingManager:\n",
    "    \"\"\"Manage distributed tracing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Configure Jaeger exporter\n",
    "        trace.set_tracer_provider(TracerProvider())\n",
    "        self.tracer = trace.get_tracer(__name__)\n",
    "        \n",
    "        # In production, configure actual Jaeger endpoint\n",
    "        # jaeger_exporter = JaegerExporter(\n",
    "        #     agent_host_name=\"localhost\",\n",
    "        #     agent_port=6831,\n",
    "        # )\n",
    "        # span_processor = BatchSpanProcessor(jaeger_exporter)\n",
    "        # trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "    \n",
    "    def trace_operation(self, name: str):\n",
    "        \"\"\"Create trace span\"\"\"\n",
    "        return self.tracer.start_as_current_span(name)\n",
    "\n",
    "# Complete Production System\n",
    "class ProductionSystemComplete:\n",
    "    \"\"\"Complete production-ready system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.circuit_breakers = {\n",
    "            \"openai\": CircuitBreaker(failure_threshold=5, timeout=60),\n",
    "            \"anthropic\": CircuitBreaker(failure_threshold=3, timeout=30)\n",
    "        }\n",
    "        self.ab_testing = ABTestManager()\n",
    "        self.tracing = TracingManager()\n",
    "        self.cost_tracker = CostTracker()\n",
    "        \n",
    "        # Setup A/B test\n",
    "        self.ab_testing.create_experiment(\n",
    "            \"model_selection\",\n",
    "            {\n",
    "                \"gpt-3.5\": {\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.7},\n",
    "                \"gpt-4\": {\"model\": \"gpt-4\", \"temperature\": 0.5}\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    async def process_with_failover(self, request: Dict) -> Dict:\n",
    "        \"\"\"Process with automatic failover\"\"\"\n",
    "        \n",
    "        with self.tracing.trace_operation(\"process_request\"):\n",
    "            # Select variant for A/B testing\n",
    "            variant = self.ab_testing.select_variant(\n",
    "                \"model_selection\", \n",
    "                request.get(\"user_id\")\n",
    "            )\n",
    "            model_config = self.ab_testing.experiments[\"model_selection\"][\"variants\"][variant]\n",
    "            \n",
    "            # Try primary provider\n",
    "            try:\n",
    "                with self.tracing.trace_operation(\"primary_call\"):\n",
    "                    result = await self.circuit_breakers[\"openai\"].call(\n",
    "                        self.call_openai, \n",
    "                        request[\"message\"],\n",
    "                        model_config\n",
    "                    )\n",
    "                    \n",
    "                    # Record A/B test result\n",
    "                    self.ab_testing.record_result(\n",
    "                        \"model_selection\",\n",
    "                        variant,\n",
    "                        True,\n",
    "                        result[\"tokens\"],\n",
    "                        result[\"cost\"]\n",
    "                    )\n",
    "                    \n",
    "                    # Track costs\n",
    "                    self.cost_tracker.record_usage(\n",
    "                        model_config[\"model\"],\n",
    "                        result[\"tokens\"],\n",
    "                        result[\"cost\"]\n",
    "                    )\n",
    "                    \n",
    "                    return result\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Primary provider failed: {e}\")\n",
    "                \n",
    "                # Failover to secondary provider\n",
    "                with self.tracing.trace_operation(\"failover_call\"):\n",
    "                    try:\n",
    "                        return await self.circuit_breakers[\"anthropic\"].call(\n",
    "                            self.call_anthropic,\n",
    "                            request[\"message\"]\n",
    "                        )\n",
    "                    except Exception as e2:\n",
    "                        logger.error(f\"All providers failed: {e2}\")\n",
    "                        raise\n",
    "    \n",
    "    async def call_openai(self, message: str, config: Dict) -> Dict:\n",
    "        \"\"\"Call OpenAI API\"\"\"\n",
    "        llm = ChatOpenAI(model=config[\"model\"], temperature=config[\"temperature\"])\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            response = await llm.ainvoke(message)\n",
    "            return {\n",
    "                \"response\": response.content,\n",
    "                \"tokens\": cb.total_tokens,\n",
    "                \"cost\": cb.total_cost,\n",
    "                \"provider\": \"openai\"\n",
    "            }\n",
    "    \n",
    "    async def call_anthropic(self, message: str) -> Dict:\n",
    "        \"\"\"Call Anthropic API (fallback)\"\"\"\n",
    "        # Implementation for Anthropic\n",
    "        return {\n",
    "            \"response\": \"Fallback response\",\n",
    "            \"tokens\": 100,\n",
    "            \"cost\": 0.01,\n",
    "            \"provider\": \"anthropic\"\n",
    "        }\n",
    "\n",
    "# Cost Tracking Dashboard\n",
    "class CostTracker:\n",
    "    \"\"\"Track and visualize costs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.usage_data = []\n",
    "        self.budget_limits = {\n",
    "            \"daily\": 100.0,\n",
    "            \"monthly\": 2000.0\n",
    "        }\n",
    "    \n",
    "    def record_usage(self, model: str, tokens: int, cost: float):\n",
    "        \"\"\"Record usage data\"\"\"\n",
    "        self.usage_data.append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"model\": model,\n",
    "            \"tokens\": tokens,\n",
    "            \"cost\": cost\n",
    "        })\n",
    "        \n",
    "        # Check budget\n",
    "        if self.is_over_budget():\n",
    "            logger.warning(\"Budget limit exceeded!\")\n",
    "    \n",
    "    def is_over_budget(self) -> bool:\n",
    "        \"\"\"Check if over budget\"\"\"\n",
    "        today_cost = sum(\n",
    "            item[\"cost\"] for item in self.usage_data\n",
    "            if item[\"timestamp\"].date() == datetime.now().date()\n",
    "        )\n",
    "        return today_cost > self.budget_limits[\"daily\"]\n",
    "    \n",
    "    def get_dashboard_data(self) -> Dict:\n",
    "        \"\"\"Get data for cost dashboard\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        today_data = [d for d in self.usage_data if d[\"timestamp\"].date() == now.date()]\n",
    "        month_data = [d for d in self.usage_data if d[\"timestamp\"].month == now.month]\n",
    "        \n",
    "        return {\n",
    "            \"today\": {\n",
    "                \"cost\": sum(d[\"cost\"] for d in today_data),\n",
    "                \"tokens\": sum(d[\"tokens\"] for d in today_data),\n",
    "                \"requests\": len(today_data),\n",
    "                \"budget_remaining\": self.budget_limits[\"daily\"] - sum(d[\"cost\"] for d in today_data)\n",
    "            },\n",
    "            \"month\": {\n",
    "                \"cost\": sum(d[\"cost\"] for d in month_data),\n",
    "                \"tokens\": sum(d[\"tokens\"] for d in month_data),\n",
    "                \"requests\": len(month_data),\n",
    "                \"budget_remaining\": self.budget_limits[\"monthly\"] - sum(d[\"cost\"] for d in month_data)\n",
    "            },\n",
    "            \"by_model\": self.get_costs_by_model(),\n",
    "            \"hourly_trend\": self.get_hourly_trend()\n",
    "        }\n",
    "    \n",
    "    def get_costs_by_model(self) -> Dict:\n",
    "        \"\"\"Get costs breakdown by model\"\"\"\n",
    "        costs = {}\n",
    "        for item in self.usage_data:\n",
    "            model = item[\"model\"]\n",
    "            if model not in costs:\n",
    "                costs[model] = {\"cost\": 0, \"tokens\": 0, \"count\": 0}\n",
    "            costs[model][\"cost\"] += item[\"cost\"]\n",
    "            costs[model][\"tokens\"] += item[\"tokens\"]\n",
    "            costs[model][\"count\"] += 1\n",
    "        return costs\n",
    "    \n",
    "    def get_hourly_trend(self) -> List:\n",
    "        \"\"\"Get hourly usage trend\"\"\"\n",
    "        hourly = {}\n",
    "        for item in self.usage_data:\n",
    "            hour = item[\"timestamp\"].hour\n",
    "            if hour not in hourly:\n",
    "                hourly[hour] = {\"cost\": 0, \"requests\": 0}\n",
    "            hourly[hour][\"cost\"] += item[\"cost\"]\n",
    "            hourly[hour][\"requests\"] += 1\n",
    "        \n",
    "        return [\n",
    "            {\"hour\": h, **data} \n",
    "            for h, data in sorted(hourly.items())\n",
    "        ]\n",
    "\n",
    "# Usage example\n",
    "system = ProductionSystemComplete()\n",
    "\n",
    "# Process request with all production features\n",
    "async def main():\n",
    "    request = {\n",
    "        \"message\": \"Tell me about Python\",\n",
    "        \"user_id\": \"user123\"\n",
    "    }\n",
    "    \n",
    "    result = await system.process_with_failover(request)\n",
    "    print(f\"Result: {result}\")\n",
    "    \n",
    "    # Get A/B test results\n",
    "    ab_results = system.ab_testing.get_results(\"model_selection\")\n",
    "    print(f\"A/B Test Results: {ab_results}\")\n",
    "    \n",
    "    # Get cost dashboard\n",
    "    dashboard = system.cost_tracker.get_dashboard_data()\n",
    "    print(f\"Cost Dashboard: {dashboard}\")\n",
    "\n",
    "print(\"Complete production system with:\")\n",
    "print(\"- Circuit breakers for fault tolerance\")\n",
    "print(\"- A/B testing for model selection\")\n",
    "print(\"- Distributed tracing with Jaeger\")\n",
    "print(\"- Cost tracking and budget management\")\n",
    "print(\"- Automatic failover between providers\")\n",
    "'''\n",
    "\n",
    "print(solution_3 if input(\"Show solution? (y/n): \").lower() == 'y' else \"Keep trying!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Summary and Next Steps**\n\n### **What You've Learned**\n1. **API Deployment**: FastAPI with proper models, authentication, and rate limiting\n2. **Containerization**: Docker best practices and Kubernetes deployment\n3. **Security**: JWT authentication, input sanitization, content moderation\n4. **Monitoring**: Prometheus metrics, distributed tracing, logging\n5. **Optimization**: Caching, batching, model selection, cost tracking\n6. **Resilience**: Circuit breakers, failover, A/B testing\n\n### **Production Checklist**\n- [ ] API documentation and versioning\n- [ ] Comprehensive error handling\n- [ ] Security measures (auth, sanitization, rate limiting)\n- [ ] Monitoring and alerting\n- [ ] Performance optimization\n- [ ] Cost tracking and optimization\n- [ ] Deployment automation (CI/CD)\n- [ ] Disaster recovery plan\n- [ ] Load testing and capacity planning\n- [ ] Compliance and data privacy\n\n### **Best Practices**\n1. **Start small**: Deploy MVP, then iterate\n2. **Monitor everything**: You can't improve what you don't measure\n3. **Plan for failure**: Everything will fail eventually\n4. **Optimize costs**: Track and optimize API usage\n5. **Security first**: Never compromise on security\n6. **Document everything**: Future you will thank you\n\n### **Further Learning**\n- LangSmith for production monitoring\n- LangServe for deployment\n- Cloud provider specific optimizations (AWS, GCP, Azure)\n- Advanced caching strategies\n- Multi-region deployment\n- GraphQL APIs for LangChain\n\n### **Final Project Ideas**\n1. Build a production-ready chatbot with full observability\n2. Create a multi-tenant LangChain SaaS platform\n3. Implement a cost-optimized document processing pipeline\n4. Deploy a globally distributed AI API with <100ms latency\n\nCongratulations! You've completed the LangChain for Beginners course and are ready to deploy production applications! \""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}