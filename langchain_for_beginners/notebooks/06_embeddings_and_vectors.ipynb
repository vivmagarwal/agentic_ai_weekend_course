{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **Embeddings and Vector Stores with LangChain**\n\n## **Learning Objectives**\nBy the end of this notebook, you will be able to:\n- Generate embeddings from text using various providers\n- Understand vector representations and similarity\n- Work with vector stores (Chroma, FAISS, etc.)\n- Perform semantic search and retrieval\n- Optimize embedding strategies for cost and performance\n- Build vector databases for RAG systems\n\n## **Why This Matters: The Heart of Semantic Search**\n\n**In RAG Systems:**\n- Embeddings enable semantic understanding\n- Vector stores provide fast retrieval\n- Similarity search finds relevant context\n\n**In Search Applications:**\n- Go beyond keyword matching\n- Find conceptually similar content\n- Enable multilingual search\n\n## **Prerequisites**\n- Completed notebooks 00-05\n- Understanding of document processing\n- Basic linear algebra concepts helpful"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Setup: Install and Import Dependencies**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-community langchain-openai chromadb faiss-cpu tiktoken\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "\n",
    "# Embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Vector stores\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# For splitting\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… Ready to work with embeddings and vectors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 1: Understanding Embeddings**\n\n### **Example 1: Generating and Comparing Embeddings**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nimport numpy as np\n\n# **Initialize embeddings model**\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\"  # Newer, cheaper model\n)\n\n# **Generate embeddings for similar and different texts**\ntexts = [\n    \"Machine learning is a type of artificial intelligence.\",\n    \"AI and ML are closely related technologies.\",\n    \"The weather today is sunny and warm.\",\n    \"Deep learning is a subset of machine learning.\"\n]\n\n# **Generate embeddings**\nvector_embeddings = embeddings.embed_documents(texts)\n\nprint(\"Embedding Analysis:\")\nprint(\"=\" * 50)\nprint(f\"Embedding dimensions: {len(vector_embeddings[0])}\")\nprint(f\"Number of texts: {len(texts)}\")\n\n# **Calculate cosine similarity**\ndef cosine_similarity(v1, v2):\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n# **Compare all pairs**\nprint(\"\\nSimilarity Matrix:\")\nfor i in range(len(texts)):\n    for j in range(i+1, len(texts)):\n        similarity = cosine_similarity(\n            vector_embeddings[i], \n            vector_embeddings[j]\n        )\n        print(f\"Text {i+1} vs Text {j+1}: {similarity:.3f}\")\n        print(f\"  '{texts[i][:30]}...' vs '{texts[j][:30]}...'\")\n\n# **Query embedding**\nquery = \"What is artificial intelligence?\"\nquery_embedding = embeddings.embed_query(query)\n\nprint(f\"\\nQuery: '{query}'\")\nprint(\"Relevance scores:\")\nfor i, text in enumerate(texts):\n    score = cosine_similarity(query_embedding, vector_embeddings[i])\n    print(f\"  {score:.3f}: {text[:50]}...\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Different Embedding Providers**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# **Different embedding models**\nembedding_models = {\n    \"OpenAI\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n    \"HuggingFace\": HuggingFaceEmbeddings(\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n    )\n}\n\ntest_text = \"LangChain is great for building LLM applications.\"\n\nprint(\"Embedding Model Comparison:\")\nprint(\"=\" * 50)\n\nfor name, model in embedding_models.items():\n    try:\n        embedding = model.embed_query(test_text)\n        print(f\"\\n{name}:\")\n        print(f\"  Dimensions: {len(embedding)}\")\n        print(f\"  Sample values: {embedding[:5]}\")\n        print(f\"  Cost: {'Paid' if 'OpenAI' in name else 'Free'}\")\n    except Exception as e:\n        print(f\"\\n{name}: Error - {str(e)[:50]}\")\n\nprint(\"\\nðŸ’¡ Choose embeddings based on cost, quality, and speed needs!\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 1: Practice with Embeddings**\n\n### **Exercise: Build a Semantic Similarity Checker**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a function that:\n",
    "# 1. Takes two texts as input\n",
    "# 2. Generates embeddings\n",
    "# 3. Calculates similarity\n",
    "# 4. Returns if they're similar (threshold > 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\ndef semantic_similarity_checker(text1: str, text2: str, threshold: float = 0.8):\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n    \n    vec1 = embeddings.embed_query(text1)\n    vec2 = embeddings.embed_query(text2)\n    \n    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n    \n    is_similar = similarity > threshold\n    \n    print(f\"Text 1: {text1[:50]}...\")\n    print(f\"Text 2: {text2[:50]}...\")\n    print(f\"Similarity: {similarity:.3f}\")\n    print(f\"Similar? {is_similar}\")\n    \n    return is_similar\n\n# **Test**\npairs = [\n    (\"Python is a programming language\", \"Python is used for coding\"),\n    (\"The sky is blue\", \"Grass is green\")\n]\n\nfor t1, t2 in pairs:\n    semantic_similarity_checker(t1, t2)\n    print()\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 2: Vector Stores**\n\n### **Example 1: Creating and Querying Chroma**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.documents import Document\n\n# **Create documents**\ndocs = [\n    Document(\n        page_content=\"LangChain is a framework for building LLM applications.\",\n        metadata={\"source\": \"intro.txt\", \"topic\": \"langchain\"}\n    ),\n    Document(\n        page_content=\"Vector stores enable semantic search over documents.\",\n        metadata={\"source\": \"vectors.txt\", \"topic\": \"search\"}\n    ),\n    Document(\n        page_content=\"RAG combines retrieval with generation for better results.\",\n        metadata={\"source\": \"rag.txt\", \"topic\": \"rag\"}\n    ),\n    Document(\n        page_content=\"Embeddings convert text into numerical vectors.\",\n        metadata={\"source\": \"embeddings.txt\", \"topic\": \"embeddings\"}\n    )\n]\n\n# **Create vector store**\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(\n    documents=docs,\n    embedding=embeddings,\n    collection_name=\"demo_collection\"\n)\n\nprint(\"Vector Store Created!\")\nprint(f\"Documents: {vectorstore._collection.count()}\")\n\n# **Similarity search**\nquery = \"How do I search documents?\"\nresults = vectorstore.similarity_search(query, k=2)\n\nprint(f\"\\nQuery: '{query}'\")\nprint(\"\\nTop Results:\")\nfor i, doc in enumerate(results, 1):\n    print(f\"{i}. {doc.page_content}\")\n    print(f\"   Source: {doc.metadata['source']}\")\n\n# **Search with scores**\nresults_with_scores = vectorstore.similarity_search_with_score(query, k=2)\n\nprint(\"\\nResults with Scores:\")\nfor doc, score in results_with_scores:\n    print(f\"Score: {score:.3f} - {doc.page_content[:50]}...\")\n\n# **Metadata filtering**\nfiltered_results = vectorstore.similarity_search(\n    query,\n    k=2,\n    filter={\"topic\": \"search\"}\n)\n\nprint(\"\\nFiltered Results (topic=search):\")\nfor doc in filtered_results:\n    print(f\"- {doc.page_content}\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: FAISS for Large-Scale Search**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# **Create larger dataset**\nlong_text = \"\"\"\nArtificial Intelligence (AI) is revolutionizing how we interact with technology.\nMachine learning models can now understand and generate human language.\nDeep learning has enabled breakthroughs in computer vision and NLP.\nNeural networks are inspired by the human brain's architecture.\nLarge language models like GPT have billions of parameters.\nTransfer learning allows models to adapt to new tasks quickly.\nReinforcement learning helps agents learn through trial and error.\nComputer vision enables machines to understand visual information.\nNatural language processing allows computers to understand text.\nAI ethics is crucial for responsible AI development.\n\"\"\"\n\n# **Split into chunks**\nsplitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\ntexts = splitter.split_text(long_text)\n\n# **Create FAISS index**\nembeddings = OpenAIEmbeddings()\nfaiss_index = FAISS.from_texts(\n    texts,\n    embeddings,\n    metadatas=[{\"chunk_id\": i} for i in range(len(texts))]\n)\n\nprint(f\"FAISS Index created with {len(texts)} chunks\")\n\n# **Search**\nquery = \"How do neural networks work?\"\ndocs = faiss_index.similarity_search(query, k=3)\n\nprint(f\"\\nQuery: {query}\")\nprint(\"\\nTop Results:\")\nfor doc in docs:\n    print(f\"- {doc.page_content}\")\n\n# **Save and load**\nfaiss_index.save_local(\"faiss_index\")\nloaded_index = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\nprint(\"\\nâœ… Index saved and loaded successfully!\")\n\n# **Maximum marginal relevance (diverse results)**\nmmr_results = faiss_index.max_marginal_relevance_search(query, k=3, fetch_k=10)\nprint(\"\\nDiverse Results (MMR):\")\nfor doc in mmr_results:\n    print(f\"- {doc.page_content}\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 2: Build a Document Search System**\n\n### **Exercise: Create a Searchable Knowledge Base**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Build a system that:\n",
    "# 1. Loads multiple documents\n",
    "# 2. Creates a vector store\n",
    "# 3. Implements search with metadata filtering\n",
    "# 4. Returns top K relevant documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nclass KnowledgeBase:\n    def __init__(self):\n        self.embeddings = OpenAIEmbeddings()\n        self.vectorstore = None\n    \n    def add_documents(self, texts: List[str], metadatas: List[dict] = None):\n        docs = [\n            Document(page_content=text, metadata=metadata or {})\n            for text, metadata in zip(texts, metadatas or [{}]*len(texts))\n        ]\n        \n        if self.vectorstore is None:\n            self.vectorstore = Chroma.from_documents(docs, self.embeddings)\n        else:\n            self.vectorstore.add_documents(docs)\n    \n    def search(self, query: str, k: int = 3, filter: dict = None):\n        if not self.vectorstore:\n            return []\n        return self.vectorstore.similarity_search(query, k=k, filter=filter)\n\n# **Test**\nkb = KnowledgeBase()\n\n# **Add documents**\nkb.add_documents(\n    texts=[\n        \"Python is great for data science.\",\n        \"JavaScript is used for web development.\",\n        \"Machine learning requires good data.\"\n    ],\n    metadatas=[\n        {\"category\": \"programming\"},\n        {\"category\": \"programming\"},\n        {\"category\": \"ml\"}\n    ]\n)\n\n# **Search**\nresults = kb.search(\"What language for AI?\", k=2)\nfor doc in results:\n    print(f\"- {doc.page_content} [{doc.metadata}]\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 3: Advanced Vector Store Techniques**\n\n### **Example: Hybrid Search and Indexing Strategies**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_community.retrievers import BM25Retriever\n\n# **Documents about programming**\ndocs = [\n    Document(page_content=\"Python pandas library is great for data analysis.\"),\n    Document(page_content=\"JavaScript async/await simplifies asynchronous code.\"),\n    Document(page_content=\"SQL JOIN operations combine data from multiple tables.\"),\n    Document(page_content=\"Machine learning models need training data.\"),\n    Document(page_content=\"REST APIs use HTTP methods like GET and POST.\")\n]\n\n# **Create vector store retriever**\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(docs, embeddings)\nvector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# **Create keyword retriever (BM25)**\nbm25_retriever = BM25Retriever.from_documents(docs)\nbm25_retriever.k = 3\n\n# **Combine retrievers**\nensemble_retriever = EnsembleRetriever(\n    retrievers=[vector_retriever, bm25_retriever],\n    weights=[0.6, 0.4]  # Weight semantic search higher\n)\n\n# **Test hybrid search**\nquery = \"How to analyze data with Python?\"\n\nprint(\"Hybrid Search Results:\")\nresults = ensemble_retriever.get_relevant_documents(query)\nfor i, doc in enumerate(results[:3], 1):\n    print(f\"{i}. {doc.page_content}\")\n\n# **Compare with individual retrievers**\nprint(\"\\nVector Only:\")\nfor doc in vector_retriever.get_relevant_documents(query)[:2]:\n    print(f\"- {doc.page_content}\")\n\nprint(\"\\nKeyword Only:\")\nfor doc in bm25_retriever.get_relevant_documents(query)[:2]:\n    print(f\"- {doc.page_content}\")\n\nprint(\"\\nðŸ’¡ Hybrid search combines semantic and keyword matching!\")\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Summary & Next Steps**\n\n### **What You've Learned**\nâœ… Generating embeddings from text  \nâœ… Understanding vector similarity  \nâœ… Using Chroma and FAISS vector stores  \nâœ… Semantic search with metadata filtering  \nâœ… Hybrid search strategies  \n\n### **Key Takeaways**\n1. **Embeddings capture semantic meaning** - Similar concepts have similar vectors\n2. **Vector stores enable fast search** - Efficient similarity matching at scale\n3. **Metadata filtering adds precision** - Combine semantic and structured search\n4. **Choose the right embedding model** - Balance cost, speed, and quality\n5. **Hybrid search improves results** - Combine semantic and keyword matching\n\n### **What's Next?**\nIn the next notebook (`07_rag_systems.ipynb`), you'll learn:\n- Building complete RAG pipelines\n- Query enhancement techniques\n- Context window management\n- RAG evaluation metrics\n\n---\n\nðŸŽ‰ **Congratulations!** You've mastered embeddings and vector stores!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}