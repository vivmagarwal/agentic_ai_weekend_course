{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **Building Chat Applications**\n\n## **Learning Objectives**\nBy the end of this section, you will be able to:\n- Create simple static chat interfaces in Colab\n- Make API calls to Google's Gemini LLM\n- Build interactive chat applications with AI responses\n- Implement conversation memory to maintain context across messages\n- Understand the architecture of chat-based AI applications\n\n## **Why This Matters: Real-World AI/RAG/Agentic Applications**\n**In AI Systems:**\n- Chat interfaces are the primary way users interact with LLMs like ChatGPT, Claude, and Gemini\n- Understanding chat architecture is fundamental to building AI applications\n- API integration skills are essential for leveraging powerful language models\n\n**In RAG Pipelines:**\n- Chat interfaces allow users to query your knowledge base naturally\n- Conversation memory enables multi-turn interactions for clarifying questions\n- Each user message can trigger document retrieval and context injection\n\n**In Agentic AI:**\n- Agents use chat-like interfaces to communicate with users and other agents\n- Memory systems track conversation state for complex multi-step tasks\n- Chat history provides context for agent decision-making\n\n## **Prerequisites**\n- Basic Python syntax (variables, strings, functions)\n- Understanding of lists and dictionaries\n- Familiarity with loops and conditionals\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Instructor Activity 1**\n**Concept**: Creating a simple static chat interface\n\n### **Example 1: Basic Chat Display**\n\n**Problem**: Display a simple chat conversation with hardcoded messages\n\n**Expected Output**:\n```\nUser: Hello!\nBot: Hi there! How can I help you?\nUser: What's the weather?\nBot: I'm sorry, I don't have access to weather information.\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Store chat messages as a list of dictionaries**\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n    {\"role\": \"bot\", \"content\": \"Hi there! How can I help you?\"},\n    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n    {\"role\": \"bot\", \"content\": \"I'm sorry, I don't have access to weather information.\"}\n]\n\n# **Display the conversation**\nfor message in messages:\n    # Capitalize the role name for display\n    role = message[\"role\"].capitalize()\n    content = message[\"content\"]\n    print(f\"{role}: {content}\")\n```\n\n**Why this works:**\n- We use a list of dictionaries to store messages\n- Each message has a \"role\" (user or bot) and \"content\" (the text)\n- This data structure mirrors how real chat APIs work (like OpenAI, Anthropic, Google)\n- The loop displays each message in order, creating a conversation flow\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Formatted Chat Display**\n\n**Problem**: Create a more visually appealing chat display with formatting\n\n**Expected Output**:\n```\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nüë§ User: Hello!\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nü§ñ Bot: Hi there! How can I help you?\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Chat messages with better formatting**\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n    {\"role\": \"bot\", \"content\": \"Hi there! How can I help you?\"},\n    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n    {\"role\": \"bot\", \"content\": \"I'm sorry, I don't have access to weather information.\"}\n]\n\n# **Function to display chat with formatting**\ndef display_chat(messages):\n    for message in messages:\n        # Choose emoji based on role\n        emoji = \"üë§\" if message[\"role\"] == \"user\" else \"ü§ñ\"\n        role = message[\"role\"].capitalize()\n        \n        # Print with separator line and emoji\n        print(\"‚îÅ\" * 40)\n        print(f\"{emoji} {role}: {message['content']}\")\n    print(\"‚îÅ\" * 40)\n\n# **Display the conversation**\ndisplay_chat(messages)\n```\n\n**Why this works:**\n- Functions make code reusable - we can display any conversation\n- Conditional expressions choose different emojis for user vs bot\n- Visual separators make the conversation easier to read\n- This pattern scales to longer conversations\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 1**\n**Practice**: Creating static chat displays\n\n### **Exercise 1: Your Own Conversation**\n\n**Task**: Create a list of messages representing a conversation about Python programming (at least 4 messages), then display them\n\n**Expected Output**: A conversation where the user asks about Python and the bot responds"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Create a conversation about Python**\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is Python?\"},\n    {\"role\": \"bot\", \"content\": \"Python is a popular programming language known for its simplicity.\"},\n    {\"role\": \"user\", \"content\": \"What can I build with it?\"},\n    {\"role\": \"bot\", \"content\": \"You can build web apps, AI systems, data analysis tools, and much more!\"}\n]\n\n# **Display the conversation**\nfor message in messages:\n    role = message[\"role\"].capitalize()\n    print(f\"{role}: {message['content']}\")\n```\n\n**Why this works:**\nThe same pattern applies to any conversation topic. The list-of-dictionaries structure is flexible and mirrors how real AI chat systems store conversations.\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Custom Display Function**\n\n**Task**: Write a function `show_conversation(messages)` that displays messages with \">>>\" prefix for user and \"<<<\" prefix for bot\n\n**Expected Output**:\n```\n>>> USER: Hello\n<<< BOT: Hi there\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\ndef show_conversation(messages):\n    \"\"\"Display messages with directional arrows\"\"\"\n    for message in messages:\n        # Choose prefix based on role\n        prefix = \">>>\" if message[\"role\"] == \"user\" else \"<<<\"\n        role = message[\"role\"].upper()\n        print(f\"{prefix} {role}: {message['content']}\")\n\n# **Test it**\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"bot\", \"content\": \"Hi there\"}\n]\n\nshow_conversation(messages)\n```\n\n**Why this works:**\nFunctions with clear purposes make code modular. The conditional expression chooses the right prefix, and `.upper()` makes role names stand out.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 2**\n**Concept**: Making basic API calls to Google's Gemini LLM\n\n### **Example 1: Setting Up the Gemini API**\n\n**Problem**: Configure and test a connection to Google's Gemini API\n\n**Expected Output**: A response from Gemini to a simple prompt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nimport google.generativeai as genai\nfrom google.colab import userdata\n\n# **Get API key from Colab secrets**\n# **To set this up: Click the key icon (üîë) in the left sidebar**\n# **Add a secret named 'GEMINI_API_KEY' with your key from https://makersuite.google.com/app/apikey**\napi_key = userdata.get('GEMINI_API_KEY')\n\n# **Configure the API**\ngenai.configure(api_key=api_key)\n\n# **Create a model instance**\nmodel = genai.GenerativeModel('gemini-pro')\n\n# **Make a simple request**\nresponse = model.generate_content(\"Say hello in a friendly way\")\nprint(response.text)\n```\n\n**Why this works:**\n- We import the Google Generative AI library\n- `userdata.get()` securely retrieves the API key from Colab secrets (never hardcode keys!)\n- `genai.configure()` sets up authentication\n- `GenerativeModel('gemini-pro')` creates a model instance\n- `generate_content()` sends a prompt and returns the AI's response\n- `response.text` extracts the text from the response object\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Handling API Responses**\n\n**Problem**: Make an API call and examine the response structure\n\n**Expected Output**: Display both the response text and metadata"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Make a request**\nprompt = \"Explain what Python is in one sentence.\"\nresponse = model.generate_content(prompt)\n\n# **Display the response text**\nprint(\"Response Text:\")\nprint(response.text)\nprint()\n\n# **Examine response metadata**\nprint(\"Response Metadata:\")\nprint(f\"Candidates: {len(response.candidates)}\")\nprint(f\"Finish Reason: {response.candidates[0].finish_reason}\")\n```\n\n**Why this works:**\n- The response object contains more than just text\n- `response.text` is the AI-generated text\n- `response.candidates` contains all possible responses (usually just one)\n- `finish_reason` tells you why the generation stopped (e.g., \"STOP\" for natural completion)\n- Understanding the response structure helps with error handling and debugging\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Error Handling**\n\n**Problem**: Safely handle potential API errors\n\n**Expected Output**: Either the response or a user-friendly error message"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\ndef ask_gemini(prompt):\n    \"\"\"Safely send a prompt to Gemini and return the response\"\"\"\n    try:\n        response = model.generate_content(prompt)\n        return response.text\n    except Exception as e:\n        # Return error message instead of crashing\n        return f\"Error: {str(e)}\"\n\n# **Test the function**\nresult = ask_gemini(\"What is artificial intelligence?\")\nprint(result)\n```\n\n**Why this works:**\n- Try-except blocks catch errors without crashing the program\n- This handles network issues, invalid API keys, rate limits, etc.\n- Wrapping API calls in functions makes code reusable and maintainable\n- In production apps, you'd log errors and potentially retry failed requests\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 2**\n**Practice**: Making API calls to Gemini\n\n### **Exercise 1: Your First AI Question**\n\n**Task**: Ask Gemini to explain any programming concept you're curious about\n\n**Expected Output**: AI-generated explanation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Ask about any concept**\nprompt = \"What are Python decorators?\"\nresponse = model.generate_content(prompt)\nprint(response.text)\n```\n\n**Why this works:**\nThe API call is the same regardless of the prompt content. This is the foundation of all LLM applications - sending text prompts and receiving generated responses.\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Multiple Questions**\n\n**Task**: Create a list of 3 questions, send each to Gemini, and display the questions with their answers\n\n**Expected Output**: Each question followed by Gemini's answer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **List of questions**\nquestions = [\n    \"What is machine learning?\",\n    \"What is the difference between a list and a tuple?\",\n    \"What is a REST API?\"\n]\n\n# **Ask each question**\nfor question in questions:\n    print(f\"Q: {question}\")\n    response = model.generate_content(question)\n    print(f\"A: {response.text}\")\n    print(\"‚Äï\" * 50)\n```\n\n**Why this works:**\nLoops let us process multiple prompts efficiently. Note that each API call is independent - the model doesn't remember previous questions (yet - we'll add memory later!).\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 3**\n**Concept**: Building an interactive chat application\n\n### **Example 1: Simple Interactive Loop**\n\n**Problem**: Create a chat loop where users can ask questions until they type 'quit'\n\n**Expected Output**: Interactive conversation that continues until user exits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nprint(\"Chat with Gemini! (type 'quit' to exit)\")\nprint(\"=\"* 50)\n\nwhile True:\n    # Get user input\n    user_message = input(\"You: \")\n    \n    # Check for exit command\n    if user_message.lower() == 'quit':\n        print(\"Goodbye!\")\n        break\n    \n    # Get AI response\n    try:\n        response = model.generate_content(user_message)\n        print(f\"Gemini: {response.text}\")\n        print(\"‚Äï\" * 50)\n    except Exception as e:\n        print(f\"Error: {e}\")\n```\n\n**Why this works:**\n- `while True` creates an infinite loop\n- `input()` waits for the user to type something\n- We check if the user wants to quit before making the API call\n- `break` exits the loop\n- Try-except handles any API errors gracefully\n- This is a simple but functional chatbot!\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Chat with Display Function**\n\n**Problem**: Enhance the chat loop with better formatting and message storage\n\n**Expected Output**: Formatted conversation with history stored"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\ndef display_message(role, content):\n    \"\"\"Display a chat message with formatting\"\"\"\n    emoji = \"üë§\" if role == \"user\" else \"ü§ñ\"\n    print(f\"{emoji} {role.upper()}: {content}\")\n    print()\n\n# **Store conversation history**\nconversation = []\n\nprint(\"Enhanced Chat with Gemini! (type 'quit' to exit)\")\nprint(\"=\"* 50)\n\nwhile True:\n    # Get user input\n    user_message = input(\"You: \")\n    \n    if user_message.lower() == 'quit':\n        print(f\"\\nTotal messages exchanged: {len(conversation)}\")\n        break\n    \n    # Store and display user message\n    conversation.append({\"role\": \"user\", \"content\": user_message})\n    \n    # Get and display AI response\n    try:\n        response = model.generate_content(user_message)\n        bot_message = response.text\n        \n        # Store and display bot message\n        conversation.append({\"role\": \"bot\", \"content\": bot_message})\n        display_message(\"gemini\", bot_message)\n        print(\"‚Äï\" * 50)\n    except Exception as e:\n        print(f\"Error: {e}\\n\")\n```\n\n**Why this works:**\n- We now store every message in a list (conversation history)\n- The display function keeps formatting consistent\n- We can track conversation length and analyze it later\n- This structure mirrors professional chat applications\n- However, note that Gemini still doesn't remember previous messages!\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 3**\n**Practice**: Building interactive chat applications\n\n### **Exercise 1: Custom Welcome Message**\n\n**Task**: Create a chat loop that:\n1. Displays a custom welcome message\n2. Takes user input\n3. Gets Gemini's response\n4. Exits when user types 'exit'\n\n**Expected Output**: Working chat interface with your custom welcome"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nprint(\"Welcome to Python Learning Assistant!\")\nprint(\"Ask me anything about Python. Type 'exit' to quit.\")\nprint(\"=\"* 50)\n\nwhile True:\n    user_input = input(\"Your question: \")\n    \n    if user_input.lower() == 'exit':\n        print(\"Happy coding!\")\n        break\n    \n    try:\n        response = model.generate_content(user_input)\n        print(f\"\\nAssistant: {response.text}\\n\")\n        print(\"‚Äï\" * 50)\n    except Exception as e:\n        print(f\"Error: {e}\\n\")\n```\n\n**Why this works:**\nCustomizing the welcome message and exit command makes the chat feel more tailored to your specific use case. This same pattern works for any specialized chatbot.\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Message Counter**\n\n**Task**: Add a feature that counts and displays the total number of messages after every exchange\n\n**Expected Output**: \n```\nYou: Hello\nBot: Hi there!\n[2 messages so far]\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "message_count = 0\n",
    "\n",
    "print(\"Chat with message counter (type 'quit' to exit)\")\n",
    "print(\"=\"* 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        print(f\"\\nFinal count: {message_count} messages\")\n",
    "        break\n",
    "    \n",
    "    # Increment for user message\n",
    "    message_count += 1\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(user_input)\n",
    "        print(f\"Bot: {response.text}\")\n",
    "        \n",
    "        # Increment for bot message\n",
    "        message_count += 1\n",
    "        \n",
    "        print(f\"[{message_count} messages so far]\")\n",
    "        print(\"‚Äï\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "We maintain a counter variable that increments after each message (user and bot). This helps track engagement and can be useful for analytics in production apps.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 4**\n**Concept**: Adding conversation memory using chat sessions\n\n### **Example 1: Understanding Memory Problem**\n\n**Problem**: Demonstrate why the current chat doesn't remember context\n\n**Expected Output**: Shows that follow-up questions don't work without memory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **First message**\nresponse1 = model.generate_content(\"My name is Alice\")\nprint(\"Message 1:\")\nprint(response1.text)\nprint()\n\n# **Follow-up question (should remember the name, but won't!)**\nresponse2 = model.generate_content(\"What's my name?\")\nprint(\"Message 2:\")\nprint(response2.text)\n```\n\n**Why this works (or doesn't!):**\n- Each `generate_content()` call is independent\n- The model has no memory of previous messages\n- It will say it doesn't know your name because each call is a fresh conversation\n- This is where chat sessions come in - they maintain context across messages\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Using Chat Sessions**\n\n**Problem**: Create a chat session that maintains context across messages\n\n**Expected Output**: AI remembers previous messages in the conversation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Start a chat session**\nchat = model.start_chat(history=[])\n\n# **First message**\nresponse1 = chat.send_message(\"My name is Alice and I love Python\")\nprint(\"User: My name is Alice and I love Python\")\nprint(f\"Gemini: {response1.text}\\n\")\n\n# **Follow-up question (now it will remember!)**\nresponse2 = chat.send_message(\"What's my name?\")\nprint(\"User: What's my name?\")\nprint(f\"Gemini: {response2.text}\\n\")\n\n# **Another follow-up**\nresponse3 = chat.send_message(\"What programming language do I like?\")\nprint(\"User: What programming language do I like?\")\nprint(f\"Gemini: {response3.text}\")\n```\n\n**Why this works:**\n- `start_chat()` creates a stateful chat session\n- `send_message()` adds messages to the conversation history\n- The model receives all previous messages as context\n- This enables natural follow-up questions and context-aware responses\n- The `history=[]` parameter starts with an empty conversation\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Inspecting Chat History**\n\n**Problem**: View the conversation history stored in the chat session\n\n**Expected Output**: Display all messages exchanged in the session"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Create a chat and exchange some messages**\nchat = model.start_chat(history=[])\nchat.send_message(\"Hello! I'm learning about AI.\")\nchat.send_message(\"What is machine learning?\")\n\n# **Display the conversation history**\nprint(\"Conversation History:\")\nprint(\"=\"* 50)\n\nfor message in chat.history:\n    # Message has 'role' and 'parts' attributes\n    role = message.role\n    content = message.parts[0].text\n    \n    emoji = \"üë§\" if role == \"user\" else \"ü§ñ\"\n    print(f\"{emoji} {role.upper()}: {content}\")\n    print(\"‚Äï\" * 50)\n```\n\n**Why this works:**\n- `chat.history` contains all messages in the session\n- Each message has a `role` (\"user\" or \"model\") and `parts` (list of content)\n- `parts[0].text` extracts the text content\n- This is useful for debugging, analytics, or saving conversations\n- In production, you'd store this history in a database\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 4**\n**Practice**: Building chat applications with memory\n\n### **Exercise 1: Multi-Turn Conversation**\n\n**Task**: Create a chat session and have a conversation with at least 3 connected messages (each building on the previous)\n\n**Expected Output**: A conversation where context carries through"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Start a chat session**\nchat = model.start_chat(history=[])\n\n# **First message - introduce a topic**\nr1 = chat.send_message(\"I'm building a weather app\")\nprint(\"You: I'm building a weather app\")\nprint(f\"Gemini: {r1.text}\\n\")\n\n# **Second message - ask related question**\nr2 = chat.send_message(\"What APIs should I use for it?\")\nprint(\"You: What APIs should I use for it?\")\nprint(f\"Gemini: {r2.text}\\n\")\n\n# **Third message - follow-up**\nr3 = chat.send_message(\"Are any of them free?\")\nprint(\"You: Are any of them free?\")\nprint(f\"Gemini: {r3.text}\")\n```\n\n**Why this works:**\nThe chat session maintains context, so \"it\" refers to the weather app, and \"them\" refers to the APIs. This natural conversation flow is only possible with memory.\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: History Viewer**\n\n**Task**: Create a function `show_history(chat)` that displays all messages in a chat session with nice formatting\n\n**Expected Output**: Formatted display of all messages in the conversation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\ndef show_history(chat):\n    \"\"\"Display all messages in a chat session\"\"\"\n    print(\"\\nüìú Conversation History\")\n    print(\"=\"* 50)\n    \n    for i, message in enumerate(chat.history, 1):\n        role = message.role\n        content = message.parts[0].text\n        emoji = \"üë§\" if role == \"user\" else \"ü§ñ\"\n        \n        print(f\"\\nMessage {i}:\")\n        print(f\"{emoji} {role.upper()}: {content}\")\n    \n    print(\"\\n\" + \"=\"* 50)\n    print(f\"Total messages: {len(chat.history)}\")\n\n# **Test it**\nchat = model.start_chat(history=[])\nchat.send_message(\"Hello!\")\nchat.send_message(\"How are you?\")\nshow_history(chat)\n```\n\n**Why this works:**\nFunctions make code reusable. We can now view any chat's history with a single function call. The `enumerate()` function gives us message numbers.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 5**\n**Concept**: Building a complete interactive chat app with memory\n\n### **Example 1: Full-Featured Chat Application**\n\n**Problem**: Combine all concepts into a production-ready chat application\n\n**Expected Output**: Interactive chat with memory, error handling, and commands"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\ndef run_chat_app():\n    \"\"\"Run an interactive chat application with memory\"\"\"\n    \n    # Start a chat session\n    chat = model.start_chat(history=[])\n    \n    # Welcome message\n    print(\"\\n\" + \"=\"*60)\n    print(\"ü§ñ Gemini Chat Application\")\n    print(\"=\"*60)\n    print(\"Commands:\")\n    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n    print(\"  - Type 'history' to view conversation history\")\n    print(\"  - Type 'clear' to start a new conversation\")\n    print(\"=\"*60 + \"\\n\")\n    \n    while True:\n        try:\n            # Get user input\n            user_input = input(\"You: \").strip()\n            \n            if not user_input:\n                continue\n            \n            # Handle commands\n            if user_input.lower() in ['quit', 'exit']:\n                print(\"\\nüëã Thank you for chatting! Goodbye!\")\n                break\n            \n            elif user_input.lower() == 'history':\n                print(\"\\nüìú Conversation History:\")\n                for msg in chat.history:\n                    role = msg.role.upper()\n                    text = msg.parts[0].text[:100]  # Truncate long messages\n                    print(f\"  {role}: {text}...\")\n                print()\n                continue\n            \n            elif user_input.lower() == 'clear':\n                chat = model.start_chat(history=[])\n                print(\"\\nüîÑ Conversation cleared. Starting fresh!\\n\")\n                continue\n            \n            # Send message and get response\n            response = chat.send_message(user_input)\n            print(f\"\\nü§ñ Gemini: {response.text}\\n\")\n            print(\"‚Äï\" * 60 + \"\\n\")\n            \n        except KeyboardInterrupt:\n            print(\"\\n\\nüëã Chat interrupted. Goodbye!\")\n            break\n        except Exception as e:\n            print(f\"\\n‚ùå Error: {e}\\n\")\n            print(\"Please try again.\\n\")\n\n# **Run the app**\nrun_chat_app()\n```\n\n**Why this works:**\n- Wrapping everything in a function makes it clean and reusable\n- We handle multiple commands (quit, history, clear)\n- Error handling catches both API errors and keyboard interrupts (Ctrl+C)\n- The chat session maintains context throughout the conversation\n- Empty input is ignored with `continue`\n- `.strip()` removes leading/trailing whitespace\n- This is a complete, production-ready chat application!\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Chat with System Prompt**\n\n**Problem**: Initialize a chat with a system prompt that sets the AI's behavior\n\n**Expected Output**: AI responds according to the system instructions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Create a model with system instruction**\nmodel_with_instructions = genai.GenerativeModel(\n    'gemini-pro',\n    system_instruction=\"You are a helpful Python tutor. You explain concepts clearly and give practical examples. Keep responses concise.\"\n)\n\n# **Start chat with this customized model**\nchat = model_with_instructions.start_chat(history=[])\n\n# **Test it**\nresponse = chat.send_message(\"What are list comprehensions?\")\nprint(f\"Tutor: {response.text}\")\n```\n\n**Why this works:**\n- System instructions guide the AI's personality and behavior\n- This is crucial for specialized chatbots (customer service, tutoring, etc.)\n- The instruction applies to all messages in conversations with this model\n- In RAG systems, system prompts often include instructions about using retrieved context\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 5**\n**Practice**: Building complete chat applications\n\n### **Exercise 1: Specialized Chatbot**\n\n**Task**: Create a chat application with a system prompt for a specific purpose (e.g., coding helper, travel advisor, recipe assistant). Include at least the 'quit' and 'history' commands.\n\n**Expected Output**: Working specialized chatbot with memory and commands"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\n# **Create a specialized model**\nrecipe_bot = genai.GenerativeModel(\n    'gemini-pro',\n    system_instruction=\"You are a helpful recipe assistant. Provide clear cooking instructions and suggest ingredient substitutions when asked.\"\n)\n\ndef run_recipe_chat():\n    chat = recipe_bot.start_chat(history=[])\n    \n    print(\"\\nüç≥ Recipe Assistant\")\n    print(\"Ask me about recipes, ingredients, or cooking tips!\")\n    print(\"Commands: 'quit' to exit, 'history' to view conversation\\n\")\n    \n    while True:\n        user_input = input(\"You: \").strip()\n        \n        if not user_input:\n            continue\n        \n        if user_input.lower() == 'quit':\n            print(\"Happy cooking!\")\n            break\n        \n        if user_input.lower() == 'history':\n            for msg in chat.history:\n                print(f\"{msg.role.upper()}: {msg.parts[0].text[:80]}...\")\n            print()\n            continue\n        \n        try:\n            response = chat.send_message(user_input)\n            print(f\"\\nü§ñ Chef: {response.text}\\n\")\n        except Exception as e:\n            print(f\"Error: {e}\\n\")\n\nrun_recipe_chat()\n```\n\n**Why this works:**\nThe system instruction customizes the AI's expertise. You can create specialized chatbots for any domain by changing the system prompt.\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Chat with Save Feature**\n\n**Task**: Add a 'save' command that writes the conversation history to a text file\n\n**Expected Output**: When user types 'save', conversation is written to a file"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "def save_conversation(chat):\n",
    "    \"\"\"Save chat history to a text file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"chat_history_{timestamp}.txt\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Conversation History\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        for msg in chat.history:\n",
    "            role = msg.role.upper()\n",
    "            text = msg.parts[0].text\n",
    "            f.write(f\"{role}: {text}\\n\\n\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def run_chat_with_save():\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"Chat with Save Feature\")\n",
    "    print(\"Commands: 'quit', 'history', 'save'\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'save':\n",
    "            filename = save_conversation(chat)\n",
    "            print(f\"‚úÖ Conversation saved to {filename}\\n\")\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'history':\n",
    "            for msg in chat.history:\n",
    "                print(f\"{msg.role.upper()}: {msg.parts[0].text[:80]}...\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"\\nBot: {response.text}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_chat_with_save()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- We use `datetime` to create unique filenames\n",
    "- File writing with `with open()` ensures proper file handling\n",
    "- This lets users keep records of important conversations\n",
    "- In production, you'd save to a database instead of text files\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Optional Extra Practice**\n**Challenge yourself with these problems that integrate all the concepts**\n\n### **Challenge 1: Multi-User Chat System**\n\n**Task**: Create a chat system that can handle multiple users. Use a dictionary to store separate chat sessions for different users. Include commands to switch between users.\n\n**Example Usage**:\n```\nCommands: /user <name> - switch user, /list - list users\nCurrent user: Alice\nYou: Hello\nBot: Hi Alice!\n/user Bob\nSwitched to Bob\nYou: Hello\nBot: Hi Bob!\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def run_multi_user_chat():\n",
    "    # Store chat sessions for each user\n",
    "    users = {}\n",
    "    current_user = \"Guest\"\n",
    "    \n",
    "    # Create default user\n",
    "    users[current_user] = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"Multi-User Chat System\")\n",
    "    print(\"Commands: /user <name> - switch user, /list - list users, /quit - exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(f\"[{current_user}] You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Handle commands\n",
    "        if user_input.lower() == '/quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        elif user_input.startswith('/user '):\n",
    "            new_user = user_input.split(' ', 1)[1]\n",
    "            \n",
    "            # Create chat session if new user\n",
    "            if new_user not in users:\n",
    "                users[new_user] = model.start_chat(history=[])\n",
    "                print(f\"Created new user: {new_user}\")\n",
    "            \n",
    "            current_user = new_user\n",
    "            print(f\"Switched to {current_user}\\n\")\n",
    "            continue\n",
    "        \n",
    "        elif user_input.lower() == '/list':\n",
    "            print(\"Users:\")\n",
    "            for user in users.keys():\n",
    "                marker = \" (current)\" if user == current_user else \"\"\n",
    "                print(f\"  - {user}{marker}\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        # Send message for current user\n",
    "        try:\n",
    "            chat = users[current_user]\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"Bot: {response.text}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_multi_user_chat()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Dictionary stores separate chat sessions per user\n",
    "- Each user has independent conversation history\n",
    "- Command parsing with `split()` extracts username\n",
    "- This architecture scales to many users\n",
    "- Similar to how real chat platforms manage multiple conversations\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Challenge 2: RAG-Enabled Chat**\n\n**Task**: Create a chat application where user messages are first used to search a \"knowledge base\" (a list of facts), and relevant facts are added to the prompt before sending to Gemini.\n\n**Example**:\n```\nKnowledge base: [\n  \"Python was created by Guido van Rossum\",\n  \"Python was released in 1991\",\n  \"Django is a Python web framework\"\n]\n\nUser: Who created Python?\nSystem: [Finds relevant fact: \"Python was created by Guido van Rossum\"]\nBot: Python was created by Guido van Rossum.\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def search_knowledge_base(query, knowledge_base):\n",
    "    \"\"\"Simple keyword-based search of knowledge base\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    relevant_facts = []\n",
    "    \n",
    "    for fact in knowledge_base:\n",
    "        # Check if any query word appears in the fact\n",
    "        if any(word in fact.lower() for word in query_lower.split()):\n",
    "            relevant_facts.append(fact)\n",
    "    \n",
    "    return relevant_facts\n",
    "\n",
    "def run_rag_chat():\n",
    "    # Knowledge base\n",
    "    knowledge_base = [\n",
    "        \"Python was created by Guido van Rossum in 1991.\",\n",
    "        \"Python is known for its simple and readable syntax.\",\n",
    "        \"Django is a popular Python web framework used by Instagram and Pinterest.\",\n",
    "        \"Flask is a lightweight Python web framework good for small projects.\",\n",
    "        \"NumPy and Pandas are essential Python libraries for data science.\",\n",
    "    ]\n",
    "    \n",
    "    # Create model with RAG instructions\n",
    "    rag_model = genai.GenerativeModel(\n",
    "        'gemini-pro',\n",
    "        system_instruction=\"You are a helpful assistant. Use the provided context to answer questions accurately. If the context doesn't contain relevant information, say so.\"\n",
    "    )\n",
    "    \n",
    "    chat = rag_model.start_chat(history=[])\n",
    "    \n",
    "    print(\"RAG-Enhanced Chat\")\n",
    "    print(\"Ask questions about Python!\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Search knowledge base\n",
    "            relevant_facts = search_knowledge_base(user_input, knowledge_base)\n",
    "            \n",
    "            # Build prompt with context\n",
    "            if relevant_facts:\n",
    "                context = \"\\n\".join(f\"- {fact}\" for fact in relevant_facts)\n",
    "                prompt = f\"Context:\\n{context}\\n\\nQuestion: {user_input}\"\n",
    "                print(f\"[Found {len(relevant_facts)} relevant fact(s)]\")\n",
    "            else:\n",
    "                prompt = user_input\n",
    "                print(\"[No relevant facts found in knowledge base]\")\n",
    "            \n",
    "            # Send to AI\n",
    "            response = chat.send_message(prompt)\n",
    "            print(f\"\\nBot: {response.text}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_rag_chat()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- This is a simplified RAG (Retrieval-Augmented Generation) system\n",
    "- We search the knowledge base for relevant facts using keyword matching\n",
    "- Relevant facts are injected into the prompt as context\n",
    "- The AI uses this context to provide accurate, grounded answers\n",
    "- Real RAG systems use embeddings and vector search, but the principle is the same\n",
    "- This pattern powers applications like \"chat with your documents\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Challenge 3: Chat Analytics Dashboard**\n\n**Task**: Create a chat application that tracks analytics:\n- Total messages exchanged\n- Average message length (user and bot separately)\n- Most common words used by user\n- Add a '/stats' command to display these analytics\n\n**Expected Output**: Working chat with analytics dashboard"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_stats(chat):\n",
    "    \"\"\"Calculate conversation statistics\"\"\"\n",
    "    user_messages = []\n",
    "    bot_messages = []\n",
    "    all_user_words = []\n",
    "    \n",
    "    for msg in chat.history:\n",
    "        text = msg.parts[0].text\n",
    "        \n",
    "        if msg.role == \"user\":\n",
    "            user_messages.append(text)\n",
    "            # Collect words (lowercase, no punctuation)\n",
    "            words = text.lower().split()\n",
    "            all_user_words.extend(words)\n",
    "        else:\n",
    "            bot_messages.append(text)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_user_length = sum(len(m) for m in user_messages) / len(user_messages) if user_messages else 0\n",
    "    avg_bot_length = sum(len(m) for m in bot_messages) / len(bot_messages) if bot_messages else 0\n",
    "    \n",
    "    # Find most common words (exclude very short words)\n",
    "    meaningful_words = [w for w in all_user_words if len(w) > 3]\n",
    "    common_words = Counter(meaningful_words).most_common(5)\n",
    "    \n",
    "    return {\n",
    "        'total_messages': len(chat.history),\n",
    "        'user_messages': len(user_messages),\n",
    "        'bot_messages': len(bot_messages),\n",
    "        'avg_user_length': avg_user_length,\n",
    "        'avg_bot_length': avg_bot_length,\n",
    "        'common_words': common_words\n",
    "    }\n",
    "\n",
    "def display_stats(stats):\n",
    "    \"\"\"Display formatted statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä CONVERSATION ANALYTICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total Messages: {stats['total_messages']}\")\n",
    "    print(f\"  - User: {stats['user_messages']}\")\n",
    "    print(f\"  - Bot: {stats['bot_messages']}\")\n",
    "    print(f\"\\nAverage Message Length:\")\n",
    "    print(f\"  - User: {stats['avg_user_length']:.1f} characters\")\n",
    "    print(f\"  - Bot: {stats['avg_bot_length']:.1f} characters\")\n",
    "    print(f\"\\nMost Common Words (User):\")\n",
    "    for word, count in stats['common_words']:\n",
    "        print(f\"  - {word}: {count} times\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "def run_chat_with_analytics():\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"Chat with Analytics\")\n",
    "    print(\"Commands: /stats - view analytics, /quit - exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == '/quit':\n",
    "            # Show final stats before exiting\n",
    "            if chat.history:\n",
    "                stats = calculate_stats(chat)\n",
    "                display_stats(stats)\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == '/stats':\n",
    "            if chat.history:\n",
    "                stats = calculate_stats(chat)\n",
    "                display_stats(stats)\n",
    "            else:\n",
    "                print(\"No conversation history yet!\\n\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"\\nBot: {response.text}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_chat_with_analytics()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- We analyze the chat history to extract insights\n",
    "- `Counter` from collections makes word frequency analysis easy\n",
    "- Statistics help understand conversation patterns and user engagement\n",
    "- This is valuable for improving chatbot design and user experience\n",
    "- Real applications track metrics like response time, user satisfaction, conversation length\n",
    "- Analytics inform decisions about bot personality, response length, and feature additions\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}