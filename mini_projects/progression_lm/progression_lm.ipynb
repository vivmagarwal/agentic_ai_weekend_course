{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Progressive LM - PDF to Web Search RAG System**\n",
    "\n",
    "This notebook implements a progressive language model system that:\n",
    "1. Searches a PDF document first for answers\n",
    "2. Falls back to web search if information is not found in the PDF\n",
    "3. Uses free-tier APIs (Google Gemini, Groq, Tavily)\n",
    "\n",
    "**Cost**: $0 - Perfect for students!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Setup: Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-google-genai langchain-groq langchain-core langchain-tavily faiss-cpu pymupdf -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Configure API Keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Free tier API keys - no cost for students\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter GOOGLE GEMINI API KEY: \")\n",
    "os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter GROQ API KEY: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter TAVILY API KEY: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Upload PDF File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# upload the pdf file\n",
    "uploaded = files.upload()\n",
    "file_name = list(uploaded.keys())[0]\n",
    "\n",
    "# get the file path and store in temporary location\n",
    "file_path = os.path.join(tempfile.gettempdir(), file_name)\n",
    "with open(file_path, \"wb\") as f:\n",
    "        f.write(uploaded[file_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Initialize Models and Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Free tier models - Gemini for embeddings, Groq for LLM\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "tool = TavilySearch(max_results=3, topic=\"general\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Define Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Create Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "answer_determination_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant tasked with determining if the provided context from a PDF contains sufficient information to answer a user's question.\n",
    "\n",
    "Context from PDF: {context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "First, carefully analyze if the context provides adequate information to answer the question.\n",
    "\n",
    "If the context contains sufficient information to answer the question, respond with a complete and accurate answer based ONLY on the provided context.\n",
    "\n",
    "If the context does NOT contain sufficient information to fully answer the question, respond with exactly: \"[NEED_WEB_SEARCH]\"\n",
    "\n",
    "Your response:\n",
    "\"\"\")\n",
    "\n",
    "web_search_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant helping a user with their question.\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Web Search Results: {web_results}\n",
    "\n",
    "Using the web search results, provide a comprehensive and accurate answer to the user's question.\n",
    "Make sure to cite sources from the search results where appropriate.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Process PDF and Create Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- PDF Processing Function ---\n",
    "def process_pdf(file_path):\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = process_pdf(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Create Retriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Build Processing Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "determination_chain = (\n",
    "  {\n",
    "    \"context\": retriever | format_docs,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "  }\n",
    "  | answer_determination_prompt\n",
    "  | llm\n",
    "  | StrOutputParser()\n",
    ")\n",
    "\n",
    "web_search_chain = (\n",
    "  {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"web_results\": lambda x: tool.invoke({\"query\": x})\n",
    "  }\n",
    "  | web_search_prompt\n",
    "  | llm\n",
    "  | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Define Agent Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(question):\n",
    "  pdf_response = determination_chain.invoke(question)\n",
    "  if \"[NEED_WEB_SEARCH]\" in pdf_response:\n",
    "    print(\"\\nℹ️ Info not found in PDF. Searching the web...\")\n",
    "    return web_search_chain.invoke(question)\n",
    "  else:\n",
    "    return pdf_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Interactive Chat Loop**\n",
    "\n",
    "Ask questions about your PDF! The system will:\n",
    "1. First search the PDF for answers\n",
    "2. If not found, search the web\n",
    "\n",
    "Type 'exit' to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions interactively\n",
    "while True:\n",
    "  query = input(\"\\nAsk a question about your PDF (or type 'exit'): \")\n",
    "  if query.lower() == 'exit':\n",
    "    break\n",
    "  answer = agent(query)\n",
    "  print(\"\\n✉️ Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
