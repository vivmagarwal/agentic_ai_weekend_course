{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a87c73",
   "metadata": {},
   "source": [
    "# 10 - Streaming and Real-time: Building Responsive AI Applications\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll learn how to build responsive AI applications with streaming capabilities. You'll implement real-time token streaming, async operations, and interactive applications that provide immediate feedback to users.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Implement token-by-token streaming from LLMs\n",
    "- Build async chains and agents\n",
    "- Create real-time chat applications\n",
    "- Handle streaming with callbacks\n",
    "- Implement progress indicators for long operations\n",
    "- Build WebSocket-based real-time systems\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of notebooks 01-09\n",
    "- Understanding of async programming basics\n",
    "- Knowledge of chains and agents\n",
    "\n",
    "## Back-and-Forth Teaching Pattern\n",
    "This notebook follows our pattern:\n",
    "1. **Instructor Activity**: Demonstrates a concept with complete examples\n",
    "2. **Learner Activity**: You apply the concept with guidance and hidden solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's install and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai asyncio nest-asyncio websockets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Any, Dict, List, Optional, AsyncIterator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler, AsyncCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain.schema.output import LLMResult\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Allow nested event loops (for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 1: Basic Streaming and Callbacks\n",
    "\n",
    "Let's start with basic streaming implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic token streaming\n",
    "\n",
    "# Create LLM with streaming enabled\n",
    "streaming_llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    streaming=True,  # Enable streaming\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]  # Print tokens as they arrive\n",
    ")\n",
    "\n",
    "# Test streaming\n",
    "print(\"Streaming Response:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = streaming_llm.predict(\"Tell me a short story about a robot learning to paint.\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 50)\n",
    "print(\"\\nFinal response stored in variable:\")\n",
    "print(f\"Length: {len(response)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_callback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom streaming callback with more control\n",
    "\n",
    "class CustomStreamingCallback(BaseCallbackHandler):\n",
    "    \"\"\"Custom callback to handle streaming tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.token_count = 0\n",
    "        self.start_time = None\n",
    "    \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):\n",
    "        \"\"\"Called when LLM starts.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.tokens = []\n",
    "        self.token_count = 0\n",
    "        print(\"\\nüöÄ Starting generation...\\n\")\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        \"\"\"Called when a new token is generated.\"\"\"\n",
    "        self.tokens.append(token)\n",
    "        self.token_count += 1\n",
    "        \n",
    "        # Print token with formatting\n",
    "        print(token, end=\"\", flush=True)\n",
    "        \n",
    "        # Show progress every 10 tokens\n",
    "        if self.token_count % 10 == 0:\n",
    "            elapsed = time.time() - self.start_time\n",
    "            tokens_per_sec = self.token_count / elapsed if elapsed > 0 else 0\n",
    "            # Print stats on same line\n",
    "            print(f\" [{self.token_count} tokens, {tokens_per_sec:.1f} tok/s]\", end=\"\", flush=True)\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        \"\"\"Called when LLM finishes.\"\"\"\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"\\n\\n‚úÖ Generation complete!\")\n",
    "        print(f\"   Total tokens: {self.token_count}\")\n",
    "        print(f\"   Time: {elapsed:.2f}s\")\n",
    "        print(f\"   Speed: {self.token_count/elapsed:.1f} tokens/second\")\n",
    "    \n",
    "    def on_llm_error(self, error: Exception, **kwargs):\n",
    "        \"\"\"Called on error.\"\"\"\n",
    "        print(f\"\\n‚ùå Error: {error}\")\n",
    "\n",
    "# Use custom callback\n",
    "custom_callback = CustomStreamingCallback()\n",
    "\n",
    "custom_llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    streaming=True,\n",
    "    callbacks=[custom_callback]\n",
    ")\n",
    "\n",
    "# Generate with custom streaming\n",
    "response = custom_llm.predict(\"Explain quantum computing in simple terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming_chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with chains\n",
    "\n",
    "class ChainStreamingCallback(BaseCallbackHandler):\n",
    "    \"\"\"Callback for streaming chain outputs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_step = \"\"\n",
    "        self.steps_completed = 0\n",
    "    \n",
    "    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs):\n",
    "        \"\"\"Called when chain starts.\"\"\"\n",
    "        print(\"\\nüîó Chain started...\")\n",
    "        print(f\"   Input: {list(inputs.keys())}\")\n",
    "    \n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs):\n",
    "        \"\"\"Called when chain ends.\"\"\"\n",
    "        print(\"\\n‚úÖ Chain completed!\")\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        \"\"\"Stream tokens from LLM in chain.\"\"\"\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs):\n",
    "        \"\"\"Called when tool starts.\"\"\"\n",
    "        tool_name = serialized.get(\"name\", \"Unknown\")\n",
    "        print(f\"\\nüîß Using tool: {tool_name}\")\n",
    "    \n",
    "    def on_tool_end(self, output: str, **kwargs):\n",
    "        \"\"\"Called when tool ends.\"\"\"\n",
    "        print(f\"\\n   Tool result: {output[:100]}...\")\n",
    "\n",
    "# Create chain with streaming\n",
    "chain_callback = ChainStreamingCallback()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a detailed explanation about {topic}. Include examples and applications.\"\n",
    ")\n",
    "\n",
    "streaming_chain = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0.7, streaming=True),\n",
    "    prompt=prompt,\n",
    "    callbacks=[chain_callback]\n",
    ")\n",
    "\n",
    "# Run chain with streaming\n",
    "result = streaming_chain.run(topic=\"neural networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "token_usage_callback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage and cost tracking callback\n",
    "\n",
    "class TokenUsageCallback(BaseCallbackHandler):\n",
    "    \"\"\"Track token usage and estimate costs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.prompt_tokens = 0\n",
    "        self.completion_tokens = 0\n",
    "        self.total_cost = 0.0\n",
    "        \n",
    "        # Pricing per 1K tokens (example rates)\n",
    "        self.pricing = {\n",
    "            \"gpt-3.5-turbo\": {\"prompt\": 0.0015, \"completion\": 0.002},\n",
    "            \"gpt-4\": {\"prompt\": 0.03, \"completion\": 0.06}\n",
    "        }\n",
    "    \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):\n",
    "        \"\"\"Estimate prompt tokens.\"\"\"\n",
    "        # Rough estimation: 1 token ‚âà 4 characters\n",
    "        for prompt in prompts:\n",
    "            self.prompt_tokens += len(prompt) // 4\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        \"\"\"Count completion tokens.\"\"\"\n",
    "        self.completion_tokens += 1\n",
    "        self.total_tokens += 1\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        \"\"\"Calculate final costs.\"\"\"\n",
    "        if response.llm_output and \"token_usage\" in response.llm_output:\n",
    "            usage = response.llm_output[\"token_usage\"]\n",
    "            self.prompt_tokens = usage.get(\"prompt_tokens\", self.prompt_tokens)\n",
    "            self.completion_tokens = usage.get(\"completion_tokens\", self.completion_tokens)\n",
    "            self.total_tokens = usage.get(\"total_tokens\", self.total_tokens)\n",
    "        \n",
    "        # Calculate cost (assuming gpt-3.5-turbo)\n",
    "        model_pricing = self.pricing[\"gpt-3.5-turbo\"]\n",
    "        prompt_cost = (self.prompt_tokens / 1000) * model_pricing[\"prompt\"]\n",
    "        completion_cost = (self.completion_tokens / 1000) * model_pricing[\"completion\"]\n",
    "        self.total_cost = prompt_cost + completion_cost\n",
    "    \n",
    "    def get_usage_report(self) -> str:\n",
    "        \"\"\"Get formatted usage report.\"\"\"\n",
    "        return f\"\"\"Token Usage Report:\n",
    "        - Prompt tokens: {self.prompt_tokens}\n",
    "        - Completion tokens: {self.completion_tokens}\n",
    "        - Total tokens: {self.total_tokens}\n",
    "        - Estimated cost: ${self.total_cost:.4f}\"\"\"\n",
    "\n",
    "# Test token tracking\n",
    "usage_callback = TokenUsageCallback()\n",
    "\n",
    "tracking_llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    streaming=True,\n",
    "    callbacks=[usage_callback, StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "response = tracking_llm.predict(\"Write a haiku about programming.\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 50)\n",
    "print(usage_callback.get_usage_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 1: Build a Real-time Progress Tracker\n",
    "\n",
    "Create a system that provides real-time feedback for long-running operations.\n",
    "\n",
    "**Task**: Build a progress tracking system that:\n",
    "1. Shows progress for document processing\n",
    "2. Provides ETA estimates\n",
    "3. Displays partial results as they become available\n",
    "4. Handles multiple concurrent operations\n",
    "5. Includes a progress bar visualization\n",
    "\n",
    "Requirements:\n",
    "- Implement custom callbacks for progress tracking\n",
    "- Show live updates during processing\n",
    "- Calculate and display ETAs\n",
    "- Handle errors gracefully with progress state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_1_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your real-time progress tracker\n",
    "\n",
    "class ProgressTracker(BaseCallbackHandler):\n",
    "    def __init__(self, total_steps: int):\n",
    "        # TODO: Initialize progress tracking\n",
    "        # - Track current step\n",
    "        # - Calculate ETA\n",
    "        # - Store partial results\n",
    "        pass\n",
    "    \n",
    "    def update_progress(self, step_name: str, progress: float):\n",
    "        \"\"\"Update progress for current operation.\"\"\"\n",
    "        # TODO: Update progress and calculate ETA\n",
    "        pass\n",
    "    \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):\n",
    "        # TODO: Track LLM operation start\n",
    "        pass\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        # TODO: Update progress as tokens arrive\n",
    "        pass\n",
    "    \n",
    "    def get_status(self) -> Dict:\n",
    "        \"\"\"Get current status and ETA.\"\"\"\n",
    "        # TODO: Return current progress status\n",
    "        pass\n",
    "\n",
    "# TODO: Create document processor with progress tracking\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize with progress tracker\n",
    "        pass\n",
    "    \n",
    "    def process_documents(self, documents: List[str]):\n",
    "        \"\"\"Process documents with real-time progress.\"\"\"\n",
    "        # TODO: Process each document\n",
    "        # - Show progress bar\n",
    "        # - Update ETA\n",
    "        # - Display partial results\n",
    "        pass\n",
    "\n",
    "# TODO: Test your progress tracker\n",
    "# Process multiple documents\n",
    "# Show live progress updates\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_1_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class ProgressTracker(BaseCallbackHandler):\n",
    "    def __init__(self, total_steps: int = 100):\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "        self.start_time = None\n",
    "        self.step_times = []\n",
    "        self.partial_results = []\n",
    "        self.current_operation = None\n",
    "        self.progress_bar = None\n",
    "        self.token_count = 0\n",
    "    \n",
    "    def start_operation(self, operation_name: str, total_items: int = None):\n",
    "        '''Start tracking a new operation.'''\n",
    "        self.current_operation = operation_name\n",
    "        self.start_time = time.time()\n",
    "        self.current_step = 0\n",
    "        self.token_count = 0\n",
    "        \n",
    "        if total_items:\n",
    "            self.total_steps = total_items\n",
    "        \n",
    "        # Create progress bar\n",
    "        self.progress_bar = tqdm(\n",
    "            total=self.total_steps,\n",
    "            desc=operation_name,\n",
    "            unit=\"steps\",\n",
    "            bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n",
    "        )\n",
    "    \n",
    "    def update_progress(self, step_name: str, progress: float = 1.0):\n",
    "        '''Update progress for current operation.'''\n",
    "        self.current_step += progress\n",
    "        \n",
    "        # Update progress bar\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.update(progress)\n",
    "            self.progress_bar.set_postfix_str(step_name[:30])\n",
    "        \n",
    "        # Track step timing\n",
    "        current_time = time.time()\n",
    "        if self.start_time:\n",
    "            elapsed = current_time - self.start_time\n",
    "            self.step_times.append(elapsed)\n",
    "    \n",
    "    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):\n",
    "        '''Track LLM operation start.'''\n",
    "        if not self.current_operation:\n",
    "            self.start_operation(\"LLM Generation\")\n",
    "        \n",
    "        # Estimate tokens from prompt\n",
    "        prompt_length = sum(len(p) for p in prompts)\n",
    "        estimated_tokens = prompt_length // 4\n",
    "        \n",
    "        self.update_progress(f\"Processing prompt (~{estimated_tokens} tokens)\", 0.1)\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        '''Update progress as tokens arrive.'''\n",
    "        self.token_count += 1\n",
    "        \n",
    "        # Update every 10 tokens to avoid too frequent updates\n",
    "        if self.token_count % 10 == 0:\n",
    "            self.update_progress(f\"Generating... ({self.token_count} tokens)\", 0.1)\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        '''Track LLM completion.'''\n",
    "        self.update_progress(\"Generation complete\", 1.0)\n",
    "        \n",
    "        # Store partial result\n",
    "        if response.generations:\n",
    "            text = response.generations[0][0].text\n",
    "            self.partial_results.append({\n",
    "                \"operation\": self.current_operation,\n",
    "                \"result\": text[:100] + \"...\" if len(text) > 100 else text,\n",
    "                \"tokens\": self.token_count\n",
    "            })\n",
    "    \n",
    "    def get_eta(self) -> float:\n",
    "        '''Calculate estimated time to completion.'''\n",
    "        if not self.step_times or self.current_step == 0:\n",
    "            return 0\n",
    "        \n",
    "        avg_step_time = sum(self.step_times) / len(self.step_times)\n",
    "        remaining_steps = self.total_steps - self.current_step\n",
    "        eta = avg_step_time * remaining_steps\n",
    "        \n",
    "        return eta\n",
    "    \n",
    "    def get_status(self) -> Dict:\n",
    "        '''Get current status and ETA.'''\n",
    "        progress_pct = (self.current_step / self.total_steps * 100) if self.total_steps > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"operation\": self.current_operation,\n",
    "            \"progress\": f\"{progress_pct:.1f}%\",\n",
    "            \"steps_completed\": self.current_step,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"eta_seconds\": self.get_eta(),\n",
    "            \"partial_results\": len(self.partial_results),\n",
    "            \"tokens_generated\": self.token_count\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        '''Close progress bar.'''\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.close()\n",
    "\n",
    "# Document processor with progress tracking\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7, streaming=True)\n",
    "        self.progress_tracker = None\n",
    "        self.results = []\n",
    "    \n",
    "    def process_documents(self, documents: List[str], operation: str = \"summarize\"):\n",
    "        '''Process documents with real-time progress.'''\n",
    "        # Create progress tracker\n",
    "        self.progress_tracker = ProgressTracker(total_steps=len(documents))\n",
    "        \n",
    "        # Configure LLM with progress callback\n",
    "        self.llm.callbacks = [self.progress_tracker]\n",
    "        \n",
    "        # Start operation\n",
    "        self.progress_tracker.start_operation(\n",
    "            f\"Processing {len(documents)} documents\",\n",
    "            total_items=len(documents)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìÑ Starting document processing: {operation}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Process each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            try:\n",
    "                # Show current document\n",
    "                self.progress_tracker.update_progress(\n",
    "                    f\"Document {i+1}/{len(documents)}: {doc[:50]}...\",\n",
    "                    0\n",
    "                )\n",
    "                \n",
    "                # Process based on operation\n",
    "                if operation == \"summarize\":\n",
    "                    prompt = f\"Summarize this text in 2 sentences: {doc}\"\n",
    "                elif operation == \"analyze\":\n",
    "                    prompt = f\"Analyze the key points in: {doc}\"\n",
    "                elif operation == \"translate\":\n",
    "                    prompt = f\"Translate to Spanish: {doc}\"\n",
    "                else:\n",
    "                    prompt = f\"Process this text: {doc}\"\n",
    "                \n",
    "                # Generate with streaming (progress tracked via callback)\n",
    "                result = self.llm.predict(prompt)\n",
    "                \n",
    "                # Store result\n",
    "                self.results.append({\n",
    "                    \"document\": doc[:100],\n",
    "                    \"result\": result,\n",
    "                    \"status\": \"completed\"\n",
    "                })\n",
    "                \n",
    "                # Update main progress\n",
    "                self.progress_tracker.update_progress(\n",
    "                    f\"Completed document {i+1}\",\n",
    "                    1\n",
    "                )\n",
    "                \n",
    "                # Show partial result\n",
    "                print(f\"\\n‚úÖ Document {i+1} processed\")\n",
    "                print(f\"   Result preview: {result[:100]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error processing document {i+1}: {e}\")\n",
    "                self.results.append({\n",
    "                    \"document\": doc[:100],\n",
    "                    \"result\": None,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                self.progress_tracker.update_progress(\"Error\", 1)\n",
    "        \n",
    "        # Close progress bar\n",
    "        self.progress_tracker.close()\n",
    "        \n",
    "        # Final status\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        status = self.progress_tracker.get_status()\n",
    "        print(\"\\nüìä Final Status:\")\n",
    "        for key, value in status.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # Summary\n",
    "        successful = sum(1 for r in self.results if r[\"status\"] == \"completed\")\n",
    "        print(f\"\\n‚úÖ Successfully processed: {successful}/{len(documents)} documents\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# Test the progress tracker\n",
    "print(\"Real-time Progress Tracking Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample documents\n",
    "test_documents = [\n",
    "    \"Artificial intelligence is transforming how we work and live.\",\n",
    "    \"Machine learning algorithms can identify patterns in vast amounts of data.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning has revolutionized computer vision applications.\",\n",
    "    \"Robotics combines AI with mechanical systems for automation.\"\n",
    "]\n",
    "\n",
    "# Process documents with progress tracking\n",
    "processor = DocumentProcessor()\n",
    "results = processor.process_documents(test_documents, operation=\"summarize\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nüìã Processing Complete! Results stored.\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build a real-time progress tracking system for document processing!\")\n",
    "print(\"The solution includes progress bars, ETA calculation, and partial results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 2: Async Operations and Concurrent Processing\n",
    "\n",
    "Let's explore async operations for better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "async_operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async LLM operations\n",
    "\n",
    "async def async_generate(llm: ChatOpenAI, prompt: str) -> str:\n",
    "    \"\"\"Generate text asynchronously.\"\"\"\n",
    "    # Use apredict for async generation\n",
    "    result = await llm.apredict(prompt)\n",
    "    return result\n",
    "\n",
    "async def process_multiple_prompts(prompts: List[str]):\n",
    "    \"\"\"Process multiple prompts concurrently.\"\"\"\n",
    "    llm = ChatOpenAI(temperature=0.7)\n",
    "    \n",
    "    # Create tasks for concurrent execution\n",
    "    tasks = [async_generate(llm, prompt) for prompt in prompts]\n",
    "    \n",
    "    # Run all tasks concurrently\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return results, elapsed\n",
    "\n",
    "# Test async processing\n",
    "test_prompts = [\n",
    "    \"Write a haiku about coding.\",\n",
    "    \"Explain recursion in one sentence.\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"Describe the color blue to a blind person.\",\n",
    "    \"Write a joke about databases.\"\n",
    "]\n",
    "\n",
    "print(\"Async Concurrent Processing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run async function\n",
    "results, time_async = asyncio.run(process_multiple_prompts(test_prompts))\n",
    "\n",
    "print(f\"\\n‚ö° Processed {len(test_prompts)} prompts in {time_async:.2f} seconds\")\n",
    "print(f\"Average time per prompt: {time_async/len(test_prompts):.2f} seconds\\n\")\n",
    "\n",
    "for i, (prompt, result) in enumerate(zip(test_prompts, results)):\n",
    "    print(f\"\\n{i+1}. Prompt: {prompt}\")\n",
    "    print(f\"   Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "async_streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async streaming with callbacks\n",
    "\n",
    "class AsyncStreamingCallback(AsyncCallbackHandler):\n",
    "    \"\"\"Async callback for streaming.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.is_streaming = False\n",
    "    \n",
    "    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):\n",
    "        \"\"\"Called when LLM starts.\"\"\"\n",
    "        self.is_streaming = True\n",
    "        self.tokens = []\n",
    "        print(\"\\nüöÄ Starting async generation...\")\n",
    "    \n",
    "    async def on_llm_new_token(self, token: str, **kwargs):\n",
    "        \"\"\"Handle new token asynchronously.\"\"\"\n",
    "        self.tokens.append(token)\n",
    "        print(token, end=\"\", flush=True)\n",
    "        \n",
    "        # Simulate async processing\n",
    "        await asyncio.sleep(0.01)  # Small delay to show async behavior\n",
    "    \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        \"\"\"Called when LLM ends.\"\"\"\n",
    "        self.is_streaming = False\n",
    "        print(\"\\n‚úÖ Async generation complete!\")\n",
    "\n",
    "# Async streaming generator\n",
    "async def stream_response(prompt: str) -> AsyncIterator[str]:\n",
    "    \"\"\"Stream response tokens asynchronously.\"\"\"\n",
    "    callback = AsyncStreamingCallback()\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.7,\n",
    "        streaming=True,\n",
    "        callbacks=[callback]\n",
    "    )\n",
    "    \n",
    "    # Start generation\n",
    "    task = asyncio.create_task(llm.apredict(prompt))\n",
    "    \n",
    "    # Yield tokens as they arrive\n",
    "    while callback.is_streaming or not task.done():\n",
    "        if callback.tokens:\n",
    "            token = callback.tokens.pop(0)\n",
    "            yield token\n",
    "        else:\n",
    "            await asyncio.sleep(0.01)\n",
    "    \n",
    "    # Ensure task completes\n",
    "    await task\n",
    "\n",
    "# Test async streaming\n",
    "async def test_streaming():\n",
    "    prompt = \"Tell me a story about a brave little robot.\"\n",
    "    \n",
    "    print(\"Async Streaming Demo:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"Response: \", end=\"\")\n",
    "    \n",
    "    async for token in stream_response(prompt):\n",
    "        # Process each token as it arrives\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\nStreaming complete!\")\n",
    "\n",
    "# Run async streaming test\n",
    "asyncio.run(test_streaming())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch_async",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing with async and progress\n",
    "\n",
    "class AsyncBatchProcessor:\n",
    "    \"\"\"Process batches of items asynchronously with progress tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 5):\n",
    "        self.batch_size = batch_size\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.results = []\n",
    "        self.errors = []\n",
    "    \n",
    "    async def process_item(self, item: Dict, index: int) -> Dict:\n",
    "        \"\"\"Process a single item.\"\"\"\n",
    "        try:\n",
    "            # Simulate different processing based on item type\n",
    "            if item[\"type\"] == \"summarize\":\n",
    "                prompt = f\"Summarize: {item['content']}\"\n",
    "            elif item[\"type\"] == \"translate\":\n",
    "                prompt = f\"Translate to {item.get('language', 'Spanish')}: {item['content']}\"\n",
    "            else:\n",
    "                prompt = f\"Process: {item['content']}\"\n",
    "            \n",
    "            # Async generation\n",
    "            result = await self.llm.apredict(prompt)\n",
    "            \n",
    "            return {\n",
    "                \"index\": index,\n",
    "                \"input\": item,\n",
    "                \"output\": result,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"index\": index,\n",
    "                \"input\": item,\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "    \n",
    "    async def process_batch(self, items: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of items concurrently.\"\"\"\n",
    "        tasks = [\n",
    "            self.process_item(item, i) \n",
    "            for i, item in enumerate(items)\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Handle exceptions\n",
    "        processed_results = []\n",
    "        for r in results:\n",
    "            if isinstance(r, Exception):\n",
    "                processed_results.append({\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(r)\n",
    "                })\n",
    "            else:\n",
    "                processed_results.append(r)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    async def process_all(self, items: List[Dict]):\n",
    "        \"\"\"Process all items in batches with progress.\"\"\"\n",
    "        total_items = len(items)\n",
    "        total_batches = (total_items + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        print(f\"\\nüì¶ Processing {total_items} items in {total_batches} batches\")\n",
    "        print(f\"   Batch size: {self.batch_size}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(total=total_items, desc=\"Processing items\")\n",
    "        \n",
    "        all_results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_num in range(total_batches):\n",
    "            # Get batch\n",
    "            start_idx = batch_num * self.batch_size\n",
    "            end_idx = min(start_idx + self.batch_size, total_items)\n",
    "            batch = items[start_idx:end_idx]\n",
    "            \n",
    "            # Process batch\n",
    "            print(f\"\\nüîÑ Processing batch {batch_num + 1}/{total_batches}...\")\n",
    "            batch_results = await self.process_batch(batch)\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.update(len(batch))\n",
    "            \n",
    "            # Store results\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Show batch summary\n",
    "            successful = sum(1 for r in batch_results if r[\"status\"] == \"success\")\n",
    "            print(f\"   Batch complete: {successful}/{len(batch)} successful\")\n",
    "        \n",
    "        pbar.close()\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Summary\n",
    "        successful_total = sum(1 for r in all_results if r[\"status\"] == \"success\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"\\nüìä Batch Processing Complete:\")\n",
    "        print(f\"   Total items: {total_items}\")\n",
    "        print(f\"   Successful: {successful_total}\")\n",
    "        print(f\"   Failed: {total_items - successful_total}\")\n",
    "        print(f\"   Time: {elapsed:.2f} seconds\")\n",
    "        print(f\"   Throughput: {total_items/elapsed:.2f} items/second\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "# Test batch processing\n",
    "async def test_batch_processing():\n",
    "    # Create test items\n",
    "    test_items = [\n",
    "        {\"type\": \"summarize\", \"content\": \"AI is transforming industries.\"},\n",
    "        {\"type\": \"translate\", \"content\": \"Hello world\", \"language\": \"French\"},\n",
    "        {\"type\": \"summarize\", \"content\": \"Machine learning uses data patterns.\"},\n",
    "        {\"type\": \"translate\", \"content\": \"Good morning\", \"language\": \"Japanese\"},\n",
    "        {\"type\": \"process\", \"content\": \"Natural language processing.\"},\n",
    "        {\"type\": \"summarize\", \"content\": \"Deep learning mimics brain neurons.\"},\n",
    "        {\"type\": \"translate\", \"content\": \"Thank you\", \"language\": \"German\"},\n",
    "        {\"type\": \"process\", \"content\": \"Computer vision applications.\"},\n",
    "    ]\n",
    "    \n",
    "    processor = AsyncBatchProcessor(batch_size=3)\n",
    "    results = await processor.process_all(test_items)\n",
    "    \n",
    "    # Show sample results\n",
    "    print(\"\\nüìù Sample Results:\")\n",
    "    for r in results[:3]:\n",
    "        if r[\"status\"] == \"success\":\n",
    "            print(f\"\\n   Input: {r['input']['content'][:50]}\")\n",
    "            print(f\"   Output: {r['output'][:100]}...\")\n",
    "\n",
    "# Run batch processing test\n",
    "asyncio.run(test_batch_processing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 2: Build an Async Chat Server\n",
    "\n",
    "Create a real-time chat server with streaming responses.\n",
    "\n",
    "**Task**: Build a chat server that:\n",
    "1. Handles multiple concurrent users\n",
    "2. Streams responses in real-time\n",
    "3. Manages conversation history per user\n",
    "4. Implements typing indicators\n",
    "5. Provides response time analytics\n",
    "\n",
    "Requirements:\n",
    "- Support multiple simultaneous conversations\n",
    "- Stream tokens as they're generated\n",
    "- Track metrics per user\n",
    "- Handle connection/disconnection gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_2_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your async chat server\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, user_id: str):\n",
    "        self.user_id = user_id\n",
    "        self.history = []\n",
    "        self.is_typing = False\n",
    "        # TODO: Initialize session components\n",
    "        pass\n",
    "    \n",
    "    async def send_message(self, message: str) -> AsyncIterator[str]:\n",
    "        \"\"\"Send message and stream response.\"\"\"\n",
    "        # TODO: Implement streaming response\n",
    "        pass\n",
    "\n",
    "class AsyncChatServer:\n",
    "    def __init__(self):\n",
    "        self.sessions = {}  # user_id -> ChatSession\n",
    "        # TODO: Initialize server components\n",
    "        pass\n",
    "    \n",
    "    async def connect_user(self, user_id: str) -> ChatSession:\n",
    "        \"\"\"Connect a new user or retrieve existing session.\"\"\"\n",
    "        # TODO: Create or retrieve session\n",
    "        pass\n",
    "    \n",
    "    async def handle_message(self, user_id: str, message: str):\n",
    "        \"\"\"Handle incoming message from user.\"\"\"\n",
    "        # TODO: Process message and stream response\n",
    "        pass\n",
    "    \n",
    "    async def broadcast_typing_indicator(self, user_id: str, is_typing: bool):\n",
    "        \"\"\"Broadcast typing status.\"\"\"\n",
    "        # TODO: Implement typing indicator\n",
    "        pass\n",
    "    \n",
    "    def get_analytics(self) -> Dict:\n",
    "        \"\"\"Get server analytics.\"\"\"\n",
    "        # TODO: Return usage statistics\n",
    "        pass\n",
    "\n",
    "# TODO: Implement WebSocket handler (simplified)\n",
    "async def simulate_chat_interaction():\n",
    "    \"\"\"Simulate chat server interactions.\"\"\"\n",
    "    # TODO: Create server\n",
    "    # TODO: Connect multiple users\n",
    "    # TODO: Send messages concurrently\n",
    "    # TODO: Show streaming responses\n",
    "    pass\n",
    "\n",
    "# TODO: Test your chat server\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_2_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, user_id: str):\n",
    "        self.user_id = user_id\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.history = []\n",
    "        self.is_typing = False\n",
    "        self.metrics = {\n",
    "            \"messages_sent\": 0,\n",
    "            \"messages_received\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"avg_response_time\": 0,\n",
    "            \"response_times\": []\n",
    "        }\n",
    "        self.llm = ChatOpenAI(temperature=0.7, streaming=True)\n",
    "        self.created_at = datetime.now()\n",
    "    \n",
    "    async def send_message(self, message: str) -> AsyncIterator[str]:\n",
    "        '''Send message and stream response.'''\n",
    "        # Update metrics\n",
    "        self.metrics[\"messages_sent\"] += 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Add to history\n",
    "        self.history.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Build prompt with history\n",
    "        messages = []\n",
    "        for h in self.history[-5:]:  # Keep last 5 messages for context\n",
    "            if h[\"role\"] == \"user\":\n",
    "                messages.append(HumanMessage(content=h[\"content\"]))\n",
    "            else:\n",
    "                messages.append(AIMessage(content=h[\"content\"]))\n",
    "        \n",
    "        # Set typing indicator\n",
    "        self.is_typing = True\n",
    "        \n",
    "        try:\n",
    "            # Stream response\n",
    "            full_response = \"\"\n",
    "            token_count = 0\n",
    "            \n",
    "            async for chunk in self.llm.astream(messages):\n",
    "                if hasattr(chunk, 'content'):\n",
    "                    token = chunk.content\n",
    "                    full_response += token\n",
    "                    token_count += 1\n",
    "                    yield token\n",
    "            \n",
    "            # Update history\n",
    "            self.history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "            \n",
    "            # Update metrics\n",
    "            response_time = time.time() - start_time\n",
    "            self.metrics[\"messages_received\"] += 1\n",
    "            self.metrics[\"total_tokens\"] += token_count\n",
    "            self.metrics[\"response_times\"].append(response_time)\n",
    "            self.metrics[\"avg_response_time\"] = sum(self.metrics[\"response_times\"]) / len(self.metrics[\"response_times\"])\n",
    "            \n",
    "        finally:\n",
    "            self.is_typing = False\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        '''Get session summary.'''\n",
    "        return {\n",
    "            \"user_id\": self.user_id,\n",
    "            \"session_id\": self.session_id,\n",
    "            \"message_count\": len(self.history),\n",
    "            \"metrics\": self.metrics,\n",
    "            \"duration\": (datetime.now() - self.created_at).total_seconds()\n",
    "        }\n",
    "\n",
    "class AsyncChatServer:\n",
    "    def __init__(self):\n",
    "        self.sessions = {}  # user_id -> ChatSession\n",
    "        self.active_users = set()\n",
    "        self.server_metrics = {\n",
    "            \"total_connections\": 0,\n",
    "            \"total_messages\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"peak_concurrent_users\": 0\n",
    "        }\n",
    "        self.typing_indicators = {}  # user_id -> is_typing\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    async def connect_user(self, user_id: str) -> ChatSession:\n",
    "        '''Connect a new user or retrieve existing session.'''\n",
    "        if user_id not in self.sessions:\n",
    "            self.sessions[user_id] = ChatSession(user_id)\n",
    "            self.server_metrics[\"total_connections\"] += 1\n",
    "            print(f\"üë§ User {user_id} connected\")\n",
    "        else:\n",
    "            print(f\"üë§ User {user_id} reconnected\")\n",
    "        \n",
    "        self.active_users.add(user_id)\n",
    "        \n",
    "        # Update peak concurrent users\n",
    "        current_users = len(self.active_users)\n",
    "        if current_users > self.server_metrics[\"peak_concurrent_users\"]:\n",
    "            self.server_metrics[\"peak_concurrent_users\"] = current_users\n",
    "        \n",
    "        return self.sessions[user_id]\n",
    "    \n",
    "    async def disconnect_user(self, user_id: str):\n",
    "        '''Disconnect a user.'''\n",
    "        if user_id in self.active_users:\n",
    "            self.active_users.remove(user_id)\n",
    "            print(f\"üë§ User {user_id} disconnected\")\n",
    "    \n",
    "    async def handle_message(self, user_id: str, message: str) -> str:\n",
    "        '''Handle incoming message from user.'''\n",
    "        if user_id not in self.sessions:\n",
    "            await self.connect_user(user_id)\n",
    "        \n",
    "        session = self.sessions[user_id]\n",
    "        \n",
    "        # Update server metrics\n",
    "        self.server_metrics[\"total_messages\"] += 1\n",
    "        \n",
    "        # Broadcast typing indicator\n",
    "        await self.broadcast_typing_indicator(user_id, True)\n",
    "        \n",
    "        # Stream response\n",
    "        full_response = \"\"\n",
    "        print(f\"\\nüí¨ User {user_id}: {message}\")\n",
    "        print(f\"ü§ñ Assistant: \", end=\"\")\n",
    "        \n",
    "        async for token in session.send_message(message):\n",
    "            full_response += token\n",
    "            print(token, end=\"\", flush=True)\n",
    "            self.server_metrics[\"total_tokens\"] += 1\n",
    "        \n",
    "        print()  # New line after response\n",
    "        \n",
    "        # Stop typing indicator\n",
    "        await self.broadcast_typing_indicator(user_id, False)\n",
    "        \n",
    "        return full_response\n",
    "    \n",
    "    async def broadcast_typing_indicator(self, user_id: str, is_typing: bool):\n",
    "        '''Broadcast typing status.'''\n",
    "        self.typing_indicators[user_id] = is_typing\n",
    "        \n",
    "        if is_typing:\n",
    "            # In real implementation, broadcast to other users\n",
    "            # print(f\"   [{user_id} is typing...]\")\n",
    "            pass\n",
    "    \n",
    "    async def handle_concurrent_users(self, user_messages: List[tuple]):\n",
    "        '''Handle messages from multiple users concurrently.'''\n",
    "        tasks = []\n",
    "        \n",
    "        for user_id, message in user_messages:\n",
    "            task = self.handle_message(user_id, message)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        # Process all messages concurrently\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        return responses\n",
    "    \n",
    "    def get_analytics(self) -> Dict:\n",
    "        '''Get server analytics.'''\n",
    "        uptime = (datetime.now() - self.start_time).total_seconds()\n",
    "        \n",
    "        # Calculate per-user statistics\n",
    "        user_stats = []\n",
    "        for user_id, session in self.sessions.items():\n",
    "            stats = session.get_summary()\n",
    "            user_stats.append(stats)\n",
    "        \n",
    "        return {\n",
    "            \"server_uptime_seconds\": uptime,\n",
    "            \"active_users\": len(self.active_users),\n",
    "            \"total_users\": len(self.sessions),\n",
    "            \"server_metrics\": self.server_metrics,\n",
    "            \"user_statistics\": user_stats,\n",
    "            \"currently_typing\": [uid for uid, typing in self.typing_indicators.items() if typing]\n",
    "        }\n",
    "\n",
    "# Simulate chat server interactions\n",
    "async def simulate_chat_interaction():\n",
    "    '''Simulate multiple users chatting.'''\n",
    "    print(\"\\nüöÄ Async Chat Server Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create server\n",
    "    server = AsyncChatServer()\n",
    "    \n",
    "    # Connect users\n",
    "    users = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "    for user in users:\n",
    "        await server.connect_user(user)\n",
    "    \n",
    "    print(f\"\\nüìä Connected {len(users)} users\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Simulate conversations\n",
    "    print(\"\\nüí¨ Starting conversations...\\n\")\n",
    "    \n",
    "    # Round 1: Individual messages\n",
    "    await server.handle_message(\"Alice\", \"What's the weather like today?\")\n",
    "    await server.handle_message(\"Bob\", \"Tell me a joke about programming.\")\n",
    "    \n",
    "    # Round 2: Concurrent messages\n",
    "    print(\"\\nüîÑ Processing concurrent messages...\")\n",
    "    concurrent_messages = [\n",
    "        (\"Alice\", \"How does machine learning work?\"),\n",
    "        (\"Bob\", \"What's recursion?\"),\n",
    "        (\"Charlie\", \"Explain quantum computing.\")\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    responses = await server.handle_concurrent_users(concurrent_messages)\n",
    "    concurrent_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚ö° Processed {len(concurrent_messages)} messages concurrently in {concurrent_time:.2f}s\")\n",
    "    \n",
    "    # Disconnect a user\n",
    "    await server.disconnect_user(\"Charlie\")\n",
    "    \n",
    "    # Get analytics\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\\nüìä Server Analytics:\")\n",
    "    \n",
    "    analytics = server.get_analytics()\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è  Server Status:\")\n",
    "    print(f\"   Uptime: {analytics['server_uptime_seconds']:.1f} seconds\")\n",
    "    print(f\"   Active users: {analytics['active_users']}\")\n",
    "    print(f\"   Total users: {analytics['total_users']}\")\n",
    "    \n",
    "    print(f\"\\nüìà Server Metrics:\")\n",
    "    for key, value in analytics['server_metrics'].items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüë• User Statistics:\")\n",
    "    for user_stat in analytics['user_statistics']:\n",
    "        print(f\"\\n   User: {user_stat['user_id']}\")\n",
    "        print(f\"   Messages: {user_stat['message_count']}\")\n",
    "        print(f\"   Avg response time: {user_stat['metrics']['avg_response_time']:.2f}s\")\n",
    "        print(f\"   Total tokens: {user_stat['metrics']['total_tokens']}\")\n",
    "\n",
    "# Run the simulation\n",
    "asyncio.run(simulate_chat_interaction())\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build an async chat server with streaming and multi-user support!\")\n",
    "print(\"The solution includes concurrent handling, metrics, and typing indicators.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 3: WebSocket Real-time Communication\n",
    "\n",
    "Let's build real-time WebSocket-based communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "websocket_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated WebSocket communication (actual WebSocket requires server setup)\n",
    "\n",
    "class WebSocketMessage:\n",
    "    \"\"\"Simulated WebSocket message.\"\"\"\n",
    "    def __init__(self, type: str, data: Any):\n",
    "        self.type = type\n",
    "        self.data = data\n",
    "        self.timestamp = datetime.now()\n",
    "\n",
    "class RealtimeStreamHandler:\n",
    "    \"\"\"Handle real-time streaming over WebSocket.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connections = {}  # connection_id -> connection_info\n",
    "        self.message_queue = asyncio.Queue()\n",
    "        self.llm = ChatOpenAI(temperature=0.7, streaming=True)\n",
    "    \n",
    "    async def handle_connection(self, connection_id: str):\n",
    "        \"\"\"Handle new WebSocket connection.\"\"\"\n",
    "        self.connections[connection_id] = {\n",
    "            \"id\": connection_id,\n",
    "            \"connected_at\": datetime.now(),\n",
    "            \"messages_sent\": 0,\n",
    "            \"messages_received\": 0\n",
    "        }\n",
    "        \n",
    "        # Send welcome message\n",
    "        await self.send_message(\n",
    "            connection_id,\n",
    "            WebSocketMessage(\"welcome\", {\"message\": \"Connected to real-time chat\"})\n",
    "        )\n",
    "    \n",
    "    async def send_message(self, connection_id: str, message: WebSocketMessage):\n",
    "        \"\"\"Send message to specific connection.\"\"\"\n",
    "        if connection_id in self.connections:\n",
    "            self.connections[connection_id][\"messages_sent\"] += 1\n",
    "            # Simulate sending over WebSocket\n",
    "            print(f\"‚Üí [{connection_id}] {message.type}: {message.data}\")\n",
    "    \n",
    "    async def broadcast_message(self, message: WebSocketMessage, exclude: str = None):\n",
    "        \"\"\"Broadcast message to all connections.\"\"\"\n",
    "        for conn_id in self.connections:\n",
    "            if conn_id != exclude:\n",
    "                await self.send_message(conn_id, message)\n",
    "    \n",
    "    async def handle_chat_message(self, connection_id: str, message_text: str):\n",
    "        \"\"\"Handle incoming chat message with streaming response.\"\"\"\n",
    "        self.connections[connection_id][\"messages_received\"] += 1\n",
    "        \n",
    "        # Send acknowledgment\n",
    "        await self.send_message(\n",
    "            connection_id,\n",
    "            WebSocketMessage(\"ack\", {\"received\": message_text})\n",
    "        )\n",
    "        \n",
    "        # Start streaming response\n",
    "        await self.send_message(\n",
    "            connection_id,\n",
    "            WebSocketMessage(\"stream_start\", {})\n",
    "        )\n",
    "        \n",
    "        # Stream tokens\n",
    "        token_buffer = \"\"\n",
    "        token_count = 0\n",
    "        \n",
    "        async for chunk in self.llm.astream(message_text):\n",
    "            if hasattr(chunk, 'content'):\n",
    "                token = chunk.content\n",
    "                token_buffer += token\n",
    "                token_count += 1\n",
    "                \n",
    "                # Send token\n",
    "                await self.send_message(\n",
    "                    connection_id,\n",
    "                    WebSocketMessage(\"stream_token\", {\"token\": token})\n",
    "                )\n",
    "                \n",
    "                # Simulate network delay\n",
    "                await asyncio.sleep(0.01)\n",
    "        \n",
    "        # End streaming\n",
    "        await self.send_message(\n",
    "            connection_id,\n",
    "            WebSocketMessage(\"stream_end\", {\n",
    "                \"total_tokens\": token_count,\n",
    "                \"complete_response\": token_buffer\n",
    "            })\n",
    "        )\n",
    "\n",
    "# Test real-time streaming\n",
    "async def test_realtime_streaming():\n",
    "    print(\"Real-time WebSocket Streaming Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    handler = RealtimeStreamHandler()\n",
    "    \n",
    "    # Simulate client connection\n",
    "    client_id = \"client_001\"\n",
    "    await handler.handle_connection(client_id)\n",
    "    \n",
    "    # Send chat message\n",
    "    print(f\"\\n‚Üê [{client_id}] Sending: 'Explain WebSockets in one paragraph'\\n\")\n",
    "    await handler.handle_chat_message(\n",
    "        client_id,\n",
    "        \"Explain WebSockets in one paragraph\"\n",
    "    )\n",
    "    \n",
    "    # Show connection stats\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\\nConnection Statistics:\")\n",
    "    for conn_id, info in handler.connections.items():\n",
    "        print(f\"  {conn_id}:\")\n",
    "        print(f\"    Messages sent: {info['messages_sent']}\")\n",
    "        print(f\"    Messages received: {info['messages_received']}\")\n",
    "\n",
    "asyncio.run(test_realtime_streaming())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 3: Build a Real-time Dashboard\n",
    "\n",
    "Create a real-time dashboard that monitors AI operations.\n",
    "\n",
    "**Task**: Build a dashboard that:\n",
    "1. Shows live token generation stats\n",
    "2. Displays concurrent user activity\n",
    "3. Tracks error rates and response times\n",
    "4. Provides cost estimates in real-time\n",
    "5. Includes alerts for anomalies\n",
    "\n",
    "Requirements:\n",
    "- Real-time metric updates\n",
    "- Historical data visualization\n",
    "- Alert system for thresholds\n",
    "- Export functionality for reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_3_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your real-time dashboard\n",
    "\n",
    "class MetricsCollector:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize metrics storage\n",
    "        pass\n",
    "    \n",
    "    def record_metric(self, metric_type: str, value: float):\n",
    "        \"\"\"Record a metric value.\"\"\"\n",
    "        # TODO: Store metric with timestamp\n",
    "        pass\n",
    "    \n",
    "    def get_current_stats(self) -> Dict:\n",
    "        \"\"\"Get current statistics.\"\"\"\n",
    "        # TODO: Calculate current stats\n",
    "        pass\n",
    "\n",
    "class RealtimeDashboard:\n",
    "    def __init__(self):\n",
    "        self.metrics = MetricsCollector()\n",
    "        # TODO: Initialize dashboard components\n",
    "        pass\n",
    "    \n",
    "    async def update_dashboard(self):\n",
    "        \"\"\"Update dashboard with latest metrics.\"\"\"\n",
    "        # TODO: Fetch and display metrics\n",
    "        pass\n",
    "    \n",
    "    async def monitor_operations(self):\n",
    "        \"\"\"Monitor AI operations in real-time.\"\"\"\n",
    "        # TODO: Track operations and update metrics\n",
    "        pass\n",
    "    \n",
    "    def check_alerts(self) -> List[str]:\n",
    "        \"\"\"Check for alert conditions.\"\"\"\n",
    "        # TODO: Check thresholds and return alerts\n",
    "        pass\n",
    "    \n",
    "    def export_report(self, format: str = \"json\") -> str:\n",
    "        \"\"\"Export dashboard report.\"\"\"\n",
    "        # TODO: Generate and export report\n",
    "        pass\n",
    "\n",
    "# TODO: Test your dashboard\n",
    "# Simulate various operations\n",
    "# Show real-time updates\n",
    "# Trigger alerts\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_3_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "from collections import deque, defaultdict\n",
    "import statistics\n",
    "\n",
    "class MetricsCollector:\n",
    "    def __init__(self, window_size: int = 100):\n",
    "        self.window_size = window_size\n",
    "        self.metrics = defaultdict(lambda: deque(maxlen=window_size))\n",
    "        self.totals = defaultdict(float)\n",
    "        self.counts = defaultdict(int)\n",
    "        self.alerts = []\n",
    "    \n",
    "    def record_metric(self, metric_type: str, value: float, timestamp: datetime = None):\n",
    "        '''Record a metric value.'''\n",
    "        timestamp = timestamp or datetime.now()\n",
    "        \n",
    "        # Store in sliding window\n",
    "        self.metrics[metric_type].append({\n",
    "            \"value\": value,\n",
    "            \"timestamp\": timestamp\n",
    "        })\n",
    "        \n",
    "        # Update totals\n",
    "        self.totals[metric_type] += value\n",
    "        self.counts[metric_type] += 1\n",
    "    \n",
    "    def get_current_stats(self) -> Dict:\n",
    "        '''Get current statistics.'''\n",
    "        stats = {}\n",
    "        \n",
    "        for metric_type, values in self.metrics.items():\n",
    "            if values:\n",
    "                recent_values = [v[\"value\"] for v in values]\n",
    "                stats[metric_type] = {\n",
    "                    \"current\": recent_values[-1] if recent_values else 0,\n",
    "                    \"mean\": statistics.mean(recent_values),\n",
    "                    \"min\": min(recent_values),\n",
    "                    \"max\": max(recent_values),\n",
    "                    \"total\": self.totals[metric_type],\n",
    "                    \"count\": self.counts[metric_type]\n",
    "                }\n",
    "                \n",
    "                # Add percentiles for numeric metrics\n",
    "                if len(recent_values) > 1:\n",
    "                    stats[metric_type][\"p50\"] = statistics.median(recent_values)\n",
    "                    if len(recent_values) > 10:\n",
    "                        sorted_values = sorted(recent_values)\n",
    "                        p95_idx = int(len(sorted_values) * 0.95)\n",
    "                        stats[metric_type][\"p95\"] = sorted_values[p95_idx]\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def get_time_series(self, metric_type: str, last_n: int = 50) -> List[Dict]:\n",
    "        '''Get time series data for a metric.'''\n",
    "        if metric_type not in self.metrics:\n",
    "            return []\n",
    "        \n",
    "        values = list(self.metrics[metric_type])\n",
    "        return values[-last_n:]\n",
    "\n",
    "class RealtimeDashboard:\n",
    "    def __init__(self):\n",
    "        self.metrics = MetricsCollector()\n",
    "        self.llm_monitor = None\n",
    "        self.start_time = datetime.now()\n",
    "        self.alert_thresholds = {\n",
    "            \"response_time\": 5.0,  # seconds\n",
    "            \"error_rate\": 0.1,     # 10%\n",
    "            \"cost_per_hour\": 10.0, # dollars\n",
    "            \"tokens_per_second\": 1000  # max throughput\n",
    "        }\n",
    "        self.active_sessions = 0\n",
    "        self.total_requests = 0\n",
    "        self.total_errors = 0\n",
    "    \n",
    "    def create_monitoring_callback(self):\n",
    "        '''Create callback for monitoring LLM operations.'''\n",
    "        parent = self\n",
    "        \n",
    "        class MonitoringCallback(BaseCallbackHandler):\n",
    "            def __init__(self):\n",
    "                self.start_time = None\n",
    "                self.token_count = 0\n",
    "            \n",
    "            def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):\n",
    "                self.start_time = time.time()\n",
    "                self.token_count = 0\n",
    "                parent.active_sessions += 1\n",
    "                parent.total_requests += 1\n",
    "            \n",
    "            def on_llm_new_token(self, token: str, **kwargs):\n",
    "                self.token_count += 1\n",
    "                parent.metrics.record_metric(\"tokens_generated\", 1)\n",
    "            \n",
    "            def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "                if self.start_time:\n",
    "                    response_time = time.time() - self.start_time\n",
    "                    parent.metrics.record_metric(\"response_time\", response_time)\n",
    "                    \n",
    "                    # Calculate tokens per second\n",
    "                    if response_time > 0:\n",
    "                        tps = self.token_count / response_time\n",
    "                        parent.metrics.record_metric(\"tokens_per_second\", tps)\n",
    "                    \n",
    "                    # Estimate cost (example: $0.002 per 1K tokens)\n",
    "                    cost = (self.token_count / 1000) * 0.002\n",
    "                    parent.metrics.record_metric(\"cost\", cost)\n",
    "                \n",
    "                parent.active_sessions -= 1\n",
    "            \n",
    "            def on_llm_error(self, error: Exception, **kwargs):\n",
    "                parent.total_errors += 1\n",
    "                parent.metrics.record_metric(\"errors\", 1)\n",
    "                parent.active_sessions -= 1\n",
    "        \n",
    "        return MonitoringCallback()\n",
    "    \n",
    "    async def update_dashboard(self):\n",
    "        '''Update dashboard with latest metrics.'''\n",
    "        stats = self.metrics.get_current_stats()\n",
    "        \n",
    "        # Clear screen (in terminal)\n",
    "        print(\"\\033[2J\\033[H\")  # Clear screen and move cursor to top\n",
    "        \n",
    "        # Dashboard header\n",
    "        print(\"=\" * 60)\n",
    "        print(\"           üéØ REAL-TIME AI OPERATIONS DASHBOARD\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Current time and uptime\n",
    "        uptime = (datetime.now() - self.start_time).total_seconds()\n",
    "        print(f\"\\n‚è∞ Current Time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"‚è±Ô∏è  Uptime: {uptime:.0f} seconds\")\n",
    "        \n",
    "        # Active sessions\n",
    "        print(f\"\\nüë• Active Sessions: {self.active_sessions}\")\n",
    "        print(f\"üìä Total Requests: {self.total_requests}\")\n",
    "        print(f\"‚ùå Total Errors: {self.total_errors}\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        print(\"\\nüìà Performance Metrics:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if \"response_time\" in stats:\n",
    "            rt = stats[\"response_time\"]\n",
    "            print(f\"Response Time:\")\n",
    "            print(f\"  Current: {rt['current']:.2f}s\")\n",
    "            print(f\"  Average: {rt['mean']:.2f}s\")\n",
    "            print(f\"  P95: {rt.get('p95', rt['max']):.2f}s\")\n",
    "        \n",
    "        if \"tokens_per_second\" in stats:\n",
    "            tps = stats[\"tokens_per_second\"]\n",
    "            print(f\"\\nThroughput:\")\n",
    "            print(f\"  Current: {tps['current']:.1f} tok/s\")\n",
    "            print(f\"  Average: {tps['mean']:.1f} tok/s\")\n",
    "            print(f\"  Peak: {tps['max']:.1f} tok/s\")\n",
    "        \n",
    "        # Cost tracking\n",
    "        print(\"\\nüí∞ Cost Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if \"cost\" in stats:\n",
    "            cost = stats[\"cost\"]\n",
    "            total_cost = cost['total']\n",
    "            cost_per_hour = (total_cost / uptime) * 3600 if uptime > 0 else 0\n",
    "            \n",
    "            print(f\"Total Cost: ${total_cost:.4f}\")\n",
    "            print(f\"Rate: ${cost_per_hour:.2f}/hour\")\n",
    "            print(f\"Avg per request: ${cost['mean']:.4f}\")\n",
    "        \n",
    "        # Error metrics\n",
    "        if self.total_requests > 0:\n",
    "            error_rate = self.total_errors / self.total_requests\n",
    "            print(f\"\\n‚ö†Ô∏è  Error Rate: {error_rate:.1%}\")\n",
    "        \n",
    "        # Alerts\n",
    "        alerts = self.check_alerts()\n",
    "        if alerts:\n",
    "            print(\"\\nüö® ACTIVE ALERTS:\")\n",
    "            print(\"-\" * 40)\n",
    "            for alert in alerts:\n",
    "                print(f\"  ‚ö†Ô∏è  {alert}\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ All systems operating normally\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    async def monitor_operations(self, duration: int = 30):\n",
    "        '''Monitor AI operations in real-time.'''\n",
    "        print(f\"Starting {duration}-second monitoring session...\\n\")\n",
    "        \n",
    "        # Create monitoring LLM\n",
    "        callback = self.create_monitoring_callback()\n",
    "        llm = ChatOpenAI(temperature=0.7, streaming=True, callbacks=[callback])\n",
    "        \n",
    "        # Simulate various operations\n",
    "        operations = [\n",
    "            \"Generate a haiku.\",\n",
    "            \"Explain quantum computing.\",\n",
    "            \"Write a short story.\",\n",
    "            \"Solve a math problem: 15 * 23\",\n",
    "            \"Translate 'Hello' to 5 languages.\"\n",
    "        ]\n",
    "        \n",
    "        start = time.time()\n",
    "        operation_count = 0\n",
    "        \n",
    "        while time.time() - start < duration:\n",
    "            # Pick random operation\n",
    "            import random\n",
    "            operation = random.choice(operations)\n",
    "            \n",
    "            # Run operation async\n",
    "            try:\n",
    "                await llm.apredict(operation)\n",
    "                operation_count += 1\n",
    "            except Exception as e:\n",
    "                self.total_errors += 1\n",
    "            \n",
    "            # Update dashboard\n",
    "            await self.update_dashboard()\n",
    "            \n",
    "            # Wait before next operation\n",
    "            await asyncio.sleep(random.uniform(0.5, 2.0))\n",
    "        \n",
    "        print(f\"\\n\\nMonitoring complete. Processed {operation_count} operations.\")\n",
    "    \n",
    "    def check_alerts(self) -> List[str]:\n",
    "        '''Check for alert conditions.'''\n",
    "        alerts = []\n",
    "        stats = self.metrics.get_current_stats()\n",
    "        \n",
    "        # Check response time\n",
    "        if \"response_time\" in stats:\n",
    "            if stats[\"response_time\"][\"current\"] > self.alert_thresholds[\"response_time\"]:\n",
    "                alerts.append(\n",
    "                    f\"High response time: {stats['response_time']['current']:.2f}s > {self.alert_thresholds['response_time']}s\"\n",
    "                )\n",
    "        \n",
    "        # Check error rate\n",
    "        if self.total_requests > 0:\n",
    "            error_rate = self.total_errors / self.total_requests\n",
    "            if error_rate > self.alert_thresholds[\"error_rate\"]:\n",
    "                alerts.append(\n",
    "                    f\"High error rate: {error_rate:.1%} > {self.alert_thresholds['error_rate']:.1%}\"\n",
    "                )\n",
    "        \n",
    "        # Check cost\n",
    "        if \"cost\" in stats:\n",
    "            uptime = (datetime.now() - self.start_time).total_seconds()\n",
    "            if uptime > 0:\n",
    "                cost_per_hour = (stats[\"cost\"][\"total\"] / uptime) * 3600\n",
    "                if cost_per_hour > self.alert_thresholds[\"cost_per_hour\"]:\n",
    "                    alerts.append(\n",
    "                        f\"High cost rate: ${cost_per_hour:.2f}/hour > ${self.alert_thresholds['cost_per_hour']}/hour\"\n",
    "                    )\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def export_report(self, format: str = \"json\") -> str:\n",
    "        '''Export dashboard report.'''\n",
    "        report_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"uptime_seconds\": (datetime.now() - self.start_time).total_seconds(),\n",
    "            \"total_requests\": self.total_requests,\n",
    "            \"total_errors\": self.total_errors,\n",
    "            \"active_sessions\": self.active_sessions,\n",
    "            \"metrics\": self.metrics.get_current_stats(),\n",
    "            \"alerts\": self.check_alerts()\n",
    "        }\n",
    "        \n",
    "        if format == \"json\":\n",
    "            return json.dumps(report_data, indent=2, default=str)\n",
    "        \n",
    "        elif format == \"text\":\n",
    "            report = \"AI Operations Report\\n\"\n",
    "            report += \"=\" * 40 + \"\\n\"\n",
    "            report += f\"Generated: {report_data['timestamp']}\\n\"\n",
    "            report += f\"Uptime: {report_data['uptime_seconds']:.0f} seconds\\n\"\n",
    "            report += f\"Total Requests: {report_data['total_requests']}\\n\"\n",
    "            report += f\"Total Errors: {report_data['total_errors']}\\n\"\n",
    "            report += f\"Active Sessions: {report_data['active_sessions']}\\n\"\n",
    "            \n",
    "            if report_data['alerts']:\n",
    "                report += \"\\nAlerts:\\n\"\n",
    "                for alert in report_data['alerts']:\n",
    "                    report += f\"  - {alert}\\n\"\n",
    "            \n",
    "            return report\n",
    "        \n",
    "        return \"Unsupported format\"\n",
    "\n",
    "# Test the dashboard\n",
    "async def test_dashboard():\n",
    "    dashboard = RealtimeDashboard()\n",
    "    \n",
    "    # Run monitoring for a short duration\n",
    "    await dashboard.monitor_operations(duration=15)\n",
    "    \n",
    "    # Export report\n",
    "    report = dashboard.export_report(\"text\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\\nFinal Report:\")\n",
    "    print(report)\n",
    "\n",
    "# Run dashboard test\n",
    "asyncio.run(test_dashboard())\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build a real-time AI operations dashboard with monitoring and alerts!\")\n",
    "print(\"The solution includes metrics collection, live updates, and reporting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've mastered streaming and real-time operations in LangChain. You can now:\n",
    "\n",
    "‚úÖ Implement token-by-token streaming from LLMs\n",
    "‚úÖ Build custom callbacks for monitoring\n",
    "‚úÖ Create async chains for concurrent processing\n",
    "‚úÖ Handle real-time WebSocket communication\n",
    "‚úÖ Build progress tracking systems\n",
    "‚úÖ Create real-time monitoring dashboards\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Streaming Improves UX**: Users see progress as it happens\n",
    "- **Async Enables Scale**: Process multiple requests concurrently\n",
    "- **Callbacks Provide Control**: Monitor and react to events\n",
    "- **Real-time Matters**: Immediate feedback enhances applications\n",
    "- **Monitoring is Essential**: Track performance and costs\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 11**: Learn about Advanced Patterns\n",
    "- **Practice**: Build real-time applications\n",
    "- **Experiment**: Try different streaming strategies\n",
    "- **Scale**: Implement WebSocket servers\n",
    "\n",
    "### Additional Challenges:\n",
    "1. Build a real-time collaborative text editor with AI assistance\n",
    "2. Create a live translation system with streaming\n",
    "3. Implement a real-time code completion service\n",
    "4. Build a streaming data analysis pipeline\n",
    "5. Create a multi-modal streaming system (text + audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}