{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a87c73",
   "metadata": {},
   "source": [
    "# 11 - Advanced Patterns: Expert-Level LangChain Techniques\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll explore advanced patterns and techniques for building sophisticated AI applications. You'll learn about routing, fallbacks, retries, conditional logic, and complex orchestration patterns.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Implement dynamic routing based on input\n",
    "- Build fallback chains for reliability\n",
    "- Create retry mechanisms with exponential backoff\n",
    "- Design conditional workflows\n",
    "- Implement map-reduce patterns\n",
    "- Build self-correcting systems\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of notebooks 01-10\n",
    "- Strong understanding of chains and agents\n",
    "- Familiarity with async programming\n",
    "\n",
    "## Back-and-Forth Teaching Pattern\n",
    "This notebook follows our pattern:\n",
    "1. **Instructor Activity**: Demonstrates a concept with complete examples\n",
    "2. **Learner Activity**: You apply the concept with guidance and hidden solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's install and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai tenacity pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, List, Optional, Union, Callable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.schema.runnable import (\n",
    "    Runnable, RunnablePassthrough, RunnableLambda, \n",
    "    RunnableBranch, RunnableParallel\n",
    ")\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import time\n",
    "import random\n",
    "from enum import Enum\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 1: Routing and Conditional Logic\n",
    "\n",
    "Let's explore advanced routing patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_routing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic routing based on input classification\n",
    "\n",
    "class QueryType(str, Enum):\n",
    "    TECHNICAL = \"technical\"\n",
    "    CREATIVE = \"creative\"\n",
    "    ANALYTICAL = \"analytical\"\n",
    "    GENERAL = \"general\"\n",
    "\n",
    "def classify_query(query: str) -> QueryType:\n",
    "    \"\"\"Classify the query type.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Simple keyword-based classification\n",
    "    technical_keywords = [\"code\", \"debug\", \"api\", \"function\", \"algorithm\", \"error\"]\n",
    "    creative_keywords = [\"story\", \"poem\", \"creative\", \"imagine\", \"design\", \"art\"]\n",
    "    analytical_keywords = [\"analyze\", \"compare\", \"evaluate\", \"assess\", \"data\", \"statistics\"]\n",
    "    \n",
    "    if any(keyword in query_lower for keyword in technical_keywords):\n",
    "        return QueryType.TECHNICAL\n",
    "    elif any(keyword in query_lower for keyword in creative_keywords):\n",
    "        return QueryType.CREATIVE\n",
    "    elif any(keyword in query_lower for keyword in analytical_keywords):\n",
    "        return QueryType.ANALYTICAL\n",
    "    else:\n",
    "        return QueryType.GENERAL\n",
    "\n",
    "# Create specialized chains for each type\n",
    "technical_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a technical expert. Provide detailed, accurate technical information.\n",
    "    Use code examples when appropriate.\n",
    "    \n",
    "    Question: {query}\n",
    "    Technical Answer:\"\"\"\n",
    ")\n",
    "\n",
    "creative_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a creative writer. Be imaginative and engaging.\n",
    "    Use vivid descriptions and creative language.\n",
    "    \n",
    "    Request: {query}\n",
    "    Creative Response:\"\"\"\n",
    ")\n",
    "\n",
    "analytical_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a data analyst. Provide structured, logical analysis.\n",
    "    Break down complex topics systematically.\n",
    "    \n",
    "    Topic: {query}\n",
    "    Analysis:\"\"\"\n",
    ")\n",
    "\n",
    "general_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Provide a clear, concise answer.\n",
    "    \n",
    "    Question: {query}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Create router using RunnableBranch\n",
    "def create_router():\n",
    "    llm = ChatOpenAI(temperature=0.7)\n",
    "    \n",
    "    # Define routing logic\n",
    "    def route(inputs: Dict) -> Runnable:\n",
    "        query_type = classify_query(inputs[\"query\"])\n",
    "        \n",
    "        if query_type == QueryType.TECHNICAL:\n",
    "            return technical_prompt | llm\n",
    "        elif query_type == QueryType.CREATIVE:\n",
    "            return creative_prompt | llm\n",
    "        elif query_type == QueryType.ANALYTICAL:\n",
    "            return analytical_prompt | llm\n",
    "        else:\n",
    "            return general_prompt | llm\n",
    "    \n",
    "    # Create router with RunnableLambda\n",
    "    router = RunnableLambda(route)\n",
    "    \n",
    "    return router\n",
    "\n",
    "# Test the router\n",
    "router = create_router()\n",
    "\n",
    "test_queries = [\n",
    "    \"Write a function to sort a list in Python\",\n",
    "    \"Create a poem about the ocean\",\n",
    "    \"Analyze the pros and cons of remote work\",\n",
    "    \"What's the capital of France?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Type: {query_type}\")\n",
    "    \n",
    "    # Route and get response\n",
    "    response = router.invoke({\"query\": query})\n",
    "    print(f\"Response: {response.content[:200]}...\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional_chains",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional chains with complex branching\n",
    "\n",
    "class ConditionalChain:\n",
    "    \"\"\"Chain with conditional execution based on intermediate results.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.setup_chains()\n",
    "    \n",
    "    def setup_chains(self):\n",
    "        # Initial analysis chain\n",
    "        self.analysis_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                \"\"\"Analyze this request and determine its complexity.\n",
    "                Rate from 1-10 and explain why.\n",
    "                \n",
    "                Request: {input}\n",
    "                \n",
    "                Return JSON: {{\"complexity\": <number>, \"reason\": \"<explanation>\"}}\"\"\"\n",
    "            )\n",
    "            | self.llm\n",
    "        )\n",
    "        \n",
    "        # Simple response chain\n",
    "        self.simple_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                \"\"\"Provide a simple, straightforward answer.\n",
    "                Request: {input}\n",
    "                Simple Answer:\"\"\"\n",
    "            )\n",
    "            | self.llm\n",
    "        )\n",
    "        \n",
    "        # Detailed response chain\n",
    "        self.detailed_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                \"\"\"Provide a comprehensive, detailed response with examples.\n",
    "                Request: {input}\n",
    "                Detailed Answer:\"\"\"\n",
    "            )\n",
    "            | self.llm\n",
    "        )\n",
    "        \n",
    "        # Expert response chain\n",
    "        self.expert_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                \"\"\"Provide an expert-level analysis with advanced concepts,\n",
    "                edge cases, and best practices.\n",
    "                Request: {input}\n",
    "                Expert Analysis:\"\"\"\n",
    "            )\n",
    "            | self.llm\n",
    "        )\n",
    "    \n",
    "    def process(self, user_input: str) -> Dict:\n",
    "        \"\"\"Process input with conditional logic.\"\"\"\n",
    "        # Step 1: Analyze complexity\n",
    "        analysis = self.analysis_chain.invoke({\"input\": user_input})\n",
    "        \n",
    "        # Parse complexity\n",
    "        try:\n",
    "            import json\n",
    "            analysis_data = json.loads(analysis.content)\n",
    "            complexity = analysis_data.get(\"complexity\", 5)\n",
    "        except:\n",
    "            complexity = 5  # Default to medium\n",
    "        \n",
    "        # Step 2: Route based on complexity\n",
    "        if complexity <= 3:\n",
    "            response_type = \"simple\"\n",
    "            response = self.simple_chain.invoke({\"input\": user_input})\n",
    "        elif complexity <= 7:\n",
    "            response_type = \"detailed\"\n",
    "            response = self.detailed_chain.invoke({\"input\": user_input})\n",
    "        else:\n",
    "            response_type = \"expert\"\n",
    "            response = self.expert_chain.invoke({\"input\": user_input})\n",
    "        \n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"complexity\": complexity,\n",
    "            \"response_type\": response_type,\n",
    "            \"response\": response.content\n",
    "        }\n",
    "\n",
    "# Test conditional chain\n",
    "conditional = ConditionalChain()\n",
    "\n",
    "test_inputs = [\n",
    "    \"What is 2+2?\",\n",
    "    \"Explain how neural networks work\",\n",
    "    \"Design a distributed system for real-time data processing with fault tolerance\"\n",
    "]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    result = conditional.process(input_text)\n",
    "    print(f\"\\nInput: {result['input']}\")\n",
    "    print(f\"Complexity: {result['complexity']}/10\")\n",
    "    print(f\"Response Type: {result['response_type']}\")\n",
    "    print(f\"Response: {result['response'][:200]}...\\n\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 1: Build an Intelligent Router\n",
    "\n",
    "Create an advanced routing system that handles multiple dimensions.\n",
    "\n",
    "**Task**: Build a router that considers:\n",
    "1. Query type (technical, business, creative)\n",
    "2. User expertise level (beginner, intermediate, expert)\n",
    "3. Response format preference (brief, detailed, structured)\n",
    "4. Language complexity adjustment\n",
    "5. Fallback routing for ambiguous queries\n",
    "\n",
    "Requirements:\n",
    "- Multi-dimensional routing logic\n",
    "- Adaptive response generation\n",
    "- Graceful handling of edge cases\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_1_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your intelligent router\n",
    "\n",
    "class UserContext(BaseModel):\n",
    "    expertise_level: str = Field(default=\"intermediate\")\n",
    "    format_preference: str = Field(default=\"detailed\")\n",
    "    # TODO: Add more context fields\n",
    "\n",
    "class IntelligentRouter:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize routing components\n",
    "        pass\n",
    "    \n",
    "    def analyze_query(self, query: str) -> Dict:\n",
    "        \"\"\"Analyze query across multiple dimensions.\"\"\"\n",
    "        # TODO: Implement multi-dimensional analysis\n",
    "        pass\n",
    "    \n",
    "    def select_chain(self, analysis: Dict, context: UserContext) -> Runnable:\n",
    "        \"\"\"Select appropriate chain based on analysis and context.\"\"\"\n",
    "        # TODO: Implement selection logic\n",
    "        pass\n",
    "    \n",
    "    def route(self, query: str, context: UserContext) -> str:\n",
    "        \"\"\"Route query to appropriate handler.\"\"\"\n",
    "        # TODO: Implement complete routing flow\n",
    "        pass\n",
    "\n",
    "# TODO: Test your router\n",
    "# Test with various query types and user contexts\n",
    "# Verify routing decisions\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_1_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "class ExpertiseLevel(str, Enum):\n",
    "    BEGINNER = \"beginner\"\n",
    "    INTERMEDIATE = \"intermediate\"\n",
    "    EXPERT = \"expert\"\n",
    "\n",
    "class FormatPreference(str, Enum):\n",
    "    BRIEF = \"brief\"\n",
    "    DETAILED = \"detailed\"\n",
    "    STRUCTURED = \"structured\"\n",
    "\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    expertise_level: ExpertiseLevel = ExpertiseLevel.INTERMEDIATE\n",
    "    format_preference: FormatPreference = FormatPreference.DETAILED\n",
    "    language: str = \"en\"\n",
    "    max_tokens: int = 500\n",
    "\n",
    "class IntelligentRouter:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.analysis_cache = {}  # Cache analysis results\n",
    "        self._setup_chains()\n",
    "    \n",
    "    def _setup_chains(self):\n",
    "        '''Setup all chain variants.'''\n",
    "        # Create chain templates for different combinations\n",
    "        self.chain_templates = {}\n",
    "        \n",
    "        # Technical chains\n",
    "        for level in ExpertiseLevel:\n",
    "            for format_pref in FormatPreference:\n",
    "                key = f\"technical_{level.value}_{format_pref.value}\"\n",
    "                \n",
    "                if level == ExpertiseLevel.BEGINNER:\n",
    "                    complexity = \"Use simple terms, avoid jargon, provide analogies.\"\n",
    "                elif level == ExpertiseLevel.INTERMEDIATE:\n",
    "                    complexity = \"Use standard technical terms with brief explanations.\"\n",
    "                else:\n",
    "                    complexity = \"Use advanced concepts, assume technical knowledge.\"\n",
    "                \n",
    "                if format_pref == FormatPreference.BRIEF:\n",
    "                    format_inst = \"Provide a concise answer in 2-3 sentences.\"\n",
    "                elif format_pref == FormatPreference.DETAILED:\n",
    "                    format_inst = \"Provide a comprehensive answer with examples.\"\n",
    "                else:\n",
    "                    format_inst = \"Structure your answer with clear sections and bullet points.\"\n",
    "                \n",
    "                template = f'''You are a technical assistant.\n",
    "                {complexity}\n",
    "                {format_inst}\n",
    "                \n",
    "                Question: {{query}}\n",
    "                \n",
    "                Answer:'''\n",
    "                \n",
    "                self.chain_templates[key] = ChatPromptTemplate.from_template(template) | self.llm\n",
    "        \n",
    "        # Business chains (similar structure)\n",
    "        for level in ExpertiseLevel:\n",
    "            key = f\"business_{level.value}_detailed\"\n",
    "            template = f'''You are a business consultant.\n",
    "            Expertise level: {level.value}\n",
    "            Provide practical business insights.\n",
    "            \n",
    "            Question: {{query}}\n",
    "            \n",
    "            Business Perspective:'''\n",
    "            \n",
    "            self.chain_templates[key] = ChatPromptTemplate.from_template(template) | self.llm\n",
    "        \n",
    "        # Creative chains\n",
    "        self.chain_templates[\"creative_any_any\"] = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                '''You are a creative assistant.\n",
    "                Be imaginative and engaging.\n",
    "                \n",
    "                Request: {query}\n",
    "                \n",
    "                Creative Response:'''\n",
    "            ) | self.llm\n",
    "        )\n",
    "        \n",
    "        # Fallback chain\n",
    "        self.fallback_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                '''Provide a helpful response to this query.\n",
    "                Query: {query}\n",
    "                Response:'''\n",
    "            ) | self.llm\n",
    "        )\n",
    "    \n",
    "    def analyze_query(self, query: str) -> Dict:\n",
    "        '''Analyze query across multiple dimensions.'''\n",
    "        # Check cache\n",
    "        cache_key = hash(query)\n",
    "        if cache_key in self.analysis_cache:\n",
    "            return self.analysis_cache[cache_key]\n",
    "        \n",
    "        analysis_prompt = ChatPromptTemplate.from_template(\n",
    "            '''Analyze this query and return JSON:\n",
    "            {{\n",
    "                \"domain\": \"technical/business/creative/general\",\n",
    "                \"complexity\": 1-10,\n",
    "                \"requires_code\": true/false,\n",
    "                \"requires_examples\": true/false,\n",
    "                \"estimated_response_length\": \"short/medium/long\"\n",
    "            }}\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            JSON:'''\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = (analysis_prompt | self.llm).invoke({\"query\": query})\n",
    "            analysis = json.loads(response.content)\n",
    "        except:\n",
    "            # Fallback analysis\n",
    "            analysis = self._simple_analysis(query)\n",
    "        \n",
    "        # Cache result\n",
    "        self.analysis_cache[cache_key] = analysis\n",
    "        return analysis\n",
    "    \n",
    "    def _simple_analysis(self, query: str) -> Dict:\n",
    "        '''Simple keyword-based analysis as fallback.'''\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        domain = \"general\"\n",
    "        if any(word in query_lower for word in [\"code\", \"function\", \"api\", \"debug\"]):\n",
    "            domain = \"technical\"\n",
    "        elif any(word in query_lower for word in [\"business\", \"market\", \"strategy\", \"roi\"]):\n",
    "            domain = \"business\"\n",
    "        elif any(word in query_lower for word in [\"create\", \"imagine\", \"story\", \"design\"]):\n",
    "            domain = \"creative\"\n",
    "        \n",
    "        return {\n",
    "            \"domain\": domain,\n",
    "            \"complexity\": len(query.split()) // 5 + 3,  # Rough estimate\n",
    "            \"requires_code\": \"code\" in query_lower,\n",
    "            \"requires_examples\": \"example\" in query_lower or \"how\" in query_lower,\n",
    "            \"estimated_response_length\": \"medium\"\n",
    "        }\n",
    "    \n",
    "    def select_chain(self, analysis: Dict, context: UserContext) -> Runnable:\n",
    "        '''Select appropriate chain based on analysis and context.'''\n",
    "        domain = analysis.get(\"domain\", \"general\")\n",
    "        complexity = analysis.get(\"complexity\", 5)\n",
    "        \n",
    "        # Adjust expertise level based on complexity\n",
    "        if complexity > 7 and context.expertise_level == ExpertiseLevel.BEGINNER:\n",
    "            # Upgrade to intermediate for complex queries\n",
    "            effective_level = ExpertiseLevel.INTERMEDIATE\n",
    "        else:\n",
    "            effective_level = context.expertise_level\n",
    "        \n",
    "        # Build chain key\n",
    "        chain_key = f\"{domain}_{effective_level.value}_{context.format_preference.value}\"\n",
    "        \n",
    "        # Get chain or fallback\n",
    "        if chain_key in self.chain_templates:\n",
    "            return self.chain_templates[chain_key]\n",
    "        \n",
    "        # Try without format preference\n",
    "        chain_key_simple = f\"{domain}_{effective_level.value}_detailed\"\n",
    "        if chain_key_simple in self.chain_templates:\n",
    "            return self.chain_templates[chain_key_simple]\n",
    "        \n",
    "        # Ultimate fallback\n",
    "        return self.fallback_chain\n",
    "    \n",
    "    def route(self, query: str, context: UserContext = None) -> Dict:\n",
    "        '''Route query to appropriate handler.'''\n",
    "        if context is None:\n",
    "            context = UserContext()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Analyze query\n",
    "        analysis = self.analyze_query(query)\n",
    "        \n",
    "        # Select and execute chain\n",
    "        chain = self.select_chain(analysis, context)\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": query})\n",
    "            response_text = response.content\n",
    "        except Exception as e:\n",
    "            # Fallback on error\n",
    "            response_text = self.fallback_chain.invoke({\"query\": query}).content\n",
    "            analysis[\"fallback_used\"] = True\n",
    "        \n",
    "        # Post-process based on context\n",
    "        if context.max_tokens and len(response_text) > context.max_tokens * 4:  # Rough char estimate\n",
    "            response_text = response_text[:context.max_tokens * 4] + \"...\"\n",
    "        \n",
    "        routing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response_text,\n",
    "            \"analysis\": analysis,\n",
    "            \"context\": {\n",
    "                \"expertise\": context.expertise_level.value,\n",
    "                \"format\": context.format_preference.value\n",
    "            },\n",
    "            \"routing_time\": routing_time,\n",
    "            \"chain_used\": chain.__class__.__name__\n",
    "        }\n",
    "\n",
    "# Test the intelligent router\n",
    "print(\"Intelligent Routing System Test\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "router = IntelligentRouter()\n",
    "\n",
    "# Test scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"query\": \"How do I write a for loop?\",\n",
    "        \"context\": UserContext(\n",
    "            expertise_level=ExpertiseLevel.BEGINNER,\n",
    "            format_preference=FormatPreference.BRIEF\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain the architecture of a microservices system with examples\",\n",
    "        \"context\": UserContext(\n",
    "            expertise_level=ExpertiseLevel.EXPERT,\n",
    "            format_preference=FormatPreference.STRUCTURED\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the ROI of digital transformation?\",\n",
    "        \"context\": UserContext(\n",
    "            expertise_level=ExpertiseLevel.INTERMEDIATE,\n",
    "            format_preference=FormatPreference.DETAILED\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Write a creative story about a robot\",\n",
    "        \"context\": UserContext(\n",
    "            expertise_level=ExpertiseLevel.INTERMEDIATE,\n",
    "            format_preference=FormatPreference.BRIEF\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    result = router.route(scenario[\"query\"], scenario[\"context\"])\n",
    "    \n",
    "    print(f\"\\nQuery: {result['query']}\")\n",
    "    print(f\"Analysis: Domain={result['analysis']['domain']}, Complexity={result['analysis']['complexity']}\")\n",
    "    print(f\"Context: {result['context']}\")\n",
    "    print(f\"Routing Time: {result['routing_time']:.3f}s\")\n",
    "    print(f\"Response Preview: {result['response'][:150]}...\")\n",
    "    print(\"-\" * 50)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build an intelligent multi-dimensional router!\")\n",
    "print(\"The solution includes query analysis, context awareness, and fallback handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 2: Retry, Fallback, and Self-Correction\n",
    "\n",
    "Let's implement robust error handling and self-correction patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retry_pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced retry with exponential backoff\n",
    "\n",
    "class RetryableChain:\n",
    "    \"\"\"Chain with sophisticated retry logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3):\n",
    "        self.max_retries = max_retries\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.retry_history = []\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=10)\n",
    "    )\n",
    "    def _execute_with_retry(self, prompt: str) -> str:\n",
    "        \"\"\"Execute with exponential backoff retry.\"\"\"\n",
    "        # Simulate occasional failures\n",
    "        if random.random() < 0.3:  # 30% failure rate for demo\n",
    "            raise Exception(\"Simulated API error\")\n",
    "        \n",
    "        response = self.llm.predict(prompt)\n",
    "        return response\n",
    "    \n",
    "    def execute(self, prompt: str) -> Dict:\n",
    "        \"\"\"Execute with comprehensive error handling.\"\"\"\n",
    "        attempt = 0\n",
    "        errors = []\n",
    "        \n",
    "        while attempt < self.max_retries:\n",
    "            try:\n",
    "                attempt += 1\n",
    "                print(f\"Attempt {attempt}/{self.max_retries}...\")\n",
    "                \n",
    "                # Add retry context to prompt if not first attempt\n",
    "                if attempt > 1:\n",
    "                    retry_context = f\"\\n\\n[Previous attempt failed. Please try again carefully.]\"\n",
    "                    enhanced_prompt = prompt + retry_context\n",
    "                else:\n",
    "                    enhanced_prompt = prompt\n",
    "                \n",
    "                result = self._execute_with_retry(enhanced_prompt)\n",
    "                \n",
    "                self.retry_history.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"success\": True,\n",
    "                    \"result\": result\n",
    "                })\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"result\": result,\n",
    "                    \"attempts\": attempt,\n",
    "                    \"errors\": errors\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": time.time()\n",
    "                })\n",
    "                \n",
    "                self.retry_history.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                \n",
    "                if attempt < self.max_retries:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"  Error: {e}. Waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "        \n",
    "        # All retries failed\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"result\": None,\n",
    "            \"attempts\": attempt,\n",
    "            \"errors\": errors\n",
    "        }\n",
    "\n",
    "# Test retry chain\n",
    "retry_chain = RetryableChain(max_retries=3)\n",
    "\n",
    "result = retry_chain.execute(\"What is the capital of Japan?\")\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Attempts: {result['attempts']}\")\n",
    "if result['success']:\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "else:\n",
    "    print(f\"Errors: {result['errors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self_correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-correcting chain with validation\n",
    "\n",
    "class SelfCorrectingChain:\n",
    "    \"\"\"Chain that validates and corrects its own output.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.validator_llm = ChatOpenAI(temperature=0)  # More deterministic for validation\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate initial response.\"\"\"\n",
    "        return self.llm.predict(prompt)\n",
    "    \n",
    "    def validate(self, prompt: str, response: str) -> Dict:\n",
    "        \"\"\"Validate the response.\"\"\"\n",
    "        validation_prompt = f\"\"\"Validate this response for accuracy and completeness.\n",
    "        \n",
    "        Original Question: {prompt}\n",
    "        Response: {response}\n",
    "        \n",
    "        Check for:\n",
    "        1. Factual accuracy\n",
    "        2. Completeness\n",
    "        3. Clarity\n",
    "        4. Relevance\n",
    "        \n",
    "        Return JSON:\n",
    "        {{\n",
    "            \"is_valid\": true/false,\n",
    "            \"issues\": [list of issues if any],\n",
    "            \"suggestions\": [list of improvements]\n",
    "        }}\n",
    "        \n",
    "        JSON:\"\"\"\n",
    "        \n",
    "        validation_result = self.validator_llm.predict(validation_prompt)\n",
    "        \n",
    "        try:\n",
    "            return json.loads(validation_result)\n",
    "        except:\n",
    "            # Default to valid if parsing fails\n",
    "            return {\"is_valid\": True, \"issues\": [], \"suggestions\": []}\n",
    "    \n",
    "    def correct(self, prompt: str, response: str, validation: Dict) -> str:\n",
    "        \"\"\"Correct the response based on validation.\"\"\"\n",
    "        correction_prompt = f\"\"\"Improve this response based on the feedback.\n",
    "        \n",
    "        Original Question: {prompt}\n",
    "        Original Response: {response}\n",
    "        \n",
    "        Issues to fix: {validation['issues']}\n",
    "        Suggestions: {validation['suggestions']}\n",
    "        \n",
    "        Improved Response:\"\"\"\n",
    "        \n",
    "        return self.llm.predict(correction_prompt)\n",
    "    \n",
    "    def execute_with_correction(self, prompt: str, max_corrections: int = 2) -> Dict:\n",
    "        \"\"\"Execute with self-correction loop.\"\"\"\n",
    "        correction_count = 0\n",
    "        history = []\n",
    "        \n",
    "        # Initial generation\n",
    "        response = self.generate(prompt)\n",
    "        history.append({\"version\": 0, \"response\": response})\n",
    "        \n",
    "        while correction_count < max_corrections:\n",
    "            # Validate\n",
    "            validation = self.validate(prompt, response)\n",
    "            \n",
    "            if validation[\"is_valid\"] and not validation[\"issues\"]:\n",
    "                # Response is valid\n",
    "                return {\n",
    "                    \"final_response\": response,\n",
    "                    \"corrections_made\": correction_count,\n",
    "                    \"history\": history,\n",
    "                    \"validation\": validation\n",
    "                }\n",
    "            \n",
    "            # Need correction\n",
    "            correction_count += 1\n",
    "            print(f\"  Correction {correction_count}: Fixing issues {validation['issues']}\")\n",
    "            \n",
    "            response = self.correct(prompt, response, validation)\n",
    "            history.append({\n",
    "                \"version\": correction_count,\n",
    "                \"response\": response,\n",
    "                \"fixed_issues\": validation[\"issues\"]\n",
    "            })\n",
    "        \n",
    "        # Max corrections reached\n",
    "        return {\n",
    "            \"final_response\": response,\n",
    "            \"corrections_made\": correction_count,\n",
    "            \"history\": history,\n",
    "            \"validation\": validation\n",
    "        }\n",
    "\n",
    "# Test self-correcting chain\n",
    "self_correcting = SelfCorrectingChain()\n",
    "\n",
    "test_prompts = [\n",
    "    \"Write a Python function to calculate factorial\",\n",
    "    \"Explain the water cycle with all major steps\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"Processing with self-correction...\")\n",
    "    \n",
    "    result = self_correcting.execute_with_correction(prompt)\n",
    "    \n",
    "    print(f\"\\nCorrections made: {result['corrections_made']}\")\n",
    "    print(f\"Final Response: {result['final_response'][:200]}...\")\n",
    "    \n",
    "    if result['corrections_made'] > 0:\n",
    "        print(\"\\nCorrection History:\")\n",
    "        for item in result['history'][1:]:  # Skip original\n",
    "            print(f\"  Version {item['version']}: Fixed {item.get('fixed_issues', [])}\")\n",
    "    \n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 2: Build a Resilient Chain System\n",
    "\n",
    "Create a comprehensive resilient system with multiple fallback strategies.\n",
    "\n",
    "**Task**: Build a system that:\n",
    "1. Has primary, secondary, and tertiary processing chains\n",
    "2. Implements intelligent fallback selection\n",
    "3. Includes self-healing capabilities\n",
    "4. Tracks and learns from failures\n",
    "5. Provides graceful degradation\n",
    "\n",
    "Requirements:\n",
    "- Multiple fallback strategies\n",
    "- Failure pattern recognition\n",
    "- Adaptive retry logic\n",
    "- Performance metrics tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_2_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your resilient chain system\n",
    "\n",
    "class ResilientSystem:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize primary, secondary, tertiary chains\n",
    "        # TODO: Setup failure tracking\n",
    "        pass\n",
    "    \n",
    "    def execute_with_fallback(self, prompt: str) -> Dict:\n",
    "        \"\"\"Execute with intelligent fallback.\"\"\"\n",
    "        # TODO: Try primary chain\n",
    "        # TODO: On failure, select appropriate fallback\n",
    "        # TODO: Track failure patterns\n",
    "        pass\n",
    "    \n",
    "    def learn_from_failure(self, failure_info: Dict):\n",
    "        \"\"\"Learn from failures to improve future routing.\"\"\"\n",
    "        # TODO: Analyze failure patterns\n",
    "        # TODO: Adjust routing strategy\n",
    "        pass\n",
    "    \n",
    "    def get_health_status(self) -> Dict:\n",
    "        \"\"\"Get system health and performance metrics.\"\"\"\n",
    "        # TODO: Return health metrics\n",
    "        pass\n",
    "\n",
    "# TODO: Test resilient system\n",
    "# Simulate various failure scenarios\n",
    "# Verify fallback behavior\n",
    "# Check learning from failures\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_2_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class ChainHealth:\n",
    "    '''Track health metrics for a chain.'''\n",
    "    def __init__(self, window_size: int = 100):\n",
    "        self.success_count = 0\n",
    "        self.failure_count = 0\n",
    "        self.recent_latencies = deque(maxlen=window_size)\n",
    "        self.recent_errors = deque(maxlen=window_size)\n",
    "        self.last_failure_time = None\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        total = self.success_count + self.failure_count\n",
    "        return self.success_count / total if total > 0 else 1.0\n",
    "    \n",
    "    @property\n",
    "    def avg_latency(self) -> float:\n",
    "        return sum(self.recent_latencies) / len(self.recent_latencies) if self.recent_latencies else 0\n",
    "    \n",
    "    @property\n",
    "    def health_score(self) -> float:\n",
    "        '''Composite health score 0-1.'''\n",
    "        # Weight factors\n",
    "        success_weight = 0.5\n",
    "        latency_weight = 0.3\n",
    "        recency_weight = 0.2\n",
    "        \n",
    "        # Success component\n",
    "        success_score = self.success_rate\n",
    "        \n",
    "        # Latency component (normalize to 0-1, assuming 5s is bad)\n",
    "        latency_score = max(0, 1 - (self.avg_latency / 5)) if self.recent_latencies else 1\n",
    "        \n",
    "        # Recency component (penalize recent failures)\n",
    "        if self.last_failure_time:\n",
    "            time_since_failure = (datetime.now() - self.last_failure_time).seconds\n",
    "            recency_score = min(1, time_since_failure / 300)  # 5 min to full recovery\n",
    "        else:\n",
    "            recency_score = 1\n",
    "        \n",
    "        return (\n",
    "            success_weight * success_score +\n",
    "            latency_weight * latency_score +\n",
    "            recency_weight * recency_score\n",
    "        )\n",
    "\n",
    "class ResilientSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize chains with different models/configs\n",
    "        self.primary_chain = self._create_chain(\"gpt-3.5-turbo\", 0.7)\n",
    "        self.secondary_chain = self._create_chain(\"gpt-3.5-turbo\", 0.5)  # Lower temp\n",
    "        self.tertiary_chain = self._create_chain(\"gpt-3.5-turbo\", 0.3)  # Even lower\n",
    "        \n",
    "        # Health tracking\n",
    "        self.chain_health = {\n",
    "            \"primary\": ChainHealth(),\n",
    "            \"secondary\": ChainHealth(),\n",
    "            \"tertiary\": ChainHealth()\n",
    "        }\n",
    "        \n",
    "        # Failure patterns\n",
    "        self.failure_patterns = defaultdict(list)\n",
    "        self.adaptive_routing = True\n",
    "        \n",
    "        # Circuit breaker settings\n",
    "        self.circuit_breaker = {\n",
    "            \"primary\": \"closed\",  # closed, open, half_open\n",
    "            \"secondary\": \"closed\",\n",
    "            \"tertiary\": \"closed\"\n",
    "        }\n",
    "        self.circuit_breaker_threshold = 0.5  # Open if success rate < 50%\n",
    "        self.circuit_breaker_timeout = 30  # Seconds before trying half_open\n",
    "    \n",
    "    def _create_chain(self, model: str, temperature: float):\n",
    "        '''Create a chain with specific configuration.'''\n",
    "        llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "        prompt = ChatPromptTemplate.from_template(\"Answer this: {prompt}\")\n",
    "        return prompt | llm\n",
    "    \n",
    "    def _check_circuit_breaker(self, chain_name: str) -> bool:\n",
    "        '''Check if circuit breaker allows execution.'''\n",
    "        state = self.circuit_breaker[chain_name]\n",
    "        health = self.chain_health[chain_name]\n",
    "        \n",
    "        if state == \"closed\":\n",
    "            # Check if should open\n",
    "            if health.success_rate < self.circuit_breaker_threshold and health.failure_count > 5:\n",
    "                self.circuit_breaker[chain_name] = \"open\"\n",
    "                print(f\"  🔴 Circuit breaker OPENED for {chain_name}\")\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        elif state == \"open\":\n",
    "            # Check if should try half_open\n",
    "            if health.last_failure_time:\n",
    "                time_since = (datetime.now() - health.last_failure_time).seconds\n",
    "                if time_since > self.circuit_breaker_timeout:\n",
    "                    self.circuit_breaker[chain_name] = \"half_open\"\n",
    "                    print(f\"  🟡 Circuit breaker HALF-OPEN for {chain_name}\")\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        else:  # half_open\n",
    "            return True\n",
    "    \n",
    "    def _execute_chain(self, chain_name: str, chain, prompt: str) -> tuple:\n",
    "        '''Execute a chain with health tracking.'''\n",
    "        health = self.chain_health[chain_name]\n",
    "        \n",
    "        # Check circuit breaker\n",
    "        if not self._check_circuit_breaker(chain_name):\n",
    "            return False, \"Circuit breaker open\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Simulate occasional failures for demo\n",
    "            if chain_name == \"primary\" and random.random() < 0.4:  # 40% failure rate\n",
    "                raise Exception(\"Simulated primary chain failure\")\n",
    "            elif chain_name == \"secondary\" and random.random() < 0.2:  # 20% failure rate\n",
    "                raise Exception(\"Simulated secondary chain failure\")\n",
    "            \n",
    "            result = chain.invoke({\"prompt\": prompt})\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Update health\n",
    "            health.success_count += 1\n",
    "            health.recent_latencies.append(latency)\n",
    "            \n",
    "            # Close circuit breaker if half_open\n",
    "            if self.circuit_breaker[chain_name] == \"half_open\":\n",
    "                self.circuit_breaker[chain_name] = \"closed\"\n",
    "                print(f\"  🟢 Circuit breaker CLOSED for {chain_name}\")\n",
    "            \n",
    "            return True, result.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Update health\n",
    "            health.failure_count += 1\n",
    "            health.recent_latencies.append(latency)\n",
    "            health.recent_errors.append(str(e))\n",
    "            health.last_failure_time = datetime.now()\n",
    "            \n",
    "            # Open circuit breaker if half_open\n",
    "            if self.circuit_breaker[chain_name] == \"half_open\":\n",
    "                self.circuit_breaker[chain_name] = \"open\"\n",
    "                print(f\"  🔴 Circuit breaker RE-OPENED for {chain_name}\")\n",
    "            \n",
    "            return False, str(e)\n",
    "    \n",
    "    def execute_with_fallback(self, prompt: str) -> Dict:\n",
    "        '''Execute with intelligent fallback.'''\n",
    "        execution_log = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine chain order based on health scores\n",
    "        if self.adaptive_routing:\n",
    "            chain_order = self._get_adaptive_order()\n",
    "        else:\n",
    "            chain_order = [(\"primary\", self.primary_chain),\n",
    "                         (\"secondary\", self.secondary_chain),\n",
    "                         (\"tertiary\", self.tertiary_chain)]\n",
    "        \n",
    "        # Try chains in order\n",
    "        for chain_name, chain in chain_order:\n",
    "            print(f\"\\n  Trying {chain_name} chain...\")\n",
    "            success, result = self._execute_chain(chain_name, chain, prompt)\n",
    "            \n",
    "            execution_log.append({\n",
    "                \"chain\": chain_name,\n",
    "                \"success\": success,\n",
    "                \"result\": result if success else None,\n",
    "                \"error\": result if not success else None\n",
    "            })\n",
    "            \n",
    "            if success:\n",
    "                total_time = time.time() - start_time\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"result\": result,\n",
    "                    \"chain_used\": chain_name,\n",
    "                    \"execution_log\": execution_log,\n",
    "                    \"total_time\": total_time\n",
    "                }\n",
    "        \n",
    "        # All chains failed\n",
    "        self._record_failure_pattern(prompt, execution_log)\n",
    "        \n",
    "        # Last resort: return cached or default response\n",
    "        fallback_response = self._get_fallback_response(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"result\": fallback_response,\n",
    "            \"chain_used\": \"fallback\",\n",
    "            \"execution_log\": execution_log,\n",
    "            \"total_time\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    def _get_adaptive_order(self) -> list:\n",
    "        '''Get chain order based on health scores.'''\n",
    "        chains_with_scores = [\n",
    "            (\"primary\", self.primary_chain, self.chain_health[\"primary\"].health_score),\n",
    "            (\"secondary\", self.secondary_chain, self.chain_health[\"secondary\"].health_score),\n",
    "            (\"tertiary\", self.tertiary_chain, self.chain_health[\"tertiary\"].health_score)\n",
    "        ]\n",
    "        \n",
    "        # Sort by health score (best first)\n",
    "        chains_with_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        return [(name, chain) for name, chain, _ in chains_with_scores]\n",
    "    \n",
    "    def _record_failure_pattern(self, prompt: str, execution_log: list):\n",
    "        '''Record failure pattern for learning.'''\n",
    "        pattern = {\n",
    "            \"prompt_length\": len(prompt),\n",
    "            \"prompt_type\": self._classify_prompt(prompt),\n",
    "            \"failures\": [log[\"chain\"] for log in execution_log if not log[\"success\"]],\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        self.failure_patterns[pattern[\"prompt_type\"]].append(pattern)\n",
    "    \n",
    "    def _classify_prompt(self, prompt: str) -> str:\n",
    "        '''Simple prompt classification.'''\n",
    "        if \"?\" in prompt:\n",
    "            return \"question\"\n",
    "        elif any(word in prompt.lower() for word in [\"create\", \"write\", \"generate\"]):\n",
    "            return \"generation\"\n",
    "        elif any(word in prompt.lower() for word in [\"analyze\", \"explain\", \"describe\"]):\n",
    "            return \"analysis\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "    \n",
    "    def _get_fallback_response(self, prompt: str) -> str:\n",
    "        '''Get fallback response when all chains fail.'''\n",
    "        prompt_type = self._classify_prompt(prompt)\n",
    "        \n",
    "        fallback_responses = {\n",
    "            \"question\": \"I'm unable to answer your question at the moment. Please try again later.\",\n",
    "            \"generation\": \"I'm unable to generate content right now. Please try again.\",\n",
    "            \"analysis\": \"I'm unable to perform the analysis currently. Please retry.\",\n",
    "            \"other\": \"I'm experiencing technical difficulties. Please try again later.\"\n",
    "        }\n",
    "        \n",
    "        return fallback_responses.get(prompt_type, fallback_responses[\"other\"])\n",
    "    \n",
    "    def learn_from_failure(self, prompt_type: str = None):\n",
    "        '''Analyze failure patterns and adjust strategy.'''\n",
    "        if prompt_type and prompt_type in self.failure_patterns:\n",
    "            patterns = self.failure_patterns[prompt_type]\n",
    "            \n",
    "            if len(patterns) > 5:  # Need enough data\n",
    "                # Find most common failing chain\n",
    "                failing_chains = []\n",
    "                for pattern in patterns[-10:]:  # Last 10 failures\n",
    "                    failing_chains.extend(pattern[\"failures\"])\n",
    "                \n",
    "                from collections import Counter\n",
    "                chain_failures = Counter(failing_chains)\n",
    "                \n",
    "                # Adjust routing if clear pattern\n",
    "                most_failed = chain_failures.most_common(1)[0] if chain_failures else None\n",
    "                if most_failed and most_failed[1] > 5:\n",
    "                    print(f\"  📊 Learning: {most_failed[0]} chain frequently fails for {prompt_type}\")\n",
    "                    # Could adjust routing strategy here\n",
    "    \n",
    "    def get_health_status(self) -> Dict:\n",
    "        '''Get comprehensive health status.'''\n",
    "        status = {\n",
    "            \"chains\": {},\n",
    "            \"circuit_breakers\": self.circuit_breaker.copy(),\n",
    "            \"failure_patterns\": {k: len(v) for k, v in self.failure_patterns.items()},\n",
    "            \"adaptive_routing\": self.adaptive_routing\n",
    "        }\n",
    "        \n",
    "        for chain_name, health in self.chain_health.items():\n",
    "            status[\"chains\"][chain_name] = {\n",
    "                \"health_score\": health.health_score,\n",
    "                \"success_rate\": health.success_rate,\n",
    "                \"avg_latency\": health.avg_latency,\n",
    "                \"total_requests\": health.success_count + health.failure_count\n",
    "            }\n",
    "        \n",
    "        return status\n",
    "\n",
    "# Test the resilient system\n",
    "print(\"Resilient System Test\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "system = ResilientSystem()\n",
    "\n",
    "# Test with multiple queries\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Write a haiku about coding\",\n",
    "    \"Explain quantum computing\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"Generate a business plan outline\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n[{i}] Query: {prompt}\")\n",
    "    result = system.execute_with_fallback(prompt)\n",
    "    \n",
    "    print(f\"\\n  Result:\")\n",
    "    print(f\"    Success: {result['success']}\")\n",
    "    print(f\"    Chain Used: {result['chain_used']}\")\n",
    "    print(f\"    Time: {result['total_time']:.2f}s\")\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"    Response: {result['result'][:100]}...\")\n",
    "    \n",
    "    # Show execution path\n",
    "    print(f\"\\n  Execution Path:\")\n",
    "    for log in result['execution_log']:\n",
    "        status = \"✅\" if log['success'] else \"❌\"\n",
    "        print(f\"    {status} {log['chain']}\")\n",
    "\n",
    "# Show health status\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\n📊 System Health Status:\")\n",
    "health = system.get_health_status()\n",
    "\n",
    "for chain_name, metrics in health['chains'].items():\n",
    "    print(f\"\\n  {chain_name.capitalize()} Chain:\")\n",
    "    print(f\"    Health Score: {metrics['health_score']:.2f}\")\n",
    "    print(f\"    Success Rate: {metrics['success_rate']:.1%}\")\n",
    "    print(f\"    Avg Latency: {metrics['avg_latency']:.2f}s\")\n",
    "    print(f\"    Circuit Breaker: {health['circuit_breakers'][chain_name]}\")\n",
    "\n",
    "# Learn from failures\n",
    "print(\"\\n📚 Learning from failures...\")\n",
    "system.learn_from_failure(\"question\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build a comprehensive resilient system with fallbacks and self-healing!\")\n",
    "print(\"The solution includes circuit breakers, adaptive routing, and failure learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 3: Map-Reduce and Complex Orchestration\n",
    "\n",
    "Let's implement advanced orchestration patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "map_reduce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map-Reduce pattern for processing large documents\n",
    "\n",
    "class MapReduceProcessor:\n",
    "    \"\"\"Process large documents using map-reduce pattern.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "    \n",
    "    def split_document(self, document: str) -> List[str]:\n",
    "        \"\"\"Split document into chunks.\"\"\"\n",
    "        words = document.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size // 5):  # Rough word count\n",
    "            chunk = \" \".join(words[i:i + self.chunk_size // 5])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def map_operation(self, chunk: str, operation: str) -> str:\n",
    "        \"\"\"Apply operation to a single chunk.\"\"\"\n",
    "        map_prompt = f\"\"\"{operation}\n",
    "        \n",
    "        Text: {chunk}\n",
    "        \n",
    "        Result:\"\"\"\n",
    "        \n",
    "        result = self.llm.predict(map_prompt)\n",
    "        return result\n",
    "    \n",
    "    def reduce_operation(self, results: List[str], operation: str) -> str:\n",
    "        \"\"\"Reduce/combine results from map phase.\"\"\"\n",
    "        reduce_prompt = f\"\"\"{operation}\n",
    "        \n",
    "        Individual Results:\n",
    "        {chr(10).join(f'{i+1}. {r}' for i, r in enumerate(results))}\n",
    "        \n",
    "        Combined Result:\"\"\"\n",
    "        \n",
    "        final_result = self.llm.predict(reduce_prompt)\n",
    "        return final_result\n",
    "    \n",
    "    def process(self, document: str, map_instruction: str, reduce_instruction: str) -> Dict:\n",
    "        \"\"\"Execute map-reduce operation.\"\"\"\n",
    "        # Split\n",
    "        chunks = self.split_document(document)\n",
    "        print(f\"Split document into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Map phase\n",
    "        print(\"\\nMap phase:\")\n",
    "        map_results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"  Processing chunk {i+1}/{len(chunks)}...\")\n",
    "            result = self.map_operation(chunk, map_instruction)\n",
    "            map_results.append(result)\n",
    "        \n",
    "        # Reduce phase\n",
    "        print(\"\\nReduce phase:\")\n",
    "        final_result = self.reduce_operation(map_results, reduce_instruction)\n",
    "        \n",
    "        return {\n",
    "            \"chunks_processed\": len(chunks),\n",
    "            \"map_results\": map_results,\n",
    "            \"final_result\": final_result\n",
    "        }\n",
    "\n",
    "# Test map-reduce\n",
    "processor = MapReduceProcessor(chunk_size=500)\n",
    "\n",
    "# Sample document\n",
    "document = \"\"\"Artificial intelligence has revolutionized many industries. \n",
    "In healthcare, AI helps diagnose diseases and develop new treatments.\n",
    "In finance, AI detects fraud and manages investments.\n",
    "In transportation, AI powers self-driving cars.\n",
    "However, AI also raises ethical concerns about privacy and job displacement.\n",
    "We must ensure AI is developed responsibly and benefits everyone.\"\"\"\n",
    "\n",
    "result = processor.process(\n",
    "    document=document,\n",
    "    map_instruction=\"Extract the main point from this text segment\",\n",
    "    reduce_instruction=\"Combine these points into a comprehensive summary\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nMap Results:\")\n",
    "for i, mr in enumerate(result[\"map_results\"]):\n",
    "    print(f\"{i+1}. {mr}\")\n",
    "\n",
    "print(\"\\nFinal Result:\")\n",
    "print(result[\"final_result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 3: Build an Advanced Orchestration System\n",
    "\n",
    "Create a sophisticated system that orchestrates multiple processing patterns.\n",
    "\n",
    "**Task**: Build a system that:\n",
    "1. Supports map-reduce, pipeline, and parallel processing\n",
    "2. Dynamically selects processing strategy\n",
    "3. Handles nested orchestrations\n",
    "4. Provides progress tracking\n",
    "5. Optimizes for cost and performance\n",
    "\n",
    "Requirements:\n",
    "- Multiple orchestration patterns\n",
    "- Dynamic strategy selection\n",
    "- Resource optimization\n",
    "- Comprehensive monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_3_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your advanced orchestration system\n",
    "\n",
    "class OrchestrationStrategy(Enum):\n",
    "    MAP_REDUCE = \"map_reduce\"\n",
    "    PIPELINE = \"pipeline\"\n",
    "    PARALLEL = \"parallel\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "class AdvancedOrchestrator:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize orchestration components\n",
    "        pass\n",
    "    \n",
    "    def select_strategy(self, task: Dict) -> OrchestrationStrategy:\n",
    "        \"\"\"Select optimal orchestration strategy.\"\"\"\n",
    "        # TODO: Analyze task and select strategy\n",
    "        pass\n",
    "    \n",
    "    def execute_map_reduce(self, task: Dict) -> Dict:\n",
    "        \"\"\"Execute using map-reduce pattern.\"\"\"\n",
    "        # TODO: Implement map-reduce execution\n",
    "        pass\n",
    "    \n",
    "    def execute_pipeline(self, task: Dict) -> Dict:\n",
    "        \"\"\"Execute using pipeline pattern.\"\"\"\n",
    "        # TODO: Implement pipeline execution\n",
    "        pass\n",
    "    \n",
    "    def execute_parallel(self, task: Dict) -> Dict:\n",
    "        \"\"\"Execute using parallel pattern.\"\"\"\n",
    "        # TODO: Implement parallel execution\n",
    "        pass\n",
    "    \n",
    "    def orchestrate(self, task: Dict) -> Dict:\n",
    "        \"\"\"Main orchestration method.\"\"\"\n",
    "        # TODO: Select and execute strategy\n",
    "        # TODO: Track progress and optimize\n",
    "        pass\n",
    "\n",
    "# TODO: Test orchestration system\n",
    "# Test different task types\n",
    "# Verify strategy selection\n",
    "# Check performance optimization\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_3_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "class AdvancedOrchestrator:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=5)\n",
    "        self.cost_tracker = {\"tokens\": 0, \"requests\": 0}\n",
    "        self.performance_metrics = []\n",
    "    \n",
    "    def select_strategy(self, task: Dict) -> OrchestrationStrategy:\n",
    "        '''Select optimal orchestration strategy based on task characteristics.'''\n",
    "        \n",
    "        task_type = task.get(\"type\", \"general\")\n",
    "        data_size = len(task.get(\"data\", \"\"))\n",
    "        complexity = task.get(\"complexity\", \"medium\")\n",
    "        \n",
    "        # Strategy selection logic\n",
    "        if task_type == \"aggregation\" or data_size > 5000:\n",
    "            return OrchestrationStrategy.MAP_REDUCE\n",
    "        elif task_type == \"sequential\" or \"steps\" in task:\n",
    "            return OrchestrationStrategy.PIPELINE\n",
    "        elif task_type == \"independent\" or task.get(\"parallelize\", False):\n",
    "            return OrchestrationStrategy.PARALLEL\n",
    "        else:\n",
    "            # Hybrid for complex tasks\n",
    "            if complexity == \"high\":\n",
    "                return OrchestrationStrategy.HYBRID\n",
    "            else:\n",
    "                return OrchestrationStrategy.PIPELINE\n",
    "    \n",
    "    def execute_map_reduce(self, task: Dict) -> Dict:\n",
    "        '''Execute using map-reduce pattern.'''\n",
    "        print(\"\\n🗺️  Executing MAP-REDUCE strategy\")\n",
    "        \n",
    "        data = task.get(\"data\", \"\")\n",
    "        map_fn = task.get(\"map_function\", \"Summarize this chunk\")\n",
    "        reduce_fn = task.get(\"reduce_function\", \"Combine these summaries\")\n",
    "        \n",
    "        # Split data\n",
    "        chunks = self._split_data(data, task.get(\"chunk_size\", 1000))\n",
    "        print(f\"  Split into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Map phase (parallel)\n",
    "        print(\"  Map phase...\")\n",
    "        map_results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = []\n",
    "            for chunk in chunks:\n",
    "                future = executor.submit(self._process_chunk, chunk, map_fn)\n",
    "                futures.append(future)\n",
    "            \n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                map_results.append(result)\n",
    "        \n",
    "        # Reduce phase\n",
    "        print(\"  Reduce phase...\")\n",
    "        reduce_prompt = f\"{reduce_fn}\\n\\nResults:\\n\" + \"\\n\".join(map_results)\n",
    "        final_result = self.llm.predict(reduce_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"map_reduce\",\n",
    "            \"chunks_processed\": len(chunks),\n",
    "            \"result\": final_result\n",
    "        }\n",
    "    \n",
    "    def execute_pipeline(self, task: Dict) -> Dict:\n",
    "        '''Execute using pipeline pattern.'''\n",
    "        print(\"\\n🚇 Executing PIPELINE strategy\")\n",
    "        \n",
    "        data = task.get(\"data\", \"\")\n",
    "        steps = task.get(\"steps\", [])\n",
    "        \n",
    "        if not steps:\n",
    "            # Default pipeline\n",
    "            steps = [\n",
    "                \"Analyze the input\",\n",
    "                \"Process and transform\",\n",
    "                \"Generate final output\"\n",
    "            ]\n",
    "        \n",
    "        current_data = data\n",
    "        pipeline_results = []\n",
    "        \n",
    "        for i, step in enumerate(steps, 1):\n",
    "            print(f\"  Step {i}/{len(steps)}: {step[:30]}...\")\n",
    "            \n",
    "            prompt = f\"{step}\\n\\nInput: {current_data}\\n\\nOutput:\"\n",
    "            result = self.llm.predict(prompt)\n",
    "            \n",
    "            pipeline_results.append({\n",
    "                \"step\": i,\n",
    "                \"instruction\": step,\n",
    "                \"output\": result\n",
    "            })\n",
    "            \n",
    "            current_data = result  # Feed to next step\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"pipeline\",\n",
    "            \"steps_executed\": len(steps),\n",
    "            \"pipeline_results\": pipeline_results,\n",
    "            \"result\": current_data\n",
    "        }\n",
    "    \n",
    "    def execute_parallel(self, task: Dict) -> Dict:\n",
    "        '''Execute using parallel pattern.'''\n",
    "        print(\"\\n⚡ Executing PARALLEL strategy\")\n",
    "        \n",
    "        data = task.get(\"data\", \"\")\n",
    "        operations = task.get(\"operations\", [])\n",
    "        \n",
    "        if not operations:\n",
    "            # Default parallel operations\n",
    "            operations = [\n",
    "                \"Provide a summary\",\n",
    "                \"Extract key points\",\n",
    "                \"Identify main themes\"\n",
    "            ]\n",
    "        \n",
    "        print(f\"  Running {len(operations)} operations in parallel\")\n",
    "        \n",
    "        # Execute all operations in parallel\n",
    "        with ThreadPoolExecutor(max_workers=len(operations)) as executor:\n",
    "            futures = []\n",
    "            for op in operations:\n",
    "                prompt = f\"{op}\\n\\nText: {data}\\n\\nResult:\"\n",
    "                future = executor.submit(self.llm.predict, prompt)\n",
    "                futures.append((op, future))\n",
    "            \n",
    "            results = []\n",
    "            for op, future in futures:\n",
    "                result = future.result()\n",
    "                results.append({\n",
    "                    \"operation\": op,\n",
    "                    \"result\": result\n",
    "                })\n",
    "        \n",
    "        # Combine results\n",
    "        combined = \"\\n\\n\".join([f\"{r['operation']}:\\n{r['result']}\" for r in results])\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"parallel\",\n",
    "            \"operations_count\": len(operations),\n",
    "            \"parallel_results\": results,\n",
    "            \"result\": combined\n",
    "        }\n",
    "    \n",
    "    def execute_hybrid(self, task: Dict) -> Dict:\n",
    "        '''Execute using hybrid pattern (map-reduce + pipeline).'''\n",
    "        print(\"\\n🔄 Executing HYBRID strategy\")\n",
    "        \n",
    "        # Phase 1: Map-Reduce for initial processing\n",
    "        print(\"  Phase 1: Map-Reduce\")\n",
    "        map_reduce_task = {\n",
    "            \"data\": task.get(\"data\", \"\"),\n",
    "            \"map_function\": \"Extract important information\",\n",
    "            \"reduce_function\": \"Synthesize the extracted information\"\n",
    "        }\n",
    "        mr_result = self.execute_map_reduce(map_reduce_task)\n",
    "        \n",
    "        # Phase 2: Pipeline for refinement\n",
    "        print(\"\\n  Phase 2: Pipeline\")\n",
    "        pipeline_task = {\n",
    "            \"data\": mr_result[\"result\"],\n",
    "            \"steps\": [\n",
    "                \"Enhance and clarify the content\",\n",
    "                \"Add structure and formatting\",\n",
    "                \"Polish and finalize\"\n",
    "            ]\n",
    "        }\n",
    "        pipeline_result = self.execute_pipeline(pipeline_task)\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"hybrid\",\n",
    "            \"phases\": [\"map_reduce\", \"pipeline\"],\n",
    "            \"result\": pipeline_result[\"result\"]\n",
    "        }\n",
    "    \n",
    "    def _split_data(self, data: str, chunk_size: int) -> List[str]:\n",
    "        '''Split data into chunks.'''\n",
    "        words = data.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size // 5):\n",
    "            chunk = \" \".join(words[i:i + chunk_size // 5])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _process_chunk(self, chunk: str, instruction: str) -> str:\n",
    "        '''Process a single chunk.'''\n",
    "        prompt = f\"{instruction}\\n\\nChunk: {chunk}\\n\\nResult:\"\n",
    "        return self.llm.predict(prompt)\n",
    "    \n",
    "    def orchestrate(self, task: Dict) -> Dict:\n",
    "        '''Main orchestration method with optimization.'''\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Select strategy\n",
    "        strategy = self.select_strategy(task)\n",
    "        print(f\"\\n🎯 Selected Strategy: {strategy.value}\")\n",
    "        \n",
    "        # Track initial cost\n",
    "        initial_tokens = self.cost_tracker[\"tokens\"]\n",
    "        \n",
    "        # Execute based on strategy\n",
    "        try:\n",
    "            if strategy == OrchestrationStrategy.MAP_REDUCE:\n",
    "                result = self.execute_map_reduce(task)\n",
    "            elif strategy == OrchestrationStrategy.PIPELINE:\n",
    "                result = self.execute_pipeline(task)\n",
    "            elif strategy == OrchestrationStrategy.PARALLEL:\n",
    "                result = self.execute_parallel(task)\n",
    "            else:  # HYBRID\n",
    "                result = self.execute_hybrid(task)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Estimate tokens (simplified)\n",
    "            estimated_tokens = len(str(result)) // 4\n",
    "            self.cost_tracker[\"tokens\"] += estimated_tokens\n",
    "            self.cost_tracker[\"requests\"] += 1\n",
    "            \n",
    "            # Add performance metrics\n",
    "            metrics = {\n",
    "                \"strategy\": strategy.value,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"estimated_tokens\": estimated_tokens,\n",
    "                \"estimated_cost\": estimated_tokens * 0.000002  # Example pricing\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics.append(metrics)\n",
    "            result[\"metrics\"] = metrics\n",
    "            \n",
    "            print(f\"\\n📊 Execution Metrics:\")\n",
    "            print(f\"  Time: {execution_time:.2f}s\")\n",
    "            print(f\"  Estimated Tokens: {estimated_tokens}\")\n",
    "            print(f\"  Estimated Cost: ${metrics['estimated_cost']:.4f}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"strategy\": strategy.value,\n",
    "                \"error\": str(e),\n",
    "                \"result\": None\n",
    "            }\n",
    "    \n",
    "    def get_optimization_report(self) -> Dict:\n",
    "        '''Get optimization and performance report.'''\n",
    "        if not self.performance_metrics:\n",
    "            return {\"message\": \"No metrics available\"}\n",
    "        \n",
    "        total_time = sum(m[\"execution_time\"] for m in self.performance_metrics)\n",
    "        total_cost = sum(m[\"estimated_cost\"] for m in self.performance_metrics)\n",
    "        \n",
    "        # Strategy distribution\n",
    "        from collections import Counter\n",
    "        strategy_counts = Counter(m[\"strategy\"] for m in self.performance_metrics)\n",
    "        \n",
    "        return {\n",
    "            \"total_executions\": len(self.performance_metrics),\n",
    "            \"total_time\": total_time,\n",
    "            \"total_estimated_cost\": total_cost,\n",
    "            \"avg_execution_time\": total_time / len(self.performance_metrics),\n",
    "            \"strategy_distribution\": dict(strategy_counts),\n",
    "            \"total_tokens\": self.cost_tracker[\"tokens\"],\n",
    "            \"recommendations\": self._get_optimization_recommendations()\n",
    "        }\n",
    "    \n",
    "    def _get_optimization_recommendations(self) -> List[str]:\n",
    "        '''Generate optimization recommendations.'''\n",
    "        recommendations = []\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            avg_time = sum(m[\"execution_time\"] for m in self.performance_metrics) / len(self.performance_metrics)\n",
    "            \n",
    "            if avg_time > 5:\n",
    "                recommendations.append(\"Consider increasing parallelization for better performance\")\n",
    "            \n",
    "            # Check strategy efficiency\n",
    "            strategy_times = {}\n",
    "            for m in self.performance_metrics:\n",
    "                if m[\"strategy\"] not in strategy_times:\n",
    "                    strategy_times[m[\"strategy\"]] = []\n",
    "                strategy_times[m[\"strategy\"]].append(m[\"execution_time\"])\n",
    "            \n",
    "            for strategy, times in strategy_times.items():\n",
    "                avg_strategy_time = sum(times) / len(times)\n",
    "                if avg_strategy_time > 10:\n",
    "                    recommendations.append(f\"Optimize {strategy} strategy - avg time {avg_strategy_time:.1f}s\")\n",
    "        \n",
    "        if self.cost_tracker[\"tokens\"] > 100000:\n",
    "            recommendations.append(\"High token usage - consider caching or compression\")\n",
    "        \n",
    "        return recommendations if recommendations else [\"System operating efficiently\"]\n",
    "\n",
    "# Test the orchestrator\n",
    "print(\"Advanced Orchestration System Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "orchestrator = AdvancedOrchestrator()\n",
    "\n",
    "# Test different task types\n",
    "test_tasks = [\n",
    "    {\n",
    "        \"name\": \"Large Document Summary\",\n",
    "        \"type\": \"aggregation\",\n",
    "        \"data\": \"AI is transforming industries. \" * 100,  # Large text\n",
    "        \"complexity\": \"high\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Sequential Processing\",\n",
    "        \"type\": \"sequential\",\n",
    "        \"data\": \"Raw data that needs processing\",\n",
    "        \"steps\": [\n",
    "            \"Clean and normalize the data\",\n",
    "            \"Extract meaningful patterns\",\n",
    "            \"Generate insights\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Parallel Analysis\",\n",
    "        \"type\": \"independent\",\n",
    "        \"data\": \"Complex topic requiring multiple perspectives\",\n",
    "        \"parallelize\": True,\n",
    "        \"operations\": [\n",
    "            \"Technical analysis\",\n",
    "            \"Business perspective\",\n",
    "            \"User experience view\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Complex Hybrid Task\",\n",
    "        \"type\": \"complex\",\n",
    "        \"data\": \"Multi-faceted problem requiring sophisticated processing\",\n",
    "        \"complexity\": \"high\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for task in test_tasks:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Task: {task['name']}\")\n",
    "    \n",
    "    result = orchestrator.orchestrate(task)\n",
    "    \n",
    "    print(f\"\\nResult Preview:\")\n",
    "    if result.get(\"result\"):\n",
    "        print(f\"  {result['result'][:200]}...\")\n",
    "\n",
    "# Show optimization report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n📈 Optimization Report:\")\n",
    "report = orchestrator.get_optimization_report()\n",
    "\n",
    "for key, value in report.items():\n",
    "    if key != \"recommendations\":\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n  Recommendations:\")\n",
    "for rec in report[\"recommendations\"]:\n",
    "    print(f\"    • {rec}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build an advanced orchestration system with multiple processing patterns!\")\n",
    "print(\"The solution includes map-reduce, pipeline, parallel, and hybrid strategies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've mastered advanced LangChain patterns. You can now:\n",
    "\n",
    "✅ Implement sophisticated routing and conditional logic\n",
    "✅ Build resilient systems with retry and fallback mechanisms\n",
    "✅ Create self-correcting chains\n",
    "✅ Design map-reduce processing patterns\n",
    "✅ Orchestrate complex multi-pattern workflows\n",
    "✅ Optimize for performance and cost\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Routing Enables Flexibility**: Dynamic routing adapts to different inputs\n",
    "- **Resilience is Critical**: Production systems need fallbacks and retries\n",
    "- **Self-Correction Improves Quality**: Validation and correction loops enhance output\n",
    "- **Orchestration Scales**: Map-reduce and parallel patterns handle large workloads\n",
    "- **Optimization Matters**: Track metrics and optimize strategies\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 12**: Learn about Production Deployment\n",
    "- **Practice**: Implement these patterns in real applications\n",
    "- **Experiment**: Combine patterns for complex use cases\n",
    "- **Scale**: Deploy these patterns in production\n",
    "\n",
    "### Additional Challenges:\n",
    "1. Build a self-optimizing routing system that learns from usage\n",
    "2. Create a distributed map-reduce system across multiple servers\n",
    "3. Implement a circuit breaker pattern with adaptive thresholds\n",
    "4. Design a cost-optimized orchestrator for different LLM providers\n",
    "5. Build a meta-orchestrator that can compose orchestration patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}