{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Chat Applications\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this section, you will be able to:\n",
    "- Create simple static chat interfaces in Colab\n",
    "- Make API calls to Google's Gemini LLM\n",
    "- Build interactive chat applications with AI responses\n",
    "- Implement conversation memory to maintain context across messages\n",
    "- Understand the architecture of chat-based AI applications\n",
    "\n",
    "## Why This Matters: Real-World AI/RAG/Agentic Applications\n",
    "**In AI Systems:**\n",
    "- Chat interfaces are the primary way users interact with LLMs like ChatGPT, Claude, and Gemini\n",
    "- Understanding chat architecture is fundamental to building AI applications\n",
    "- API integration skills are essential for leveraging powerful language models\n",
    "\n",
    "**In RAG Pipelines:**\n",
    "- Chat interfaces allow users to query your knowledge base naturally\n",
    "- Conversation memory enables multi-turn interactions for clarifying questions\n",
    "- Each user message can trigger document retrieval and context injection\n",
    "\n",
    "**In Agentic AI:**\n",
    "- Agents use chat-like interfaces to communicate with users and other agents\n",
    "- Memory systems track conversation state for complex multi-step tasks\n",
    "- Chat history provides context for agent decision-making\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python syntax (variables, strings, functions)\n",
    "- Understanding of lists and dictionaries\n",
    "- Familiarity with loops and conditionals\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Activity 1\n",
    "**Concept**: Creating a simple static chat interface\n",
    "\n",
    "### Example 1: Basic Chat Display\n",
    "\n",
    "**Problem**: Display a simple chat conversation with hardcoded messages\n",
    "\n",
    "**Expected Output**:\n",
    "```\n",
    "User: Hello!\n",
    "Bot: Hi there! How can I help you?\n",
    "User: What's the weather?\n",
    "Bot: I'm sorry, I don't have access to weather information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Store chat messages as a list of dictionaries\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"bot\", \"content\": \"Hi there! How can I help you?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
    "    {\"role\": \"bot\", \"content\": \"I'm sorry, I don't have access to weather information.\"}\n",
    "]\n",
    "\n",
    "# Display the conversation\n",
    "for message in messages:\n",
    "    # Capitalize the role name for display\n",
    "    role = message[\"role\"].capitalize()\n",
    "    content = message[\"content\"]\n",
    "    print(f\"{role}: {content}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- We use a list of dictionaries to store messages\n",
    "- Each message has a \"role\" (user or bot) and \"content\" (the text)\n",
    "- This data structure mirrors how real chat APIs work (like OpenAI, Anthropic, Google)\n",
    "- The loop displays each message in order, creating a conversation flow\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Formatted Chat Display\n",
    "\n",
    "**Problem**: Create a more visually appealing chat display with formatting\n",
    "\n",
    "**Expected Output**:\n",
    "```\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "👤 User: Hello!\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "🤖 Bot: Hi there! How can I help you?\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Chat messages with better formatting\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"bot\", \"content\": \"Hi there! How can I help you?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
    "    {\"role\": \"bot\", \"content\": \"I'm sorry, I don't have access to weather information.\"}\n",
    "]\n",
    "\n",
    "# Function to display chat with formatting\n",
    "def display_chat(messages):\n",
    "    for message in messages:\n",
    "        # Choose emoji based on role\n",
    "        emoji = \"👤\" if message[\"role\"] == \"user\" else \"🤖\"\n",
    "        role = message[\"role\"].capitalize()\n",
    "        \n",
    "        # Print with separator line and emoji\n",
    "        print(\"━\" * 40)\n",
    "        print(f\"{emoji} {role}: {message['content']}\")\n",
    "    print(\"━\" * 40)\n",
    "\n",
    "# Display the conversation\n",
    "display_chat(messages)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Functions make code reusable - we can display any conversation\n",
    "- Conditional expressions choose different emojis for user vs bot\n",
    "- Visual separators make the conversation easier to read\n",
    "- This pattern scales to longer conversations\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 1\n",
    "**Practice**: Creating static chat displays\n",
    "\n",
    "### Exercise 1: Your Own Conversation\n",
    "\n",
    "**Task**: Create a list of messages representing a conversation about Python programming (at least 4 messages), then display them\n",
    "\n",
    "**Expected Output**: A conversation where the user asks about Python and the bot responds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Create a conversation about Python\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "    {\"role\": \"bot\", \"content\": \"Python is a popular programming language known for its simplicity.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What can I build with it?\"},\n",
    "    {\"role\": \"bot\", \"content\": \"You can build web apps, AI systems, data analysis tools, and much more!\"}\n",
    "]\n",
    "\n",
    "# Display the conversation\n",
    "for message in messages:\n",
    "    role = message[\"role\"].capitalize()\n",
    "    print(f\"{role}: {message['content']}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "The same pattern applies to any conversation topic. The list-of-dictionaries structure is flexible and mirrors how real AI chat systems store conversations.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Display Function\n",
    "\n",
    "**Task**: Write a function `show_conversation(messages)` that displays messages with \">>>\" prefix for user and \"<<<\" prefix for bot\n",
    "\n",
    "**Expected Output**:\n",
    "```\n",
    ">>> USER: Hello\n",
    "<<< BOT: Hi there\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def show_conversation(messages):\n",
    "    \"\"\"Display messages with directional arrows\"\"\"\n",
    "    for message in messages:\n",
    "        # Choose prefix based on role\n",
    "        prefix = \">>>\" if message[\"role\"] == \"user\" else \"<<<\"\n",
    "        role = message[\"role\"].upper()\n",
    "        print(f\"{prefix} {role}: {message['content']}\")\n",
    "\n",
    "# Test it\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"bot\", \"content\": \"Hi there\"}\n",
    "]\n",
    "\n",
    "show_conversation(messages)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "Functions with clear purposes make code modular. The conditional expression chooses the right prefix, and `.upper()` makes role names stand out.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 2\n",
    "**Concept**: Making basic API calls to Google's Gemini LLM\n",
    "\n",
    "### Example 1: Setting Up the Gemini API\n",
    "\n",
    "**Problem**: Configure and test a connection to Google's Gemini API\n",
    "\n",
    "**Expected Output**: A response from Gemini to a simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "\n",
    "# Get API key from Colab secrets\n",
    "# To set this up: Click the key icon (🔑) in the left sidebar\n",
    "# Add a secret named 'GEMINI_API_KEY' with your key from https://makersuite.google.com/app/apikey\n",
    "api_key = userdata.get('GEMINI_API_KEY')\n",
    "\n",
    "# Configure the API\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create a model instance\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "# Make a simple request\n",
    "response = model.generate_content(\"Say hello in a friendly way\")\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- We import the Google Generative AI library\n",
    "- `userdata.get()` securely retrieves the API key from Colab secrets (never hardcode keys!)\n",
    "- `genai.configure()` sets up authentication\n",
    "- `GenerativeModel('gemini-pro')` creates a model instance\n",
    "- `generate_content()` sends a prompt and returns the AI's response\n",
    "- `response.text` extracts the text from the response object\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Handling API Responses\n",
    "\n",
    "**Problem**: Make an API call and examine the response structure\n",
    "\n",
    "**Expected Output**: Display both the response text and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Make a request\n",
    "prompt = \"Explain what Python is in one sentence.\"\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "# Display the response text\n",
    "print(\"Response Text:\")\n",
    "print(response.text)\n",
    "print()\n",
    "\n",
    "# Examine response metadata\n",
    "print(\"Response Metadata:\")\n",
    "print(f\"Candidates: {len(response.candidates)}\")\n",
    "print(f\"Finish Reason: {response.candidates[0].finish_reason}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- The response object contains more than just text\n",
    "- `response.text` is the AI-generated text\n",
    "- `response.candidates` contains all possible responses (usually just one)\n",
    "- `finish_reason` tells you why the generation stopped (e.g., \"STOP\" for natural completion)\n",
    "- Understanding the response structure helps with error handling and debugging\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Error Handling\n",
    "\n",
    "**Problem**: Safely handle potential API errors\n",
    "\n",
    "**Expected Output**: Either the response or a user-friendly error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def ask_gemini(prompt):\n",
    "    \"\"\"Safely send a prompt to Gemini and return the response\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        # Return error message instead of crashing\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test the function\n",
    "result = ask_gemini(\"What is artificial intelligence?\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Try-except blocks catch errors without crashing the program\n",
    "- This handles network issues, invalid API keys, rate limits, etc.\n",
    "- Wrapping API calls in functions makes code reusable and maintainable\n",
    "- In production apps, you'd log errors and potentially retry failed requests\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 2\n",
    "**Practice**: Making API calls to Gemini\n",
    "\n",
    "### Exercise 1: Your First AI Question\n",
    "\n",
    "**Task**: Ask Gemini to explain any programming concept you're curious about\n",
    "\n",
    "**Expected Output**: AI-generated explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Ask about any concept\n",
    "prompt = \"What are Python decorators?\"\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "The API call is the same regardless of the prompt content. This is the foundation of all LLM applications - sending text prompts and receiving generated responses.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Multiple Questions\n",
    "\n",
    "**Task**: Create a list of 3 questions, send each to Gemini, and display the questions with their answers\n",
    "\n",
    "**Expected Output**: Each question followed by Gemini's answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# List of questions\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What is the difference between a list and a tuple?\",\n",
    "    \"What is a REST API?\"\n",
    "]\n",
    "\n",
    "# Ask each question\n",
    "for question in questions:\n",
    "    print(f\"Q: {question}\")\n",
    "    response = model.generate_content(question)\n",
    "    print(f\"A: {response.text}\")\n",
    "    print(\"―\" * 50)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "Loops let us process multiple prompts efficiently. Note that each API call is independent - the model doesn't remember previous questions (yet - we'll add memory later!).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 3\n",
    "**Concept**: Building an interactive chat application\n",
    "\n",
    "### Example 1: Simple Interactive Loop\n",
    "\n",
    "**Problem**: Create a chat loop where users can ask questions until they type 'quit'\n",
    "\n",
    "**Expected Output**: Interactive conversation that continues until user exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "print(\"Chat with Gemini! (type 'quit' to exit)\")\n",
    "print(\"=\"* 50)\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_message = input(\"You: \")\n",
    "    \n",
    "    # Check for exit command\n",
    "    if user_message.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Get AI response\n",
    "    try:\n",
    "        response = model.generate_content(user_message)\n",
    "        print(f\"Gemini: {response.text}\")\n",
    "        print(\"―\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `while True` creates an infinite loop\n",
    "- `input()` waits for the user to type something\n",
    "- We check if the user wants to quit before making the API call\n",
    "- `break` exits the loop\n",
    "- Try-except handles any API errors gracefully\n",
    "- This is a simple but functional chatbot!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Chat with Display Function\n",
    "\n",
    "**Problem**: Enhance the chat loop with better formatting and message storage\n",
    "\n",
    "**Expected Output**: Formatted conversation with history stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def display_message(role, content):\n",
    "    \"\"\"Display a chat message with formatting\"\"\"\n",
    "    emoji = \"👤\" if role == \"user\" else \"🤖\"\n",
    "    print(f\"{emoji} {role.upper()}: {content}\")\n",
    "    print()\n",
    "\n",
    "# Store conversation history\n",
    "conversation = []\n",
    "\n",
    "print(\"Enhanced Chat with Gemini! (type 'quit' to exit)\")\n",
    "print(\"=\"* 50)\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_message = input(\"You: \")\n",
    "    \n",
    "    if user_message.lower() == 'quit':\n",
    "        print(f\"\\nTotal messages exchanged: {len(conversation)}\")\n",
    "        break\n",
    "    \n",
    "    # Store and display user message\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get and display AI response\n",
    "    try:\n",
    "        response = model.generate_content(user_message)\n",
    "        bot_message = response.text\n",
    "        \n",
    "        # Store and display bot message\n",
    "        conversation.append({\"role\": \"bot\", \"content\": bot_message})\n",
    "        display_message(\"gemini\", bot_message)\n",
    "        print(\"―\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- We now store every message in a list (conversation history)\n",
    "- The display function keeps formatting consistent\n",
    "- We can track conversation length and analyze it later\n",
    "- This structure mirrors professional chat applications\n",
    "- However, note that Gemini still doesn't remember previous messages!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 3\n",
    "**Practice**: Building interactive chat applications\n",
    "\n",
    "### Exercise 1: Custom Welcome Message\n",
    "\n",
    "**Task**: Create a chat loop that:\n",
    "1. Displays a custom welcome message\n",
    "2. Takes user input\n",
    "3. Gets Gemini's response\n",
    "4. Exits when user types 'exit'\n",
    "\n",
    "**Expected Output**: Working chat interface with your custom welcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "print(\"Welcome to Python Learning Assistant!\")\n",
    "print(\"Ask me anything about Python. Type 'exit' to quit.\")\n",
    "print(\"=\"* 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Your question: \")\n",
    "    \n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Happy coding!\")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(user_input)\n",
    "        print(f\"\\nAssistant: {response.text}\\n\")\n",
    "        print(\"―\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "Customizing the welcome message and exit command makes the chat feel more tailored to your specific use case. This same pattern works for any specialized chatbot.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Message Counter\n",
    "\n",
    "**Task**: Add a feature that counts and displays the total number of messages after every exchange\n",
    "\n",
    "**Expected Output**: \n",
    "```\n",
    "You: Hello\n",
    "Bot: Hi there!\n",
    "[2 messages so far]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "message_count = 0\n",
    "\n",
    "print(\"Chat with message counter (type 'quit' to exit)\")\n",
    "print(\"=\"* 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        print(f\"\\nFinal count: {message_count} messages\")\n",
    "        break\n",
    "    \n",
    "    # Increment for user message\n",
    "    message_count += 1\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(user_input)\n",
    "        print(f\"Bot: {response.text}\")\n",
    "        \n",
    "        # Increment for bot message\n",
    "        message_count += 1\n",
    "        \n",
    "        print(f\"[{message_count} messages so far]\")\n",
    "        print(\"―\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "We maintain a counter variable that increments after each message (user and bot). This helps track engagement and can be useful for analytics in production apps.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 4\n",
    "**Concept**: Adding conversation memory using chat sessions\n",
    "\n",
    "### Example 1: Understanding Memory Problem\n",
    "\n",
    "**Problem**: Demonstrate why the current chat doesn't remember context\n",
    "\n",
    "**Expected Output**: Shows that follow-up questions don't work without memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# First message\n",
    "response1 = model.generate_content(\"My name is Alice\")\n",
    "print(\"Message 1:\")\n",
    "print(response1.text)\n",
    "print()\n",
    "\n",
    "# Follow-up question (should remember the name, but won't!)\n",
    "response2 = model.generate_content(\"What's my name?\")\n",
    "print(\"Message 2:\")\n",
    "print(response2.text)\n",
    "```\n",
    "\n",
    "**Why this works (or doesn't!):**\n",
    "- Each `generate_content()` call is independent\n",
    "- The model has no memory of previous messages\n",
    "- It will say it doesn't know your name because each call is a fresh conversation\n",
    "- This is where chat sessions come in - they maintain context across messages\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Using Chat Sessions\n",
    "\n",
    "**Problem**: Create a chat session that maintains context across messages\n",
    "\n",
    "**Expected Output**: AI remembers previous messages in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Start a chat session\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "# First message\n",
    "response1 = chat.send_message(\"My name is Alice and I love Python\")\n",
    "print(\"User: My name is Alice and I love Python\")\n",
    "print(f\"Gemini: {response1.text}\\n\")\n",
    "\n",
    "# Follow-up question (now it will remember!)\n",
    "response2 = chat.send_message(\"What's my name?\")\n",
    "print(\"User: What's my name?\")\n",
    "print(f\"Gemini: {response2.text}\\n\")\n",
    "\n",
    "# Another follow-up\n",
    "response3 = chat.send_message(\"What programming language do I like?\")\n",
    "print(\"User: What programming language do I like?\")\n",
    "print(f\"Gemini: {response3.text}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `start_chat()` creates a stateful chat session\n",
    "- `send_message()` adds messages to the conversation history\n",
    "- The model receives all previous messages as context\n",
    "- This enables natural follow-up questions and context-aware responses\n",
    "- The `history=[]` parameter starts with an empty conversation\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Inspecting Chat History\n",
    "\n",
    "**Problem**: View the conversation history stored in the chat session\n",
    "\n",
    "**Expected Output**: Display all messages exchanged in the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Create a chat and exchange some messages\n",
    "chat = model.start_chat(history=[])\n",
    "chat.send_message(\"Hello! I'm learning about AI.\")\n",
    "chat.send_message(\"What is machine learning?\")\n",
    "\n",
    "# Display the conversation history\n",
    "print(\"Conversation History:\")\n",
    "print(\"=\"* 50)\n",
    "\n",
    "for message in chat.history:\n",
    "    # Message has 'role' and 'parts' attributes\n",
    "    role = message.role\n",
    "    content = message.parts[0].text\n",
    "    \n",
    "    emoji = \"👤\" if role == \"user\" else \"🤖\"\n",
    "    print(f\"{emoji} {role.upper()}: {content}\")\n",
    "    print(\"―\" * 50)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `chat.history` contains all messages in the session\n",
    "- Each message has a `role` (\"user\" or \"model\") and `parts` (list of content)\n",
    "- `parts[0].text` extracts the text content\n",
    "- This is useful for debugging, analytics, or saving conversations\n",
    "- In production, you'd store this history in a database\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 4\n",
    "**Practice**: Building chat applications with memory\n",
    "\n",
    "### Exercise 1: Multi-Turn Conversation\n",
    "\n",
    "**Task**: Create a chat session and have a conversation with at least 3 connected messages (each building on the previous)\n",
    "\n",
    "**Expected Output**: A conversation where context carries through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Start a chat session\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "# First message - introduce a topic\n",
    "r1 = chat.send_message(\"I'm building a weather app\")\n",
    "print(\"You: I'm building a weather app\")\n",
    "print(f\"Gemini: {r1.text}\\n\")\n",
    "\n",
    "# Second message - ask related question\n",
    "r2 = chat.send_message(\"What APIs should I use for it?\")\n",
    "print(\"You: What APIs should I use for it?\")\n",
    "print(f\"Gemini: {r2.text}\\n\")\n",
    "\n",
    "# Third message - follow-up\n",
    "r3 = chat.send_message(\"Are any of them free?\")\n",
    "print(\"You: Are any of them free?\")\n",
    "print(f\"Gemini: {r3.text}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "The chat session maintains context, so \"it\" refers to the weather app, and \"them\" refers to the APIs. This natural conversation flow is only possible with memory.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: History Viewer\n",
    "\n",
    "**Task**: Create a function `show_history(chat)` that displays all messages in a chat session with nice formatting\n",
    "\n",
    "**Expected Output**: Formatted display of all messages in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def show_history(chat):\n",
    "    \"\"\"Display all messages in a chat session\"\"\"\n",
    "    print(\"\\n📜 Conversation History\")\n",
    "    print(\"=\"* 50)\n",
    "    \n",
    "    for i, message in enumerate(chat.history, 1):\n",
    "        role = message.role\n",
    "        content = message.parts[0].text\n",
    "        emoji = \"👤\" if role == \"user\" else \"🤖\"\n",
    "        \n",
    "        print(f\"\\nMessage {i}:\")\n",
    "        print(f\"{emoji} {role.upper()}: {content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"* 50)\n",
    "    print(f\"Total messages: {len(chat.history)}\")\n",
    "\n",
    "# Test it\n",
    "chat = model.start_chat(history=[])\n",
    "chat.send_message(\"Hello!\")\n",
    "chat.send_message(\"How are you?\")\n",
    "show_history(chat)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "Functions make code reusable. We can now view any chat's history with a single function call. The `enumerate()` function gives us message numbers.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructor Activity 5\n",
    "**Concept**: Building a complete interactive chat app with memory\n",
    "\n",
    "### Example 1: Full-Featured Chat Application\n",
    "\n",
    "**Problem**: Combine all concepts into a production-ready chat application\n",
    "\n",
    "**Expected Output**: Interactive chat with memory, error handling, and commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for live demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def run_chat_app():\n",
    "    \"\"\"Run an interactive chat application with memory\"\"\"\n",
    "    \n",
    "    # Start a chat session\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    # Welcome message\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🤖 Gemini Chat Application\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Commands:\")\n",
    "    print(\"  - Type 'quit' or 'exit' to end the conversation\")\n",
    "    print(\"  - Type 'history' to view conversation history\")\n",
    "    print(\"  - Type 'clear' to start a new conversation\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Handle commands\n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"\\n👋 Thank you for chatting! Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'history':\n",
    "                print(\"\\n📜 Conversation History:\")\n",
    "                for msg in chat.history:\n",
    "                    role = msg.role.upper()\n",
    "                    text = msg.parts[0].text[:100]  # Truncate long messages\n",
    "                    print(f\"  {role}: {text}...\")\n",
    "                print()\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'clear':\n",
    "                chat = model.start_chat(history=[])\n",
    "                print(\"\\n🔄 Conversation cleared. Starting fresh!\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Send message and get response\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"\\n🤖 Gemini: {response.text}\\n\")\n",
    "            print(\"―\" * 60 + \"\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n👋 Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error: {e}\\n\")\n",
    "            print(\"Please try again.\\n\")\n",
    "\n",
    "# Run the app\n",
    "run_chat_app()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Wrapping everything in a function makes it clean and reusable\n",
    "- We handle multiple commands (quit, history, clear)\n",
    "- Error handling catches both API errors and keyboard interrupts (Ctrl+C)\n",
    "- The chat session maintains context throughout the conversation\n",
    "- Empty input is ignored with `continue`\n",
    "- `.strip()` removes leading/trailing whitespace\n",
    "- This is a complete, production-ready chat application!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Chat with System Prompt\n",
    "\n",
    "**Problem**: Initialize a chat with a system prompt that sets the AI's behavior\n",
    "\n",
    "**Expected Output**: AI responds according to the system instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Create a model with system instruction\n",
    "model_with_instructions = genai.GenerativeModel(\n",
    "    'gemini-pro',\n",
    "    system_instruction=\"You are a helpful Python tutor. You explain concepts clearly and give practical examples. Keep responses concise.\"\n",
    ")\n",
    "\n",
    "# Start chat with this customized model\n",
    "chat = model_with_instructions.start_chat(history=[])\n",
    "\n",
    "# Test it\n",
    "response = chat.send_message(\"What are list comprehensions?\")\n",
    "print(f\"Tutor: {response.text}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- System instructions guide the AI's personality and behavior\n",
    "- This is crucial for specialized chatbots (customer service, tutoring, etc.)\n",
    "- The instruction applies to all messages in conversations with this model\n",
    "- In RAG systems, system prompts often include instructions about using retrieved context\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learner Activity 5\n",
    "**Practice**: Building complete chat applications\n",
    "\n",
    "### Exercise 1: Specialized Chatbot\n",
    "\n",
    "**Task**: Create a chat application with a system prompt for a specific purpose (e.g., coding helper, travel advisor, recipe assistant). Include at least the 'quit' and 'history' commands.\n",
    "\n",
    "**Expected Output**: Working specialized chatbot with memory and commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "# Create a specialized model\n",
    "recipe_bot = genai.GenerativeModel(\n",
    "    'gemini-pro',\n",
    "    system_instruction=\"You are a helpful recipe assistant. Provide clear cooking instructions and suggest ingredient substitutions when asked.\"\n",
    ")\n",
    "\n",
    "def run_recipe_chat():\n",
    "    chat = recipe_bot.start_chat(history=[])\n",
    "    \n",
    "    print(\"\\n🍳 Recipe Assistant\")\n",
    "    print(\"Ask me about recipes, ingredients, or cooking tips!\")\n",
    "    print(\"Commands: 'quit' to exit, 'history' to view conversation\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Happy cooking!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'history':\n",
    "            for msg in chat.history:\n",
    "                print(f\"{msg.role.upper()}: {msg.parts[0].text[:80]}...\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"\\n🤖 Chef: {response.text}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_recipe_chat()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "The system instruction customizes the AI's expertise. You can create specialized chatbots for any domain by changing the system prompt.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Chat with Save Feature\n",
    "\n",
    "**Task**: Add a 'save' command that writes the conversation history to a text file\n",
    "\n",
    "**Expected Output**: When user types 'save', conversation is written to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "def save_conversation(chat):\n",
    "    \"\"\"Save chat history to a text file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"chat_history_{timestamp}.txt\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Conversation History\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        for msg in chat.history:\n",
    "            role = msg.role.upper()\n",
    "            text = msg.parts[0].text\n",
    "            f.write(f\"{role}: {text}\\n\\n\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def run_chat_with_save():\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"Chat with Save Feature\")\n",
    "    print(\"Commands: 'quit', 'history', 'save'\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'save':\n",
    "            filename = save_conversation(chat)\n",
    "            print(f\"✅ Conversation saved to {filename}\\n\")\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'history':\n",
    "            for msg in chat.history:\n",
    "                print(f\"{msg.role.upper()}: {msg.parts[0].text[:80]}...\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"\\nBot: {response.text}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_chat_with_save()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- We use `datetime` to create unique filenames\n",
    "- File writing with `with open()` ensures proper file handling\n",
    "- This lets users keep records of important conversations\n",
    "- In production, you'd save to a database instead of text files\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional Extra Practice\n",
    "**Challenge yourself with these problems that integrate all the concepts**\n",
    "\n",
    "### Challenge 1: Multi-User Chat System\n",
    "\n",
    "**Task**: Create a chat system that can handle multiple users. Use a dictionary to store separate chat sessions for different users. Include commands to switch between users.\n",
    "\n",
    "**Example Usage**:\n",
    "```\n",
    "Commands: /user <name> - switch user, /list - list users\n",
    "Current user: Alice\n",
    "You: Hello\n",
    "Bot: Hi Alice!\n",
    "/user Bob\n",
    "Switched to Bob\n",
    "You: Hello\n",
    "Bot: Hi Bob!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def run_multi_user_chat():\n",
    "    # Store chat sessions for each user\n",
    "    users = {}\n",
    "    current_user = \"Guest\"\n",
    "    \n",
    "    # Create default user\n",
    "    users[current_user] = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"Multi-User Chat System\")\n",
    "    print(\"Commands: /user <name> - switch user, /list - list users, /quit - exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(f\"[{current_user}] You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Handle commands\n",
    "        if user_input.lower() == '/quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        elif user_input.startswith('/user '):\n",
    "            new_user = user_input.split(' ', 1)[1]\n",
    "            \n",
    "            # Create chat session if new user\n",
    "            if new_user not in users:\n",
    "                users[new_user] = model.start_chat(history=[])\n",
    "                print(f\"Created new user: {new_user}\")\n",
    "            \n",
    "            current_user = new_user\n",
    "            print(f\"Switched to {current_user}\\n\")\n",
    "            continue\n",
    "        \n",
    "        elif user_input.lower() == '/list':\n",
    "            print(\"Users:\")\n",
    "            for user in users.keys():\n",
    "                marker = \" (current)\" if user == current_user else \"\"\n",
    "                print(f\"  - {user}{marker}\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        # Send message for current user\n",
    "        try:\n",
    "            chat = users[current_user]\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"Bot: {response.text}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_multi_user_chat()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Dictionary stores separate chat sessions per user\n",
    "- Each user has independent conversation history\n",
    "- Command parsing with `split()` extracts username\n",
    "- This architecture scales to many users\n",
    "- Similar to how real chat platforms manage multiple conversations\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: RAG-Enabled Chat\n",
    "\n",
    "**Task**: Create a chat application where user messages are first used to search a \"knowledge base\" (a list of facts), and relevant facts are added to the prompt before sending to Gemini.\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Knowledge base: [\n",
    "  \"Python was created by Guido van Rossum\",\n",
    "  \"Python was released in 1991\",\n",
    "  \"Django is a Python web framework\"\n",
    "]\n",
    "\n",
    "User: Who created Python?\n",
    "System: [Finds relevant fact: \"Python was created by Guido van Rossum\"]\n",
    "Bot: Python was created by Guido van Rossum.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def search_knowledge_base(query, knowledge_base):\n",
    "    \"\"\"Simple keyword-based search of knowledge base\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    relevant_facts = []\n",
    "    \n",
    "    for fact in knowledge_base:\n",
    "        # Check if any query word appears in the fact\n",
    "        if any(word in fact.lower() for word in query_lower.split()):\n",
    "            relevant_facts.append(fact)\n",
    "    \n",
    "    return relevant_facts\n",
    "\n",
    "def run_rag_chat():\n",
    "    # Knowledge base\n",
    "    knowledge_base = [\n",
    "        \"Python was created by Guido van Rossum in 1991.\",\n",
    "        \"Python is known for its simple and readable syntax.\",\n",
    "        \"Django is a popular Python web framework used by Instagram and Pinterest.\",\n",
    "        \"Flask is a lightweight Python web framework good for small projects.\",\n",
    "        \"NumPy and Pandas are essential Python libraries for data science.\",\n",
    "    ]\n",
    "    \n",
    "    # Create model with RAG instructions\n",
    "    rag_model = genai.GenerativeModel(\n",
    "        'gemini-pro',\n",
    "        system_instruction=\"You are a helpful assistant. Use the provided context to answer questions accurately. If the context doesn't contain relevant information, say so.\"\n",
    "    )\n",
    "    \n",
    "    chat = rag_model.start_chat(history=[])\n",
    "    \n",
    "    print(\"RAG-Enhanced Chat\")\n",
    "    print(\"Ask questions about Python!\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Search knowledge base\n",
    "            relevant_facts = search_knowledge_base(user_input, knowledge_base)\n",
    "            \n",
    "            # Build prompt with context\n",
    "            if relevant_facts:\n",
    "                context = \"\\n\".join(f\"- {fact}\" for fact in relevant_facts)\n",
    "                prompt = f\"Context:\\n{context}\\n\\nQuestion: {user_input}\"\n",
    "                print(f\"[Found {len(relevant_facts)} relevant fact(s)]\")\n",
    "            else:\n",
    "                prompt = user_input\n",
    "                print(\"[No relevant facts found in knowledge base]\")\n",
    "            \n",
    "            # Send to AI\n",
    "            response = chat.send_message(prompt)\n",
    "            print(f\"\\nBot: {response.text}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_rag_chat()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- This is a simplified RAG (Retrieval-Augmented Generation) system\n",
    "- We search the knowledge base for relevant facts using keyword matching\n",
    "- Relevant facts are injected into the prompt as context\n",
    "- The AI uses this context to provide accurate, grounded answers\n",
    "- Real RAG systems use embeddings and vector search, but the principle is the same\n",
    "- This pattern powers applications like \"chat with your documents\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Chat Analytics Dashboard\n",
    "\n",
    "**Task**: Create a chat application that tracks analytics:\n",
    "- Total messages exchanged\n",
    "- Average message length (user and bot separately)\n",
    "- Most common words used by user\n",
    "- Add a '/stats' command to display these analytics\n",
    "\n",
    "**Expected Output**: Working chat with analytics dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_stats(chat):\n",
    "    \"\"\"Calculate conversation statistics\"\"\"\n",
    "    user_messages = []\n",
    "    bot_messages = []\n",
    "    all_user_words = []\n",
    "    \n",
    "    for msg in chat.history:\n",
    "        text = msg.parts[0].text\n",
    "        \n",
    "        if msg.role == \"user\":\n",
    "            user_messages.append(text)\n",
    "            # Collect words (lowercase, no punctuation)\n",
    "            words = text.lower().split()\n",
    "            all_user_words.extend(words)\n",
    "        else:\n",
    "            bot_messages.append(text)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_user_length = sum(len(m) for m in user_messages) / len(user_messages) if user_messages else 0\n",
    "    avg_bot_length = sum(len(m) for m in bot_messages) / len(bot_messages) if bot_messages else 0\n",
    "    \n",
    "    # Find most common words (exclude very short words)\n",
    "    meaningful_words = [w for w in all_user_words if len(w) > 3]\n",
    "    common_words = Counter(meaningful_words).most_common(5)\n",
    "    \n",
    "    return {\n",
    "        'total_messages': len(chat.history),\n",
    "        'user_messages': len(user_messages),\n",
    "        'bot_messages': len(bot_messages),\n",
    "        'avg_user_length': avg_user_length,\n",
    "        'avg_bot_length': avg_bot_length,\n",
    "        'common_words': common_words\n",
    "    }\n",
    "\n",
    "def display_stats(stats):\n",
    "    \"\"\"Display formatted statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 CONVERSATION ANALYTICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total Messages: {stats['total_messages']}\")\n",
    "    print(f\"  - User: {stats['user_messages']}\")\n",
    "    print(f\"  - Bot: {stats['bot_messages']}\")\n",
    "    print(f\"\\nAverage Message Length:\")\n",
    "    print(f\"  - User: {stats['avg_user_length']:.1f} characters\")\n",
    "    print(f\"  - Bot: {stats['avg_bot_length']:.1f} characters\")\n",
    "    print(f\"\\nMost Common Words (User):\")\n",
    "    for word, count in stats['common_words']:\n",
    "        print(f\"  - {word}: {count} times\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "def run_chat_with_analytics():\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"Chat with Analytics\")\n",
    "    print(\"Commands: /stats - view analytics, /quit - exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == '/quit':\n",
    "            # Show final stats before exiting\n",
    "            if chat.history:\n",
    "                stats = calculate_stats(chat)\n",
    "                display_stats(stats)\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == '/stats':\n",
    "            if chat.history:\n",
    "                stats = calculate_stats(chat)\n",
    "                display_stats(stats)\n",
    "            else:\n",
    "                print(\"No conversation history yet!\\n\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"\\nBot: {response.text}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "run_chat_with_analytics()\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- We analyze the chat history to extract insights\n",
    "- `Counter` from collections makes word frequency analysis easy\n",
    "- Statistics help understand conversation patterns and user engagement\n",
    "- This is valuable for improving chatbot design and user experience\n",
    "- Real applications track metrics like response time, user satisfaction, conversation length\n",
    "- Analytics inform decisions about bot personality, response length, and feature additions\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
