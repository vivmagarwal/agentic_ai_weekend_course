{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **Prompts and Templates in LangChain**\n\n## **Learning Objectives**\nBy the end of this notebook, you will be able to:\n- Create reusable prompt templates with variables\n- Use ChatPromptTemplate for structured conversations\n- Implement few-shot prompting for better results\n- Work with MessagesPlaceholder for dynamic content\n- Build prompt libraries for different use cases\n- Apply advanced prompt engineering techniques\n\n## **Why This Matters: Production-Ready AI Applications**\n\n**In Enterprise Applications:**\n- Prompt templates ensure consistency across teams\n- Variables enable dynamic, personalized responses\n- Version control for prompts becomes possible\n\n**In AI Products:**\n- Few-shot learning improves accuracy without fine-tuning\n- Templates make prompts maintainable and testable\n- Separation of prompt logic from application code\n\n**In Development:**\n- Rapid iteration on prompt designs\n- A/B testing different prompt strategies\n- Reusable components across projects\n\n## **Prerequisites**\n- Completed notebooks 00 and 01\n- Understanding of basic LLM calls\n- Familiarity with Python string formatting"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Setup: Install and Import Dependencies**\n\nRun this cell first to set up your environment:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai python-dotenv\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    FewShotPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Verify setup\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ Environment ready! Let's work with prompts and templates.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please set your OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 1: Basic Prompt Templates**\n\n**Concept**: Prompt templates allow you to create reusable prompts with variable substitution, making your code more maintainable and flexible.\n\n### **Example 1: Simple PromptTemplate**\n\n**Problem**: Create a reusable prompt with variables\n**Expected Output**: Dynamic prompt generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# **Create a simple template**\ntemplate = PromptTemplate(\n    input_variables=[\"topic\", \"audience\"],\n    template=\"\"\"Explain {topic} in simple terms suitable for {audience}.\n    \n    Make sure your explanation:\n    - Uses appropriate language for the audience\n    - Includes relevant examples\n    - Is engaging and clear\"\"\"\n)\n\n# **Format the template with different values**\nprompt1 = template.format(topic=\"machine learning\", audience=\"5th graders\")\nprompt2 = template.format(topic=\"machine learning\", audience=\"executives\")\n\nprint(\"Prompt for 5th graders:\")\nprint(\"=\" * 50)\nprint(prompt1)\nprint(\"\\nPrompt for executives:\")\nprint(\"=\" * 50)\nprint(prompt2)\n\n# **Use with LLM**\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nresponse = llm.invoke(prompt1)\nprint(\"\\nLLM Response (for 5th graders):\")\nprint(\"=\" * 50)\nprint(response.content[:300] + \"...\")\n```\n\n**Why templates are powerful:**\n- Reuse the same structure with different inputs\n- Maintain consistency across your application\n- Easy to update prompts in one place\n- Variables make prompts dynamic\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Template with Validation**\n\n**Problem**: Create templates with input validation and formatting\n**Expected Output**: Safe, validated prompt generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import PromptTemplate\nfrom typing import Dict, Any\n\n# **Template with partial variables and validation**\ndef create_code_review_template():\n    \"\"\"Create a code review template with validation\"\"\"\n    \n    template = PromptTemplate(\n        input_variables=[\"language\", \"code\", \"focus_areas\"],\n        template=\"\"\"You are an expert {language} code reviewer.\n        \nPlease review the following code:\n\n```{language}\n{code}\n```\n\nFocus on these areas:\n{focus_areas}\n\nProvide:\n1. Issues found (if any)\n2. Suggestions for improvement\n3. Best practices to follow\n4. Overall assessment\"\"\",\n        validate_template=True  # Ensure all variables are used\n    )\n    \n    return template\n\n# **Create template with validation**\nreview_template = create_code_review_template()\n\n# **Example with Python code**\npython_code = \"\"\"\ndef calculate_average(numbers):\n    sum = 0\n    for n in numbers:\n        sum = sum + n\n    average = sum / len(numbers)\n    return average\n\"\"\"\n\n# **Format with specific focus areas**\nreview_prompt = review_template.format(\n    language=\"Python\",\n    code=python_code,\n    focus_areas=\"- Error handling\\n- Variable naming\\n- Pythonic style\\n- Performance\"\n)\n\nprint(\"Generated Code Review Prompt:\")\nprint(\"=\" * 50)\nprint(review_prompt[:400] + \"...\")\n\n# **Use with LLM**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\nreview = llm.invoke(review_prompt)\nprint(\"\\nCode Review Results:\")\nprint(\"=\" * 50)\nprint(review.content[:500] + \"...\")\n```\n\n**Key benefits of validation:**\n- Catches missing variables early\n- Ensures template consistency\n- Prevents runtime errors\n- Makes templates more maintainable\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Partial Templates**\n\n**Problem**: Create templates with some variables pre-filled\n**Expected Output**: Partially configured templates"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import PromptTemplate\nfrom datetime import datetime\n\n# **Template with both dynamic and partial variables**\nbase_template = PromptTemplate(\n    input_variables=[\"task\", \"context\"],\n    partial_variables={\"date\": datetime.now().strftime(\"%Y-%m-%d\"), \"version\": \"1.0\"},\n    template=\"\"\"[Report Date: {date}]\n[Template Version: {version}]\n\nTask: {task}\n\nContext:\n{context}\n\nPlease provide a detailed analysis including:\n- Current situation assessment\n- Recommended actions\n- Expected outcomes\n- Risk factors\"\"\"\n)\n\n# **Create specialized versions with partial variables**\nmarketing_template = base_template.partial(\n    context=\"Marketing department quarterly review\"\n)\n\nengineering_template = base_template.partial(\n    context=\"Engineering team sprint retrospective\"\n)\n\n# **Now only need to provide 'task' variable**\nmarketing_prompt = marketing_template.format(\n    task=\"Analyze social media campaign performance\"\n)\n\nengineering_prompt = engineering_template.format(\n    task=\"Review deployment pipeline efficiency\"\n)\n\nprint(\"Marketing Template Output:\")\nprint(\"=\" * 50)\nprint(marketing_prompt)\nprint(\"\\nEngineering Template Output:\")\nprint(\"=\" * 50)\nprint(engineering_prompt)\n\nprint(\"\\nüí° Partial templates allow creating specialized versions!\")\n```\n\n**Partial template use cases:**\n- Department-specific templates\n- Templates with timestamps\n- Configuration-based templates\n- Multi-tenant applications\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 1: Practice with Basic Templates**\n\n**Practice Focus**: Create and use various prompt templates\n\n### **Exercise 1: Create a Product Description Template**\n\n**Task**: Build a template for generating product descriptions\n**Expected Output**: Dynamic product descriptions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a PromptTemplate with variables:\n",
    "# - product_name\n",
    "# - features (list of features)\n",
    "# - target_audience\n",
    "# Generate descriptions for 2 different products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# **Create product description template**\nproduct_template = PromptTemplate(\n    input_variables=[\"product_name\", \"features\", \"target_audience\"],\n    template=\"\"\"Create an engaging product description for {product_name}.\n\nKey Features:\n{features}\n\nTarget Audience: {target_audience}\n\nWrite a compelling 3-paragraph description that:\n1. Hooks the reader with benefits\n2. Highlights the key features\n3. Includes a call to action\n\nUse language appropriate for the target audience.\"\"\"\n)\n\n# **Test with different products**\nproducts = [\n    {\n        \"product_name\": \"SmartFit Pro Watch\",\n        \"features\": \"\"\"- Heart rate monitoring\n- GPS tracking\n- 7-day battery life\n- Water resistant to 50m\n- Sleep tracking\"\"\",\n        \"target_audience\": \"fitness enthusiasts aged 25-45\"\n    },\n    {\n        \"product_name\": \"KiddoLearn Tablet\",\n        \"features\": \"\"\"- Parental controls\n- Educational apps pre-installed\n- Shatterproof screen\n- 10-hour battery\n- Age-appropriate content filtering\"\"\",\n        \"target_audience\": \"parents of children aged 5-10\"\n    }\n]\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n\nfor product in products:\n    # Format the template\n    prompt = product_template.format(**product)\n    \n    # Generate description\n    response = llm.invoke(prompt)\n    \n    print(f\"\\nüì¶ {product['product_name']} Description:\")\n    print(\"=\" * 50)\n    print(response.content[:400] + \"...\\n\")\n\nprint(\"‚úÖ Templates make it easy to generate consistent product descriptions!\")\n```\n\n**What you learned:**\n- Templates ensure consistent structure\n- Variables make content dynamic\n- Same template works for different products\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Exercise 2: Build a Template with Validation**\n\n**Task**: Create a template that validates inputs\n**Expected Output**: Error messages for invalid inputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a template for email generation that:\n",
    "# - Validates that all required variables are present\n",
    "# - Has variables: recipient_name, sender_name, subject, main_content\n",
    "# - Test with both valid and invalid inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import PromptTemplate\n\ndef create_email_template():\n    \"\"\"Create an email template with validation\"\"\"\n    \n    template = PromptTemplate(\n        input_variables=[\"recipient_name\", \"sender_name\", \"subject\", \"main_content\"],\n        template=\"\"\"Draft a professional email with the following details:\n\nTo: {recipient_name}\nFrom: {sender_name}\nSubject: {subject}\n\nMain Content:\n{main_content}\n\nFormat the email with:\n- Appropriate greeting\n- Clear and concise body\n- Professional closing\n- Signature with {sender_name}\"\"\",\n        validate_template=True\n    )\n    return template\n\n# **Create the template**\nemail_template = create_email_template()\n\n# **Test with valid inputs**\ntry:\n    valid_email = email_template.format(\n        recipient_name=\"John Smith\",\n        sender_name=\"Sarah Johnson\",\n        subject=\"Project Update\",\n        main_content=\"Update on Q4 deliverables and timeline adjustments\"\n    )\n    print(\"‚úÖ Valid template formatting succeeded!\")\n    print(\"=\" * 50)\n    print(valid_email[:200] + \"...\")\nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")\n\n# **Test with missing variable (will fail)**\nprint(\"\\nTesting with missing variable:\")\ntry:\n    invalid_email = email_template.format(\n        recipient_name=\"John Smith\",\n        sender_name=\"Sarah Johnson\",\n        # Missing: subject and main_content\n    )\nexcept KeyError as e:\n    print(f\"‚ùå Validation caught missing variable: {e}\")\n    print(\"This is good - template validation works!\")\n\nprint(\"\\nüí° Template validation helps catch errors early!\")\n```\n\n**Key takeaway:**\n- Validation prevents runtime errors\n- Catches missing variables immediately\n- Makes debugging easier\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 2: ChatPromptTemplate for Conversations**\n\n**Concept**: ChatPromptTemplate is specifically designed for chat models, providing structured conversation templates.\n\n### **Example 1: Basic ChatPromptTemplate**\n\n**Problem**: Create structured chat conversations\n**Expected Output**: Properly formatted chat messages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# **Create a chat prompt template**\nchat_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a {role} expert. Your tone should be {tone}.\"),\n    (\"human\", \"Explain {concept} to me.\"),\n    (\"ai\", \"I'd be happy to explain {concept}. Let me break it down for you.\"),\n    (\"human\", \"{follow_up_question}\")\n])\n\n# **Format with specific values**\nmessages = chat_template.format_messages(\n    role=\"data science\",\n    tone=\"friendly and encouraging\",\n    concept=\"neural networks\",\n    follow_up_question=\"Can you give me a real-world example?\"\n)\n\nprint(\"Generated Chat Messages:\")\nprint(\"=\" * 50)\nfor msg in messages:\n    print(f\"{msg.__class__.__name__}: {msg.content[:100]}...\")\n\n# **Use with LLM**\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nresponse = llm.invoke(messages)\n\nprint(\"\\nLLM Response:\")\nprint(\"=\" * 50)\nprint(response.content[:400] + \"...\")\n```\n\n**Why ChatPromptTemplate is powerful:**\n- Maintains proper message structure\n- Clear role separation\n- Supports conversation flow\n- Type-safe message handling\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Dynamic Conversations with MessagesPlaceholder**\n\n**Problem**: Create templates that can include dynamic conversation history\n**Expected Output**: Flexible conversation templates"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\n\n# **Template with dynamic message history**\nprompt_with_history = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are a helpful programming tutor. \n    Remember what the student has told you and build upon previous explanations.\"\"\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"human\", \"{current_question}\")\n])\n\n# **Example conversation history**\nhistory = [\n    HumanMessage(content=\"I'm learning Python and struggling with loops.\"),\n    AIMessage(content=\"I understand you're having trouble with loops. They're fundamental in Python. Let me help you understand them better.\"),\n    HumanMessage(content=\"I specifically don't understand the difference between 'for' and 'while' loops.\"),\n    AIMessage(content=\"Great question! 'for' loops are used when you know how many times to iterate, while 'while' loops continue until a condition becomes false.\")\n]\n\n# **Format with history and new question**\nmessages = prompt_with_history.format_messages(\n    chat_history=history,\n    current_question=\"Can you show me an example where I would choose 'while' over 'for'?\"\n)\n\nprint(\"Conversation with Dynamic History:\")\nprint(\"=\" * 50)\nprint(f\"Total messages in conversation: {len(messages)}\")\nprint(f\"\\nLast 3 messages:\")\nfor msg in messages[-3:]:\n    msg_type = msg.__class__.__name__\n    content_preview = msg.content[:80] + \"...\" if len(msg.content) > 80 else msg.content\n    print(f\"  {msg_type}: {content_preview}\")\n\n# **Get response**\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nresponse = llm.invoke(messages)\n\nprint(\"\\nAI Response (with context):\")\nprint(\"=\" * 50)\nprint(response.content[:400] + \"...\")\n\nprint(\"\\nüí° MessagesPlaceholder enables dynamic conversation management!\")\n```\n\n**MessagesPlaceholder benefits:**\n- Inject conversation history dynamically\n- Flexible conversation management\n- Maintains context across turns\n- Essential for chatbots\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 3: Multi-Modal Templates**\n\n**Problem**: Create templates that combine different message types\n**Expected Output**: Complex conversation structures"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\n\n# **Complex template with multiple components**\nmulti_modal_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are a {assistant_type} assistant.\n    Your expertise: {expertise_areas}\n    Communication style: {communication_style}\"\"\"),\n    (\"human\", \"Here's my situation: {user_context}\"),\n    (\"ai\", \"I understand your situation. Let me help you with that.\"),\n    MessagesPlaceholder(variable_name=\"examples\", optional=True),\n    (\"human\", \"My specific question: {question}\"),\n    (\"system\", \"Remember to {special_instructions}\")\n])\n\n# **Create a customer service scenario**\ncs_messages = multi_modal_template.format_messages(\n    assistant_type=\"customer service\",\n    expertise_areas=\"product returns, warranty claims, technical support\",\n    communication_style=\"empathetic and solution-focused\",\n    user_context=\"I bought a laptop 2 months ago and the screen stopped working\",\n    examples=[],  # Could include example interactions\n    question=\"What are my options for getting this fixed?\",\n    special_instructions=\"provide clear options with pros and cons\"\n)\n\n# **Create a technical support scenario**\ntech_messages = multi_modal_template.format_messages(\n    assistant_type=\"technical support\",\n    expertise_areas=\"Python, debugging, performance optimization\",\n    communication_style=\"technical but accessible\",\n    user_context=\"My Python script is running very slowly with large datasets\",\n    examples=[],\n    question=\"How can I profile and optimize my code?\",\n    special_instructions=\"include code examples and step-by-step instructions\"\n)\n\nprint(\"Multi-Modal Template Examples:\")\nprint(\"=\" * 50)\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\n# **Customer service response**\nprint(\"\\nüõçÔ∏è Customer Service Response:\")\ncs_response = llm.invoke(cs_messages)\nprint(cs_response.content[:300] + \"...\")\n\n# **Technical support response**\nprint(\"\\nüíª Technical Support Response:\")\ntech_response = llm.invoke(tech_messages)\nprint(tech_response.content[:300] + \"...\")\n\nprint(\"\\n‚úÖ Same template, different personalities and expertise!\")\n```\n\n**Multi-modal template advantages:**\n- Combine system, human, and AI messages\n- Optional components with MessagesPlaceholder\n- Highly customizable conversations\n- Reusable across different domains\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 2: Practice with ChatPromptTemplate**\n\n**Practice Focus**: Build conversation templates for different scenarios\n\n### **Exercise 1: Create a Teaching Assistant Template**\n\n**Task**: Build a template for a teaching assistant that remembers context\n**Expected Output**: Context-aware tutoring responses"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a ChatPromptTemplate that:\n",
    "# - Has a system message defining the teaching assistant role\n",
    "# - Uses MessagesPlaceholder for conversation history\n",
    "# - Takes subject and difficulty_level as variables\n",
    "# - Test with a math question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\n\n# **Create teaching assistant template**\nteaching_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are a {subject} teaching assistant.\n    Difficulty level: {difficulty_level}\n    \n    Your approach:\n    - Break down complex concepts into simple steps\n    - Use examples relevant to the difficulty level\n    - Check understanding with questions\n    - Be encouraging and patient\"\"\"),\n    MessagesPlaceholder(variable_name=\"conversation_history\"),\n    (\"human\", \"{student_question}\")\n])\n\n# **Simulate a learning session**\nconversation_history = [\n    HumanMessage(content=\"I'm struggling with algebra.\"),\n    AIMessage(content=\"I'm here to help you with algebra! What specific topic are you finding challenging?\")\n]\n\n# **Format the template**\nmessages = teaching_template.format_messages(\n    subject=\"Mathematics\",\n    difficulty_level=\"High School\",\n    conversation_history=conversation_history,\n    student_question=\"Can you explain how to solve quadratic equations?\"\n)\n\n# **Get response**\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\nresponse = llm.invoke(messages)\n\nprint(\"üéì Teaching Assistant Response:\")\nprint(\"=\" * 50)\nprint(response.content[:500] + \"...\")\n\n# **Add to history and ask follow-up**\nconversation_history.extend([\n    HumanMessage(content=\"Can you explain how to solve quadratic equations?\"),\n    AIMessage(content=response.content)\n])\n\n# **Follow-up question**\nfollow_up_messages = teaching_template.format_messages(\n    subject=\"Mathematics\",\n    difficulty_level=\"High School\",\n    conversation_history=conversation_history,\n    student_question=\"What's the quadratic formula?\"\n)\n\nfollow_up_response = llm.invoke(follow_up_messages)\nprint(\"\\nüìö Follow-up Response (with context):\")\nprint(follow_up_response.content[:300] + \"...\")\n\nprint(\"\\n‚úÖ The assistant remembers the conversation context!\")\n```\n\n**What you learned:**\n- ChatPromptTemplate structures conversations\n- MessagesPlaceholder enables history\n- Context improves response relevance\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 3: Few-Shot Prompting**\n\n**Concept**: Few-shot prompting teaches the LLM by example, improving accuracy for specific tasks without fine-tuning.\n\n### **Example 1: Basic Few-Shot Template**\n\n**Problem**: Teach the LLM a specific format through examples\n**Expected Output**: Responses following the demonstrated pattern"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# **Define examples of the pattern we want**\nexamples = [\n    {\n        \"input\": \"The movie was fantastic! Best film I've seen all year.\",\n        \"sentiment\": \"positive\",\n        \"confidence\": \"high\",\n        \"keywords\": \"fantastic, best\"\n    },\n    {\n        \"input\": \"It was okay, nothing special but not terrible.\",\n        \"sentiment\": \"neutral\",\n        \"confidence\": \"high\",\n        \"keywords\": \"okay, nothing special\"\n    },\n    {\n        \"input\": \"Completely disappointed. Waste of time and money.\",\n        \"sentiment\": \"negative\",\n        \"confidence\": \"high\",\n        \"keywords\": \"disappointed, waste\"\n    },\n    {\n        \"input\": \"I'm not sure how I feel about it. It had good and bad parts.\",\n        \"sentiment\": \"neutral\",\n        \"confidence\": \"low\",\n        \"keywords\": \"not sure, good and bad\"\n    }\n]\n\n# **Create template for examples**\nexample_template = PromptTemplate(\n    input_variables=[\"input\", \"sentiment\", \"confidence\", \"keywords\"],\n    template=\"\"\"Input: {input}\nSentiment: {sentiment}\nConfidence: {confidence}\nKeywords: {keywords}\"\"\"\n)\n\n# **Create few-shot template**\nfew_shot_prompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_template,\n    prefix=\"Analyze the sentiment of text with confidence level and key words.\\n\",\n    suffix=\"\\nInput: {input}\\nSentiment:\",\n    input_variables=[\"input\"]\n)\n\n# **Test with new inputs**\ntest_inputs = [\n    \"This product exceeded all my expectations! Absolutely love it!\",\n    \"The service was slow and the food was cold.\"\n]\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\nprint(\"Few-Shot Sentiment Analysis:\")\nprint(\"=\" * 50)\n\nfor test_input in test_inputs:\n    prompt = few_shot_prompt.format(input=test_input)\n    response = llm.invoke(prompt)\n    \n    print(f\"\\nüìù Input: {test_input}\")\n    print(f\"ü§ñ Analysis:\\n{response.content}\")\n\nprint(\"\\nüí° Few-shot learning teaches consistent output format!\")\n```\n\n**Why few-shot prompting works:**\n- LLM learns from examples\n- Ensures consistent output format\n- No fine-tuning required\n- Improves accuracy for specific tasks\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **Example 2: Dynamic Example Selection**\n\n**Problem**: Select relevant examples based on input\n**Expected Output**: Context-appropriate examples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# **Examples for different types of writing**\nwriting_examples = [\n    {\"style\": \"formal\", \"input\": \"Hello\", \"output\": \"Good day. I hope this message finds you well.\"},\n    {\"style\": \"formal\", \"input\": \"Thanks\", \"output\": \"I would like to express my sincere gratitude.\"},\n    {\"style\": \"casual\", \"input\": \"Hello\", \"output\": \"Hey there! How's it going?\"},\n    {\"style\": \"casual\", \"input\": \"Thanks\", \"output\": \"Thanks a bunch! Really appreciate it!\"},\n    {\"style\": \"professional\", \"input\": \"Hello\", \"output\": \"Hello, I hope you're having a productive day.\"},\n    {\"style\": \"professional\", \"input\": \"Thanks\", \"output\": \"Thank you for your assistance with this matter.\"},\n]\n\n# **Example template**\nexample_prompt = PromptTemplate(\n    input_variables=[\"style\", \"input\", \"output\"],\n    template=\"Style: {style}\\nInput: {input}\\nOutput: {output}\"\n)\n\n# **Dynamic selector (selects based on length to fit token limits)**\nexample_selector = LengthBasedExampleSelector(\n    examples=writing_examples,\n    example_prompt=example_prompt,\n    max_length=100,  # Maximum length in words\n)\n\n# **Create dynamic few-shot template**\ndynamic_prompt = FewShotPromptTemplate(\n    example_selector=example_selector,\n    example_prompt=example_prompt,\n    prefix=\"Transform text into different writing styles based on examples:\\n\",\n    suffix=\"\\nStyle: {style}\\nInput: {input}\\nOutput:\",\n    input_variables=[\"style\", \"input\"]\n)\n\n# **Test with different styles**\ntest_cases = [\n    {\"style\": \"formal\", \"input\": \"I disagree\"},\n    {\"style\": \"casual\", \"input\": \"I disagree\"},\n    {\"style\": \"professional\", \"input\": \"I disagree\"}\n]\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n\nprint(\"Dynamic Few-Shot Style Transfer:\")\nprint(\"=\" * 50)\n\nfor test in test_cases:\n    prompt = dynamic_prompt.format(**test)\n    response = llm.invoke(prompt)\n    \n    print(f\"\\nüé® Style: {test['style']}\")\n    print(f\"üì• Input: {test['input']}\")\n    print(f\"üì§ Output: {response.content}\")\n\nprint(\"\\n‚úÖ Dynamic example selection optimizes token usage!\")\n```\n\n**Dynamic selection benefits:**\n- Fits within token limits\n- Selects most relevant examples\n- Optimizes performance\n- Reduces costs\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 3: Practice Few-Shot Prompting**\n\n**Practice Focus**: Create few-shot templates for specific tasks\n\n### **Exercise 1: Build a Data Extraction Template**\n\n**Task**: Create a few-shot template for extracting information\n**Expected Output**: Structured data extraction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create a few-shot template that extracts:\n",
    "# - Product name\n",
    "# - Price\n",
    "# - Rating\n",
    "# From product review text\n",
    "# Provide 3 examples and test with new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# **Examples of product review extraction**\nextraction_examples = [\n    {\n        \"text\": \"I bought the UltraSound Pro headphones for $199 and they're amazing! Definitely worth 5 stars.\",\n        \"product\": \"UltraSound Pro headphones\",\n        \"price\": \"$199\",\n        \"rating\": \"5 stars\"\n    },\n    {\n        \"text\": \"The QuickBlend mixer at $89 is okay. Works fine but nothing special. 3 out of 5.\",\n        \"product\": \"QuickBlend mixer\",\n        \"price\": \"$89\",\n        \"rating\": \"3 out of 5\"\n    },\n    {\n        \"text\": \"Waste of money! The TechGadget X for $450 broke after a week. 1 star!\",\n        \"product\": \"TechGadget X\",\n        \"price\": \"$450\",\n        \"rating\": \"1 star\"\n    }\n]\n\n# **Template for examples**\nexample_template = PromptTemplate(\n    input_variables=[\"text\", \"product\", \"price\", \"rating\"],\n    template=\"\"\"Review: {text}\nExtracted Data:\n- Product: {product}\n- Price: {price}\n- Rating: {rating}\"\"\"\n)\n\n# **Create few-shot template**\nextraction_prompt = FewShotPromptTemplate(\n    examples=extraction_examples,\n    example_prompt=example_template,\n    prefix=\"Extract product information from reviews:\\n\",\n    suffix=\"\\nReview: {text}\\nExtracted Data:\",\n    input_variables=[\"text\"]\n)\n\n# **Test with new reviews**\ntest_reviews = [\n    \"Just got the SmartWatch Ultra for $299. It's fantastic! Easy 5 star rating from me.\",\n    \"The CoffeMaker Pro costs $159 and makes decent coffee. I'd give it 4 stars overall.\"\n]\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\nprint(\"Product Information Extraction:\")\nprint(\"=\" * 50)\n\nfor review in test_reviews:\n    prompt = extraction_prompt.format(text=review)\n    response = llm.invoke(prompt)\n    \n    print(f\"\\nüìù Review: {review}\")\n    print(f\"üìä Extracted:\\n{response.content}\")\n\nprint(\"\\n‚úÖ Few-shot prompting enables consistent data extraction!\")\n```\n\n**What you learned:**\n- Few-shot examples teach extraction patterns\n- Consistent output format\n- Works for various data types\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Optional Extra Practice**\n\n### **Challenge 1: Build a Multi-Purpose Template System**\n\n**Task**: Create a template factory for different business documents\n**Expected Output**: Reusable template system"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom typing import Dict, Any\nfrom enum import Enum\n\nclass DocumentType(Enum):\n    EMAIL = \"email\"\n    REPORT = \"report\"\n    PROPOSAL = \"proposal\"\n    MEMO = \"memo\"\n\nclass TemplateFactory:\n    \"\"\"Factory for creating different document templates\"\"\"\n    \n    def __init__(self):\n        self.templates = {\n            DocumentType.EMAIL: self._create_email_template(),\n            DocumentType.REPORT: self._create_report_template(),\n            DocumentType.PROPOSAL: self._create_proposal_template(),\n            DocumentType.MEMO: self._create_memo_template()\n        }\n    \n    def _create_email_template(self) -> ChatPromptTemplate:\n        return ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a professional email writer.\"),\n            (\"human\", \"\"\"Write an email with:\n            To: {recipient}\n            Subject: {subject}\n            Purpose: {purpose}\n            Tone: {tone}\"\"\")\n        ])\n    \n    def _create_report_template(self) -> ChatPromptTemplate:\n        return ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a business analyst creating professional reports.\"),\n            (\"human\", \"\"\"Create a report on:\n            Topic: {topic}\n            Data: {data}\n            Recommendations needed: {recommendations}\n            Length: {length} pages\"\"\")\n        ])\n    \n    def _create_proposal_template(self) -> ChatPromptTemplate:\n        return ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a business development expert.\"),\n            (\"human\", \"\"\"Create a proposal for:\n            Client: {client}\n            Service/Product: {offering}\n            Budget: {budget}\n            Timeline: {timeline}\"\"\")\n        ])\n    \n    def _create_memo_template(self) -> ChatPromptTemplate:\n        return ChatPromptTemplate.from_messages([\n            (\"system\", \"You are writing internal company communications.\"),\n            (\"human\", \"\"\"Write a memo about:\n            Topic: {topic}\n            Audience: {audience}\n            Action items: {actions}\n            Urgency: {urgency}\"\"\")\n        ])\n    \n    def generate_document(self, doc_type: DocumentType, **kwargs) -> str:\n        \"\"\"Generate a document using the appropriate template\"\"\"\n        template = self.templates[doc_type]\n        messages = template.format_messages(**kwargs)\n        \n        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n        response = llm.invoke(messages)\n        return response.content\n\n# **Use the template factory**\nfactory = TemplateFactory()\n\n# **Generate different documents**\nprint(\"Template Factory System:\")\nprint(\"=\" * 50)\n\n# **Email**\nemail = factory.generate_document(\n    DocumentType.EMAIL,\n    recipient=\"team@company.com\",\n    subject=\"Project Update\",\n    purpose=\"inform about milestone completion\",\n    tone=\"professional but friendly\"\n)\nprint(\"\\nüìß Email:\")\nprint(email[:300] + \"...\")\n\n# **Memo**\nmemo = factory.generate_document(\n    DocumentType.MEMO,\n    topic=\"New Remote Work Policy\",\n    audience=\"all employees\",\n    actions=\"review policy, submit feedback by Friday\",\n    urgency=\"high\"\n)\nprint(\"\\nüìù Memo:\")\nprint(memo[:300] + \"...\")\n\nprint(\"\\n‚úÖ Template factory enables scalable document generation!\")\n```\n\n**Factory pattern benefits:**\n- Centralized template management\n- Easy to add new document types\n- Consistent interface\n- Reusable across applications\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## **Summary & Next Steps**\n\n### **What You've Learned**\n‚úÖ Creating reusable PromptTemplates with variables  \n‚úÖ Using ChatPromptTemplate for structured conversations  \n‚úÖ Working with MessagesPlaceholder for dynamic content  \n‚úÖ Implementing few-shot prompting for better results  \n‚úÖ Building template libraries and factories  \n‚úÖ Partial templates and validation techniques  \n\n### **Key Takeaways**\n1. **Templates ensure consistency** - Maintain quality across your application\n2. **Variables make prompts dynamic** - One template, many uses\n3. **Few-shot learning is powerful** - Teach by example without fine-tuning\n4. **ChatPromptTemplate for chat models** - Proper message structure matters\n5. **MessagesPlaceholder enables flexibility** - Dynamic conversation management\n\n### **What's Next?**\nIn the next notebook (`03_chains_and_lcel.ipynb`), you'll learn:\n- Building chains with LangChain Expression Language (LCEL)\n- The pipe operator for component composition\n- Creating complex workflows\n- Parallel and sequential execution\n- Error handling in chains\n\n### **Resources**\n- [LangChain Prompts Documentation](https://python.langchain.com/docs/modules/model_io/prompts/)\n- [Few-Shot Prompting Guide](https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples)\n- [ChatPromptTemplate API](https://python.langchain.com/docs/modules/model_io/prompts/message_prompts)\n- [Prompt Engineering Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n\n---\n\nüéâ **Congratulations!** You've mastered prompts and templates in LangChain! You can now build maintainable, reusable prompt systems for production applications."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}