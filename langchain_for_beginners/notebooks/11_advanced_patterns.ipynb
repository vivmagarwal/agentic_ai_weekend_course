{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a87c73",
   "metadata": {},
   "source": "# **11 - Advanced Patterns: Expert-Level LangChain Techniques**\n\n## **Overview**\nIn this notebook, we'll explore advanced patterns and techniques for building sophisticated AI applications. You'll learn about routing, fallbacks, retries, conditional logic, and complex orchestration patterns.\n\n## **Learning Objectives**\nBy the end of this notebook, you will be able to:\n- Implement dynamic routing based on input\n- Build fallback chains for reliability\n- Create retry mechanisms with exponential backoff\n- Design conditional workflows\n- Implement map-reduce patterns\n- Build self-correcting systems\n\n## **Prerequisites**\n- Completion of notebooks 01-10\n- Strong understanding of chains and agents\n- Familiarity with async programming\n\n## **Back-and-Forth Teaching Pattern**\nThis notebook follows our pattern:\n1. **Instructor Activity**: Demonstrates a concept with complete examples\n2. **Learner Activity**: You apply the concept with guidance and hidden solutions"
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": "## **Setup**\n\nLet's install and import the necessary libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai tenacity pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, List, Optional, Union, Callable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.schema.runnable import (\n",
    "    Runnable, RunnablePassthrough, RunnableLambda, \n",
    "    RunnableBranch, RunnableParallel\n",
    ")\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import time\n",
    "import random\n",
    "from enum import Enum\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_1",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 1: Routing and Conditional Logic**\n\nLet's explore advanced routing patterns:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_routing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic routing based on input classification\n",
    "\n",
    "class QueryType(str, Enum):\n",
    "    TECHNICAL = \"technical\"\n",
    "    CREATIVE = \"creative\"\n",
    "    ANALYTICAL = \"analytical\"\n",
    "    GENERAL = \"general\"\n",
    "\n",
    "def classify_query(query: str) -> QueryType:\n",
    "    \"\"\"Classify the query type.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Simple keyword-based classification\n",
    "    technical_keywords = [\"code\", \"debug\", \"api\", \"function\", \"algorithm\", \"error\"]\n",
    "    creative_keywords = [\"story\", \"poem\", \"creative\", \"imagine\", \"design\", \"art\"]\n",
    "    analytical_keywords = [\"analyze\", \"compare\", \"evaluate\", \"assess\", \"data\", \"statistics\"]\n",
    "    \n",
    "    if any(keyword in query_lower for keyword in technical_keywords):\n",
    "        return QueryType.TECHNICAL\n",
    "    elif any(keyword in query_lower for keyword in creative_keywords):\n",
    "        return QueryType.CREATIVE\n",
    "    elif any(keyword in query_lower for keyword in analytical_keywords):\n",
    "        return QueryType.ANALYTICAL\n",
    "    else:\n",
    "        return QueryType.GENERAL\n",
    "\n",
    "# Create specialized chains for each type\n",
    "technical_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a technical expert. Provide detailed, accurate technical information.\n",
    "    Use code examples when appropriate.\n",
    "    \n",
    "    Question: {query}\n",
    "    Technical Answer:\"\"\"\n",
    ")\n",
    "\n",
    "creative_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a creative writer. Be imaginative and engaging.\n",
    "    Use vivid descriptions and creative language.\n",
    "    \n",
    "    Request: {query}\n",
    "    Creative Response:\"\"\"\n",
    ")\n",
    "\n",
    "analytical_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a data analyst. Provide structured, logical analysis.\n",
    "    Break down complex topics systematically.\n",
    "    \n",
    "    Topic: {query}\n",
    "    Analysis:\"\"\"\n",
    ")\n",
    "\n",
    "general_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Provide a clear, concise answer.\n",
    "    \n",
    "    Question: {query}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Create router using RunnableBranch\n",
    "def create_router():\n",
    "    llm = ChatOpenAI(temperature=0.7)\n",
    "    \n",
    "    # Define routing logic\n",
    "    def route(inputs: Dict) -> Runnable:\n",
    "        query_type = classify_query(inputs[\"query\"])\n",
    "        \n",
    "        if query_type == QueryType.TECHNICAL:\n",
    "            return technical_prompt | llm\n",
    "        elif query_type == QueryType.CREATIVE:\n",
    "            return creative_prompt | llm\n",
    "        elif query_type == QueryType.ANALYTICAL:\n",
    "            return analytical_prompt | llm\n",
    "        else:\n",
    "            return general_prompt | llm\n",
    "    \n",
    "    # Create router with RunnableLambda\n",
    "    router = RunnableLambda(route)\n",
    "    \n",
    "    return router\n",
    "\n",
    "# Test the router\n",
    "router = create_router()\n",
    "\n",
    "test_queries = [\n",
    "    \"Write a function to sort a list in Python\",\n",
    "    \"Create a poem about the ocean\",\n",
    "    \"Analyze the pros and cons of remote work\",\n",
    "    \"What's the capital of France?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Type: {query_type}\")\n",
    "    \n",
    "    # Route and get response\n",
    "    response = router.invoke({\"query\": query})\n",
    "    print(f\"Response: {response.content[:200]}...\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional_chains",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional chains with complex branching\n",
    "\n",
    "class ConditionalChain:\n",
    "    \"\"\"Chain with conditional execution based on intermediate results.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.setup_chains()\n",
    "    \n",
    "    def setup_chains(self):\n",
    "        # Initial analysis chain\n",
    "        self.analysis_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                \"\"\"Analyze this request and determine its complexity.\n",
    "                Rate from 1-10 and explain why.\n",
    "                \n",
    "                Request: {input}\n",
    "                \n",
    "                Return JSON: {{\"complexity\": <number>, \"reason\": \"<explanation>\"}}\"\"\"\n",
    "            )\n",
    "            | self.llm\n",
    "        )\n",
    "        \n",
    "        # Simple response chain\n",
    "        self.simple_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                \"\"\"Provide a simple, straightforward answer.\n",
    "                Request: {input}\n",
    "                Simple Answer:\"\"\"\n",
    "            )\n",
    "            | self.llm\n",
    "        )\n",
    "        \n",
    "        # Detailed response chain\n",
    "        self.detailed_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                \"\"\"Provide a comprehensive, detailed response with examples.\n",
    "                Request: {input}\n",
    "                Detailed Answer:\"\"\"\n",
    "            )\n",
    "            | self.llm\n",
    "        )\n",
    "        \n",
    "        # Expert response chain\n",
    "        self.expert_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                \"\"\"Provide an expert-level analysis with advanced concepts,\n",
    "                edge cases, and best practices.\n",
    "                Request: {input}\n",
    "                Expert Analysis:\"\"\"\n",
    "            )\n",
    "            | self.llm\n",
    "        )\n",
    "    \n",
    "    def process(self, user_input: str) -> Dict:\n",
    "        \"\"\"Process input with conditional logic.\"\"\"\n",
    "        # Step 1: Analyze complexity\n",
    "        analysis = self.analysis_chain.invoke({\"input\": user_input})\n",
    "        \n",
    "        # Parse complexity\n",
    "        try:\n",
    "            import json\n",
    "            analysis_data = json.loads(analysis.content)\n",
    "            complexity = analysis_data.get(\"complexity\", 5)\n",
    "        except:\n",
    "            complexity = 5  # Default to medium\n",
    "        \n",
    "        # Step 2: Route based on complexity\n",
    "        if complexity <= 3:\n",
    "            response_type = \"simple\"\n",
    "            response = self.simple_chain.invoke({\"input\": user_input})\n",
    "        elif complexity <= 7:\n",
    "            response_type = \"detailed\"\n",
    "            response = self.detailed_chain.invoke({\"input\": user_input})\n",
    "        else:\n",
    "            response_type = \"expert\"\n",
    "            response = self.expert_chain.invoke({\"input\": user_input})\n",
    "        \n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"complexity\": complexity,\n",
    "            \"response_type\": response_type,\n",
    "            \"response\": response.content\n",
    "        }\n",
    "\n",
    "# Test conditional chain\n",
    "conditional = ConditionalChain()\n",
    "\n",
    "test_inputs = [\n",
    "    \"What is 2+2?\",\n",
    "    \"Explain how neural networks work\",\n",
    "    \"Design a distributed system for real-time data processing with fault tolerance\"\n",
    "]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    result = conditional.process(input_text)\n",
    "    print(f\"\\nInput: {result['input']}\")\n",
    "    print(f\"Complexity: {result['complexity']}/10\")\n",
    "    print(f\"Response Type: {result['response_type']}\")\n",
    "    print(f\"Response: {result['response'][:200]}...\\n\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_1",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 1: Build an Intelligent Router**\n\nCreate an advanced routing system that handles multiple dimensions.\n\n**Task**: Build a router that considers:\n1. Query type (technical, business, creative)\n2. User expertise level (beginner, intermediate, expert)\n3. Response format preference (brief, detailed, structured)\n4. Language complexity adjustment\n5. Fallback routing for ambiguous queries\n\nRequirements:\n- Multi-dimensional routing logic\n- Adaptive response generation\n- Graceful handling of edge cases\n- Performance optimization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_1_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your intelligent router\n",
    "\n",
    "class UserContext(BaseModel):\n",
    "    expertise_level: str = Field(default=\"intermediate\")\n",
    "    format_preference: str = Field(default=\"detailed\")\n",
    "    # TODO: Add more context fields\n",
    "\n",
    "class IntelligentRouter:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize routing components\n",
    "        pass\n",
    "    \n",
    "    def analyze_query(self, query: str) -> Dict:\n",
    "        \"\"\"Analyze query across multiple dimensions.\"\"\"\n",
    "        # TODO: Implement multi-dimensional analysis\n",
    "        pass\n",
    "    \n",
    "    def select_chain(self, analysis: Dict, context: UserContext) -> Runnable:\n",
    "        \"\"\"Select appropriate chain based on analysis and context.\"\"\"\n",
    "        # TODO: Implement selection logic\n",
    "        pass\n",
    "    \n",
    "    def route(self, query: str, context: UserContext) -> str:\n",
    "        \"\"\"Route query to appropriate handler.\"\"\"\n",
    "        # TODO: Implement complete routing flow\n",
    "        pass\n",
    "\n",
    "# TODO: Test your router\n",
    "# Test with various query types and user contexts\n",
    "# Verify routing decisions\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_1_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "class ExpertiseLevel(str, Enum):\n",
    "    BEGINNER = \"beginner\"\n",
    "    INTERMEDIATE = \"intermediate\"\n",
    "    EXPERT = \"expert\"\n",
    "\n",
    "class FormatPreference(str, Enum):\n",
    "    BRIEF = \"brief\"\n",
    "    DETAILED = \"detailed\"\n",
    "    STRUCTURED = \"structured\"\n",
    "\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    expertise_level: ExpertiseLevel = ExpertiseLevel.INTERMEDIATE\n",
    "    format_preference: FormatPreference = FormatPreference.DETAILED\n",
    "    language: str = \"en\"\n",
    "    max_tokens: int = 500\n",
    "\n",
    "class IntelligentRouter:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.analysis_cache = {}  # Cache analysis results\n",
    "        self._setup_chains()\n",
    "    \n",
    "    def _setup_chains(self):\n",
    "        '''Setup all chain variants.'''\n",
    "        # Create chain templates for different combinations\n",
    "        self.chain_templates = {}\n",
    "        \n",
    "        # Technical chains\n",
    "        for level in ExpertiseLevel:\n",
    "            for format_pref in FormatPreference:\n",
    "                key = f\"technical_{level.value}_{format_pref.value}\"\n",
    "                \n",
    "                if level == ExpertiseLevel.BEGINNER:\n",
    "                    complexity = \"Use simple terms, avoid jargon, provide analogies.\"\n",
    "                elif level == ExpertiseLevel.INTERMEDIATE:\n",
    "                    complexity = \"Use standard technical terms with brief explanations.\"\n",
    "                else:\n",
    "                    complexity = \"Use advanced concepts, assume technical knowledge.\"\n",
    "                \n",
    "                if format_pref == FormatPreference.BRIEF:\n",
    "                    format_inst = \"Provide a concise answer in 2-3 sentences.\"\n",
    "                elif format_pref == FormatPreference.DETAILED:\n",
    "                    format_inst = \"Provide a comprehensive answer with examples.\"\n",
    "                else:\n",
    "                    format_inst = \"Structure your answer with clear sections and bullet points.\"\n",
    "                \n",
    "                template = f'''You are a technical assistant.\n",
    "                {complexity}\n",
    "                {format_inst}\n",
    "                \n",
    "                Question: {{query}}\n",
    "                \n",
    "                Answer:'''\n",
    "                \n",
    "                self.chain_templates[key] = ChatPromptTemplate.from_template(template) | self.llm\n",
    "        \n",
    "        # Business chains (similar structure)\n",
    "        for level in ExpertiseLevel:\n",
    "            key = f\"business_{level.value}_detailed\"\n",
    "            template = f'''You are a business consultant.\n",
    "            Expertise level: {level.value}\n",
    "            Provide practical business insights.\n",
    "            \n",
    "            Question: {{query}}\n",
    "            \n",
    "            Business Perspective:'''\n",
    "            \n",
    "            self.chain_templates[key] = ChatPromptTemplate.from_template(template) | self.llm\n",
    "        \n",
    "        # Creative chains\n",
    "        self.chain_templates[\"creative_any_any\"] = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                '''You are a creative assistant.\n",
    "                Be imaginative and engaging.\n",
    "                \n",
    "                Request: {query}\n",
    "                \n",
    "                Creative Response:'''\n",
    "            ) | self.llm\n",
    "        )\n",
    "        \n",
    "        # Fallback chain\n",
    "        self.fallback_chain = (\n",
    "            ChatPromptTemplate.from_template(\n",
    "                '''Provide a helpful response to this query.\n",
    "                Query: {query}\n",
    "                Response:'''\n",
    "            ) | self.llm\n",
    "        )\n",
    "    \n",
    "    def analyze_query(self, query: str) -> Dict:\n",
    "        '''Analyze query across multiple dimensions.'''\n",
    "        # Check cache\n",
    "        cache_key = hash(query)\n",
    "        if cache_key in self.analysis_cache:\n",
    "            return self.analysis_cache[cache_key]\n",
    "        \n",
    "        analysis_prompt = ChatPromptTemplate.from_template(\n",
    "            '''Analyze this query and return JSON:\n",
    "            {{\n",
    "                \"domain\": \"technical/business/creative/general\",\n",
    "                \"complexity\": 1-10,\n",
    "                \"requires_code\": true/false,\n",
    "                \"requires_examples\": true/false,\n",
    "                \"estimated_response_length\": \"short/medium/long\"\n",
    "            }}\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            JSON:'''\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = (analysis_prompt | self.llm).invoke({\"query\": query})\n",
    "            analysis = json.loads(response.content)\n",
    "        except:\n",
    "            # Fallback analysis\n",
    "            analysis = self._simple_analysis(query)\n",
    "        \n",
    "        # Cache result\n",
    "        self.analysis_cache[cache_key] = analysis\n",
    "        return analysis\n",
    "    \n",
    "    def _simple_analysis(self, query: str) -> Dict:\n",
    "        '''Simple keyword-based analysis as fallback.'''\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        domain = \"general\"\n",
    "        if any(word in query_lower for word in [\"code\", \"function\", \"api\", \"debug\"]):\n",
    "            domain = \"technical\"\n",
    "        elif any(word in query_lower for word in [\"business\", \"market\", \"strategy\", \"roi\"]):\n",
    "            domain = \"business\"\n",
    "        elif any(word in query_lower for word in [\"create\", \"imagine\", \"story\", \"design\"]):\n",
    "            domain = \"creative\"\n",
    "        \n",
    "        return {\n",
    "            \"domain\": domain,\n",
    "            \"complexity\": len(query.split()) // 5 + 3,  # Rough estimate\n",
    "            \"requires_code\": \"code\" in query_lower,\n",
    "            \"requires_examples\": \"example\" in query_lower or \"how\" in query_lower,\n",
    "            \"estimated_response_length\": \"medium\"\n",
    "        }\n",
    "    \n",
    "    def select_chain(self, analysis: Dict, context: UserContext) -> Runnable:\n",
    "        '''Select appropriate chain based on analysis and context.'''\n",
    "        domain = analysis.get(\"domain\", \"general\")\n",
    "        complexity = analysis.get(\"complexity\", 5)\n",
    "        \n",
    "        # Adjust expertise level based on complexity\n",
    "        if complexity > 7 and context.expertise_level == ExpertiseLevel.BEGINNER:\n",
    "            # Upgrade to intermediate for complex queries\n",
    "            effective_level = ExpertiseLevel.INTERMEDIATE\n",
    "        else:\n",
    "            effective_level = context.expertise_level\n",
    "        \n",
    "        # Build chain key\n",
    "        chain_key = f\"{domain}_{effective_level.value}_{context.format_preference.value}\"\n",
    "        \n",
    "        # Get chain or fallback\n",
    "        if chain_key in self.chain_templates:\n",
    "            return self.chain_templates[chain_key]\n",
    "        \n",
    "        # Try without format preference\n",
    "        chain_key_simple = f\"{domain}_{effective_level.value}_detailed\"\n",
    "        if chain_key_simple in self.chain_templates:\n",
    "            return self.chain_templates[chain_key_simple]\n",
    "        \n",
    "        # Ultimate fallback\n",
    "        return self.fallback_chain\n",
    "    \n",
    "    def route(self, query: str, context: UserContext = None) -> Dict:\n",
    "        '''Route query to appropriate handler.'''\n",
    "        if context is None:\n",
    "            context = UserContext()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Analyze query\n",
    "        analysis = self.analyze_query(query)\n",
    "        \n",
    "        # Select and execute chain\n",
    "        chain = self.select_chain(analysis, context)\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": query})\n",
    "            response_text = response.content\n",
    "        except Exception as e:\n",
    "            # Fallback on error\n",
    "            response_text = self.fallback_chain.invoke({\"query\": query}).content\n",
    "            analysis[\"fallback_used\"] = True\n",
    "        \n",
    "        # Post-process based on context\n",
    "        if context.max_tokens and len(response_text) > context.max_tokens * 4:  # Rough char estimate\n",
    "            response_text = response_text[:context.max_tokens * 4] + \"...\"\n",
    "        \n",
    "        routing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response_text,\n",
    "            \"analysis\": analysis,\n",
    "            \"context\": {\n",
    "                \"expertise\": context.expertise_level.value,\n",
    "                \"format\": context.format_preference.value\n",
    "            },\n",
    "            \"routing_time\": routing_time,\n",
    "            \"chain_used\": chain.__class__.__name__\n",
    "        }\n",
    "\n",
    "# Test the intelligent router\n",
    "print(\"Intelligent Routing System Test\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "router = IntelligentRouter()\n",
    "\n",
    "# Test scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"query\": \"How do I write a for loop?\",\n",
    "        \"context\": UserContext(\n",
    "            expertise_level=ExpertiseLevel.BEGINNER,\n",
    "            format_preference=FormatPreference.BRIEF\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain the architecture of a microservices system with examples\",\n",
    "        \"context\": UserContext(\n",
    "            expertise_level=ExpertiseLevel.EXPERT,\n",
    "            format_preference=FormatPreference.STRUCTURED\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the ROI of digital transformation?\",\n",
    "        \"context\": UserContext(\n",
    "            expertise_level=ExpertiseLevel.INTERMEDIATE,\n",
    "            format_preference=FormatPreference.DETAILED\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Write a creative story about a robot\",\n",
    "        \"context\": UserContext(\n",
    "            expertise_level=ExpertiseLevel.INTERMEDIATE,\n",
    "            format_preference=FormatPreference.BRIEF\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    result = router.route(scenario[\"query\"], scenario[\"context\"])\n",
    "    \n",
    "    print(f\"\\nQuery: {result['query']}\")\n",
    "    print(f\"Analysis: Domain={result['analysis']['domain']}, Complexity={result['analysis']['complexity']}\")\n",
    "    print(f\"Context: {result['context']}\")\n",
    "    print(f\"Routing Time: {result['routing_time']:.3f}s\")\n",
    "    print(f\"Response Preview: {result['response'][:150]}...\")\n",
    "    print(\"-\" * 50)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build an intelligent multi-dimensional router!\")\n",
    "print(\"The solution includes query analysis, context awareness, and fallback handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_2",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 2: Retry, Fallback, and Self-Correction**\n\nLet's implement robust error handling and self-correction patterns:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retry_pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced retry with exponential backoff\n",
    "\n",
    "class RetryableChain:\n",
    "    \"\"\"Chain with sophisticated retry logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3):\n",
    "        self.max_retries = max_retries\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.retry_history = []\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=10)\n",
    "    )\n",
    "    def _execute_with_retry(self, prompt: str) -> str:\n",
    "        \"\"\"Execute with exponential backoff retry.\"\"\"\n",
    "        # Simulate occasional failures\n",
    "        if random.random() < 0.3:  # 30% failure rate for demo\n",
    "            raise Exception(\"Simulated API error\")\n",
    "        \n",
    "        response = self.llm.predict(prompt)\n",
    "        return response\n",
    "    \n",
    "    def execute(self, prompt: str) -> Dict:\n",
    "        \"\"\"Execute with comprehensive error handling.\"\"\"\n",
    "        attempt = 0\n",
    "        errors = []\n",
    "        \n",
    "        while attempt < self.max_retries:\n",
    "            try:\n",
    "                attempt += 1\n",
    "                print(f\"Attempt {attempt}/{self.max_retries}...\")\n",
    "                \n",
    "                # Add retry context to prompt if not first attempt\n",
    "                if attempt > 1:\n",
    "                    retry_context = f\"\\n\\n[Previous attempt failed. Please try again carefully.]\"\n",
    "                    enhanced_prompt = prompt + retry_context\n",
    "                else:\n",
    "                    enhanced_prompt = prompt\n",
    "                \n",
    "                result = self._execute_with_retry(enhanced_prompt)\n",
    "                \n",
    "                self.retry_history.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"success\": True,\n",
    "                    \"result\": result\n",
    "                })\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"result\": result,\n",
    "                    \"attempts\": attempt,\n",
    "                    \"errors\": errors\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": time.time()\n",
    "                })\n",
    "                \n",
    "                self.retry_history.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                \n",
    "                if attempt < self.max_retries:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"  Error: {e}. Waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "        \n",
    "        # All retries failed\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"result\": None,\n",
    "            \"attempts\": attempt,\n",
    "            \"errors\": errors\n",
    "        }\n",
    "\n",
    "# Test retry chain\n",
    "retry_chain = RetryableChain(max_retries=3)\n",
    "\n",
    "result = retry_chain.execute(\"What is the capital of Japan?\")\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Attempts: {result['attempts']}\")\n",
    "if result['success']:\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "else:\n",
    "    print(f\"Errors: {result['errors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self_correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-correcting chain with validation\n",
    "\n",
    "class SelfCorrectingChain:\n",
    "    \"\"\"Chain that validates and corrects its own output.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.validator_llm = ChatOpenAI(temperature=0)  # More deterministic for validation\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate initial response.\"\"\"\n",
    "        return self.llm.predict(prompt)\n",
    "    \n",
    "    def validate(self, prompt: str, response: str) -> Dict:\n",
    "        \"\"\"Validate the response.\"\"\"\n",
    "        validation_prompt = f\"\"\"Validate this response for accuracy and completeness.\n",
    "        \n",
    "        Original Question: {prompt}\n",
    "        Response: {response}\n",
    "        \n",
    "        Check for:\n",
    "        1. Factual accuracy\n",
    "        2. Completeness\n",
    "        3. Clarity\n",
    "        4. Relevance\n",
    "        \n",
    "        Return JSON:\n",
    "        {{\n",
    "            \"is_valid\": true/false,\n",
    "            \"issues\": [list of issues if any],\n",
    "            \"suggestions\": [list of improvements]\n",
    "        }}\n",
    "        \n",
    "        JSON:\"\"\"\n",
    "        \n",
    "        validation_result = self.validator_llm.predict(validation_prompt)\n",
    "        \n",
    "        try:\n",
    "            return json.loads(validation_result)\n",
    "        except:\n",
    "            # Default to valid if parsing fails\n",
    "            return {\"is_valid\": True, \"issues\": [], \"suggestions\": []}\n",
    "    \n",
    "    def correct(self, prompt: str, response: str, validation: Dict) -> str:\n",
    "        \"\"\"Correct the response based on validation.\"\"\"\n",
    "        correction_prompt = f\"\"\"Improve this response based on the feedback.\n",
    "        \n",
    "        Original Question: {prompt}\n",
    "        Original Response: {response}\n",
    "        \n",
    "        Issues to fix: {validation['issues']}\n",
    "        Suggestions: {validation['suggestions']}\n",
    "        \n",
    "        Improved Response:\"\"\"\n",
    "        \n",
    "        return self.llm.predict(correction_prompt)\n",
    "    \n",
    "    def execute_with_correction(self, prompt: str, max_corrections: int = 2) -> Dict:\n",
    "        \"\"\"Execute with self-correction loop.\"\"\"\n",
    "        correction_count = 0\n",
    "        history = []\n",
    "        \n",
    "        # Initial generation\n",
    "        response = self.generate(prompt)\n",
    "        history.append({\"version\": 0, \"response\": response})\n",
    "        \n",
    "        while correction_count < max_corrections:\n",
    "            # Validate\n",
    "            validation = self.validate(prompt, response)\n",
    "            \n",
    "            if validation[\"is_valid\"] and not validation[\"issues\"]:\n",
    "                # Response is valid\n",
    "                return {\n",
    "                    \"final_response\": response,\n",
    "                    \"corrections_made\": correction_count,\n",
    "                    \"history\": history,\n",
    "                    \"validation\": validation\n",
    "                }\n",
    "            \n",
    "            # Need correction\n",
    "            correction_count += 1\n",
    "            print(f\"  Correction {correction_count}: Fixing issues {validation['issues']}\")\n",
    "            \n",
    "            response = self.correct(prompt, response, validation)\n",
    "            history.append({\n",
    "                \"version\": correction_count,\n",
    "                \"response\": response,\n",
    "                \"fixed_issues\": validation[\"issues\"]\n",
    "            })\n",
    "        \n",
    "        # Max corrections reached\n",
    "        return {\n",
    "            \"final_response\": response,\n",
    "            \"corrections_made\": correction_count,\n",
    "            \"history\": history,\n",
    "            \"validation\": validation\n",
    "        }\n",
    "\n",
    "# Test self-correcting chain\n",
    "self_correcting = SelfCorrectingChain()\n",
    "\n",
    "test_prompts = [\n",
    "    \"Write a Python function to calculate factorial\",\n",
    "    \"Explain the water cycle with all major steps\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"Processing with self-correction...\")\n",
    "    \n",
    "    result = self_correcting.execute_with_correction(prompt)\n",
    "    \n",
    "    print(f\"\\nCorrections made: {result['corrections_made']}\")\n",
    "    print(f\"Final Response: {result['final_response'][:200]}...\")\n",
    "    \n",
    "    if result['corrections_made'] > 0:\n",
    "        print(\"\\nCorrection History:\")\n",
    "        for item in result['history'][1:]:  # Skip original\n",
    "            print(f\"  Version {item['version']}: Fixed {item.get('fixed_issues', [])}\")\n",
    "    \n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_2",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 2: Build a Resilient Chain System**\n\nCreate a comprehensive resilient system with multiple fallback strategies.\n\n**Task**: Build a system that:\n1. Has primary, secondary, and tertiary processing chains\n2. Implements intelligent fallback selection\n3. Includes self-healing capabilities\n4. Tracks and learns from failures\n5. Provides graceful degradation\n\nRequirements:\n- Multiple fallback strategies\n- Failure pattern recognition\n- Adaptive retry logic\n- Performance metrics tracking"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_2_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your resilient chain system\n",
    "\n",
    "class ResilientSystem:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize primary, secondary, tertiary chains\n",
    "        # TODO: Setup failure tracking\n",
    "        pass\n",
    "    \n",
    "    def execute_with_fallback(self, prompt: str) -> Dict:\n",
    "        \"\"\"Execute with intelligent fallback.\"\"\"\n",
    "        # TODO: Try primary chain\n",
    "        # TODO: On failure, select appropriate fallback\n",
    "        # TODO: Track failure patterns\n",
    "        pass\n",
    "    \n",
    "    def learn_from_failure(self, failure_info: Dict):\n",
    "        \"\"\"Learn from failures to improve future routing.\"\"\"\n",
    "        # TODO: Analyze failure patterns\n",
    "        # TODO: Adjust routing strategy\n",
    "        pass\n",
    "    \n",
    "    def get_health_status(self) -> Dict:\n",
    "        \"\"\"Get system health and performance metrics.\"\"\"\n",
    "        # TODO: Return health metrics\n",
    "        pass\n",
    "\n",
    "# TODO: Test resilient system\n",
    "# Simulate various failure scenarios\n",
    "# Verify fallback behavior\n",
    "# Check learning from failures\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_2_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class ChainHealth:\n",
    "    '''Track health metrics for a chain.'''\n",
    "    def __init__(self, window_size: int = 100):\n",
    "        self.success_count = 0\n",
    "        self.failure_count = 0\n",
    "        self.recent_latencies = deque(maxlen=window_size)\n",
    "        self.recent_errors = deque(maxlen=window_size)\n",
    "        self.last_failure_time = None\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        total = self.success_count + self.failure_count\n",
    "        return self.success_count / total if total > 0 else 1.0\n",
    "    \n",
    "    @property\n",
    "    def avg_latency(self) -> float:\n",
    "        return sum(self.recent_latencies) / len(self.recent_latencies) if self.recent_latencies else 0\n",
    "    \n",
    "    @property\n",
    "    def health_score(self) -> float:\n",
    "        '''Composite health score 0-1.'''\n",
    "        # Weight factors\n",
    "        success_weight = 0.5\n",
    "        latency_weight = 0.3\n",
    "        recency_weight = 0.2\n",
    "        \n",
    "        # Success component\n",
    "        success_score = self.success_rate\n",
    "        \n",
    "        # Latency component (normalize to 0-1, assuming 5s is bad)\n",
    "        latency_score = max(0, 1 - (self.avg_latency / 5)) if self.recent_latencies else 1\n",
    "        \n",
    "        # Recency component (penalize recent failures)\n",
    "        if self.last_failure_time:\n",
    "            time_since_failure = (datetime.now() - self.last_failure_time).seconds\n",
    "            recency_score = min(1, time_since_failure / 300)  # 5 min to full recovery\n",
    "        else:\n",
    "            recency_score = 1\n",
    "        \n",
    "        return (\n",
    "            success_weight * success_score +\n",
    "            latency_weight * latency_score +\n",
    "            recency_weight * recency_score\n",
    "        )\n",
    "\n",
    "class ResilientSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize chains with different models/configs\n",
    "        self.primary_chain = self._create_chain(\"gpt-3.5-turbo\", 0.7)\n",
    "        self.secondary_chain = self._create_chain(\"gpt-3.5-turbo\", 0.5)  # Lower temp\n",
    "        self.tertiary_chain = self._create_chain(\"gpt-3.5-turbo\", 0.3)  # Even lower\n",
    "        \n",
    "        # Health tracking\n",
    "        self.chain_health = {\n",
    "            \"primary\": ChainHealth(),\n",
    "            \"secondary\": ChainHealth(),\n",
    "            \"tertiary\": ChainHealth()\n",
    "        }\n",
    "        \n",
    "        # Failure patterns\n",
    "        self.failure_patterns = defaultdict(list)\n",
    "        self.adaptive_routing = True\n",
    "        \n",
    "        # Circuit breaker settings\n",
    "        self.circuit_breaker = {\n",
    "            \"primary\": \"closed\",  # closed, open, half_open\n",
    "            \"secondary\": \"closed\",\n",
    "            \"tertiary\": \"closed\"\n",
    "        }\n",
    "        self.circuit_breaker_threshold = 0.5  # Open if success rate < 50%\n",
    "        self.circuit_breaker_timeout = 30  # Seconds before trying half_open\n",
    "    \n",
    "    def _create_chain(self, model: str, temperature: float):\n",
    "        '''Create a chain with specific configuration.'''\n",
    "        llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "        prompt = ChatPromptTemplate.from_template(\"Answer this: {prompt}\")\n",
    "        return prompt | llm\n",
    "    \n",
    "    def _check_circuit_breaker(self, chain_name: str) -> bool:\n",
    "        '''Check if circuit breaker allows execution.'''\n",
    "        state = self.circuit_breaker[chain_name]\n",
    "        health = self.chain_health[chain_name]\n",
    "        \n",
    "        if state == \"closed\":\n",
    "            # Check if should open\n",
    "            if health.success_rate < self.circuit_breaker_threshold and health.failure_count > 5:\n",
    "                self.circuit_breaker[chain_name] = \"open\"\n",
    "                print(f\"  ðŸ”´ Circuit breaker OPENED for {chain_name}\")\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        elif state == \"open\":\n",
    "            # Check if should try half_open\n",
    "            if health.last_failure_time:\n",
    "                time_since = (datetime.now() - health.last_failure_time).seconds\n",
    "                if time_since > self.circuit_breaker_timeout:\n",
    "                    self.circuit_breaker[chain_name] = \"half_open\"\n",
    "                    print(f\"  ðŸŸ¡ Circuit breaker HALF-OPEN for {chain_name}\")\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        else:  # half_open\n",
    "            return True\n",
    "    \n",
    "    def _execute_chain(self, chain_name: str, chain, prompt: str) -> tuple:\n",
    "        '''Execute a chain with health tracking.'''\n",
    "        health = self.chain_health[chain_name]\n",
    "        \n",
    "        # Check circuit breaker\n",
    "        if not self._check_circuit_breaker(chain_name):\n",
    "            return False, \"Circuit breaker open\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Simulate occasional failures for demo\n",
    "            if chain_name == \"primary\" and random.random() < 0.4:  # 40% failure rate\n",
    "                raise Exception(\"Simulated primary chain failure\")\n",
    "            elif chain_name == \"secondary\" and random.random() < 0.2:  # 20% failure rate\n",
    "                raise Exception(\"Simulated secondary chain failure\")\n",
    "            \n",
    "            result = chain.invoke({\"prompt\": prompt})\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Update health\n",
    "            health.success_count += 1\n",
    "            health.recent_latencies.append(latency)\n",
    "            \n",
    "            # Close circuit breaker if half_open\n",
    "            if self.circuit_breaker[chain_name] == \"half_open\":\n",
    "                self.circuit_breaker[chain_name] = \"closed\"\n",
    "                print(f\"  ðŸŸ¢ Circuit breaker CLOSED for {chain_name}\")\n",
    "            \n",
    "            return True, result.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Update health\n",
    "            health.failure_count += 1\n",
    "            health.recent_latencies.append(latency)\n",
    "            health.recent_errors.append(str(e))\n",
    "            health.last_failure_time = datetime.now()\n",
    "            \n",
    "            # Open circuit breaker if half_open\n",
    "            if self.circuit_breaker[chain_name] == \"half_open\":\n",
    "                self.circuit_breaker[chain_name] = \"open\"\n",
    "                print(f\"  ðŸ”´ Circuit breaker RE-OPENED for {chain_name}\")\n",
    "            \n",
    "            return False, str(e)\n",
    "    \n",
    "    def execute_with_fallback(self, prompt: str) -> Dict:\n",
    "        '''Execute with intelligent fallback.'''\n",
    "        execution_log = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine chain order based on health scores\n",
    "        if self.adaptive_routing:\n",
    "            chain_order = self._get_adaptive_order()\n",
    "        else:\n",
    "            chain_order = [(\"primary\", self.primary_chain),\n",
    "                         (\"secondary\", self.secondary_chain),\n",
    "                         (\"tertiary\", self.tertiary_chain)]\n",
    "        \n",
    "        # Try chains in order\n",
    "        for chain_name, chain in chain_order:\n",
    "            print(f\"\\n  Trying {chain_name} chain...\")\n",
    "            success, result = self._execute_chain(chain_name, chain, prompt)\n",
    "            \n",
    "            execution_log.append({\n",
    "                \"chain\": chain_name,\n",
    "                \"success\": success,\n",
    "                \"result\": result if success else None,\n",
    "                \"error\": result if not success else None\n",
    "            })\n",
    "            \n",
    "            if success:\n",
    "                total_time = time.time() - start_time\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"result\": result,\n",
    "                    \"chain_used\": chain_name,\n",
    "                    \"execution_log\": execution_log,\n",
    "                    \"total_time\": total_time\n",
    "                }\n",
    "        \n",
    "        # All chains failed\n",
    "        self._record_failure_pattern(prompt, execution_log)\n",
    "        \n",
    "        # Last resort: return cached or default response\n",
    "        fallback_response = self._get_fallback_response(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"result\": fallback_response,\n",
    "            \"chain_used\": \"fallback\",\n",
    "            \"execution_log\": execution_log,\n",
    "            \"total_time\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    def _get_adaptive_order(self) -> list:\n",
    "        '''Get chain order based on health scores.'''\n",
    "        chains_with_scores = [\n",
    "            (\"primary\", self.primary_chain, self.chain_health[\"primary\"].health_score),\n",
    "            (\"secondary\", self.secondary_chain, self.chain_health[\"secondary\"].health_score),\n",
    "            (\"tertiary\", self.tertiary_chain, self.chain_health[\"tertiary\"].health_score)\n",
    "        ]\n",
    "        \n",
    "        # Sort by health score (best first)\n",
    "        chains_with_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        return [(name, chain) for name, chain, _ in chains_with_scores]\n",
    "    \n",
    "    def _record_failure_pattern(self, prompt: str, execution_log: list):\n",
    "        '''Record failure pattern for learning.'''\n",
    "        pattern = {\n",
    "            \"prompt_length\": len(prompt),\n",
    "            \"prompt_type\": self._classify_prompt(prompt),\n",
    "            \"failures\": [log[\"chain\"] for log in execution_log if not log[\"success\"]],\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        self.failure_patterns[pattern[\"prompt_type\"]].append(pattern)\n",
    "    \n",
    "    def _classify_prompt(self, prompt: str) -> str:\n",
    "        '''Simple prompt classification.'''\n",
    "        if \"?\" in prompt:\n",
    "            return \"question\"\n",
    "        elif any(word in prompt.lower() for word in [\"create\", \"write\", \"generate\"]):\n",
    "            return \"generation\"\n",
    "        elif any(word in prompt.lower() for word in [\"analyze\", \"explain\", \"describe\"]):\n",
    "            return \"analysis\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "    \n",
    "    def _get_fallback_response(self, prompt: str) -> str:\n",
    "        '''Get fallback response when all chains fail.'''\n",
    "        prompt_type = self._classify_prompt(prompt)\n",
    "        \n",
    "        fallback_responses = {\n",
    "            \"question\": \"I'm unable to answer your question at the moment. Please try again later.\",\n",
    "            \"generation\": \"I'm unable to generate content right now. Please try again.\",\n",
    "            \"analysis\": \"I'm unable to perform the analysis currently. Please retry.\",\n",
    "            \"other\": \"I'm experiencing technical difficulties. Please try again later.\"\n",
    "        }\n",
    "        \n",
    "        return fallback_responses.get(prompt_type, fallback_responses[\"other\"])\n",
    "    \n",
    "    def learn_from_failure(self, prompt_type: str = None):\n",
    "        '''Analyze failure patterns and adjust strategy.'''\n",
    "        if prompt_type and prompt_type in self.failure_patterns:\n",
    "            patterns = self.failure_patterns[prompt_type]\n",
    "            \n",
    "            if len(patterns) > 5:  # Need enough data\n",
    "                # Find most common failing chain\n",
    "                failing_chains = []\n",
    "                for pattern in patterns[-10:]:  # Last 10 failures\n",
    "                    failing_chains.extend(pattern[\"failures\"])\n",
    "                \n",
    "                from collections import Counter\n",
    "                chain_failures = Counter(failing_chains)\n",
    "                \n",
    "                # Adjust routing if clear pattern\n",
    "                most_failed = chain_failures.most_common(1)[0] if chain_failures else None\n",
    "                if most_failed and most_failed[1] > 5:\n",
    "                    print(f\"  ðŸ“Š Learning: {most_failed[0]} chain frequently fails for {prompt_type}\")\n",
    "                    # Could adjust routing strategy here\n",
    "    \n",
    "    def get_health_status(self) -> Dict:\n",
    "        '''Get comprehensive health status.'''\n",
    "        status = {\n",
    "            \"chains\": {},\n",
    "            \"circuit_breakers\": self.circuit_breaker.copy(),\n",
    "            \"failure_patterns\": {k: len(v) for k, v in self.failure_patterns.items()},\n",
    "            \"adaptive_routing\": self.adaptive_routing\n",
    "        }\n",
    "        \n",
    "        for chain_name, health in self.chain_health.items():\n",
    "            status[\"chains\"][chain_name] = {\n",
    "                \"health_score\": health.health_score,\n",
    "                \"success_rate\": health.success_rate,\n",
    "                \"avg_latency\": health.avg_latency,\n",
    "                \"total_requests\": health.success_count + health.failure_count\n",
    "            }\n",
    "        \n",
    "        return status\n",
    "\n",
    "# Test the resilient system\n",
    "print(\"Resilient System Test\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "system = ResilientSystem()\n",
    "\n",
    "# Test with multiple queries\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Write a haiku about coding\",\n",
    "    \"Explain quantum computing\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"Generate a business plan outline\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n[{i}] Query: {prompt}\")\n",
    "    result = system.execute_with_fallback(prompt)\n",
    "    \n",
    "    print(f\"\\n  Result:\")\n",
    "    print(f\"    Success: {result['success']}\")\n",
    "    print(f\"    Chain Used: {result['chain_used']}\")\n",
    "    print(f\"    Time: {result['total_time']:.2f}s\")\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"    Response: {result['result'][:100]}...\")\n",
    "    \n",
    "    # Show execution path\n",
    "    print(f\"\\n  Execution Path:\")\n",
    "    for log in result['execution_log']:\n",
    "        status = \"âœ…\" if log['success'] else \"âŒ\"\n",
    "        print(f\"    {status} {log['chain']}\")\n",
    "\n",
    "# Show health status\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nðŸ“Š System Health Status:\")\n",
    "health = system.get_health_status()\n",
    "\n",
    "for chain_name, metrics in health['chains'].items():\n",
    "    print(f\"\\n  {chain_name.capitalize()} Chain:\")\n",
    "    print(f\"    Health Score: {metrics['health_score']:.2f}\")\n",
    "    print(f\"    Success Rate: {metrics['success_rate']:.1%}\")\n",
    "    print(f\"    Avg Latency: {metrics['avg_latency']:.2f}s\")\n",
    "    print(f\"    Circuit Breaker: {health['circuit_breakers'][chain_name]}\")\n",
    "\n",
    "# Learn from failures\n",
    "print(\"\\nðŸ“š Learning from failures...\")\n",
    "system.learn_from_failure(\"question\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build a comprehensive resilient system with fallbacks and self-healing!\")\n",
    "print(\"The solution includes circuit breakers, adaptive routing, and failure learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructor_3",
   "metadata": {},
   "source": "---\n\n## **Instructor Activity 3: Map-Reduce and Complex Orchestration**\n\nLet's implement advanced orchestration patterns:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "map_reduce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map-Reduce pattern for processing large documents\n",
    "\n",
    "class MapReduceProcessor:\n",
    "    \"\"\"Process large documents using map-reduce pattern.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "    \n",
    "    def split_document(self, document: str) -> List[str]:\n",
    "        \"\"\"Split document into chunks.\"\"\"\n",
    "        words = document.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size // 5):  # Rough word count\n",
    "            chunk = \" \".join(words[i:i + self.chunk_size // 5])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def map_operation(self, chunk: str, operation: str) -> str:\n",
    "        \"\"\"Apply operation to a single chunk.\"\"\"\n",
    "        map_prompt = f\"\"\"{operation}\n",
    "        \n",
    "        Text: {chunk}\n",
    "        \n",
    "        Result:\"\"\"\n",
    "        \n",
    "        result = self.llm.predict(map_prompt)\n",
    "        return result\n",
    "    \n",
    "    def reduce_operation(self, results: List[str], operation: str) -> str:\n",
    "        \"\"\"Reduce/combine results from map phase.\"\"\"\n",
    "        reduce_prompt = f\"\"\"{operation}\n",
    "        \n",
    "        Individual Results:\n",
    "        {chr(10).join(f'{i+1}. {r}' for i, r in enumerate(results))}\n",
    "        \n",
    "        Combined Result:\"\"\"\n",
    "        \n",
    "        final_result = self.llm.predict(reduce_prompt)\n",
    "        return final_result\n",
    "    \n",
    "    def process(self, document: str, map_instruction: str, reduce_instruction: str) -> Dict:\n",
    "        \"\"\"Execute map-reduce operation.\"\"\"\n",
    "        # Split\n",
    "        chunks = self.split_document(document)\n",
    "        print(f\"Split document into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Map phase\n",
    "        print(\"\\nMap phase:\")\n",
    "        map_results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"  Processing chunk {i+1}/{len(chunks)}...\")\n",
    "            result = self.map_operation(chunk, map_instruction)\n",
    "            map_results.append(result)\n",
    "        \n",
    "        # Reduce phase\n",
    "        print(\"\\nReduce phase:\")\n",
    "        final_result = self.reduce_operation(map_results, reduce_instruction)\n",
    "        \n",
    "        return {\n",
    "            \"chunks_processed\": len(chunks),\n",
    "            \"map_results\": map_results,\n",
    "            \"final_result\": final_result\n",
    "        }\n",
    "\n",
    "# Test map-reduce\n",
    "processor = MapReduceProcessor(chunk_size=500)\n",
    "\n",
    "# Sample document\n",
    "document = \"\"\"Artificial intelligence has revolutionized many industries. \n",
    "In healthcare, AI helps diagnose diseases and develop new treatments.\n",
    "In finance, AI detects fraud and manages investments.\n",
    "In transportation, AI powers self-driving cars.\n",
    "However, AI also raises ethical concerns about privacy and job displacement.\n",
    "We must ensure AI is developed responsibly and benefits everyone.\"\"\"\n",
    "\n",
    "result = processor.process(\n",
    "    document=document,\n",
    "    map_instruction=\"Extract the main point from this text segment\",\n",
    "    reduce_instruction=\"Combine these points into a comprehensive summary\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nMap Results:\")\n",
    "for i, mr in enumerate(result[\"map_results\"]):\n",
    "    print(f\"{i+1}. {mr}\")\n",
    "\n",
    "print(\"\\nFinal Result:\")\n",
    "print(result[\"final_result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learner_3",
   "metadata": {},
   "source": "---\n\n## **Learner Activity 3: Build an Advanced Orchestration System**\n\nCreate a sophisticated system that orchestrates multiple processing patterns.\n\n**Task**: Build a system that:\n1. Supports map-reduce, pipeline, and parallel processing\n2. Dynamically selects processing strategy\n3. Handles nested orchestrations\n4. Provides progress tracking\n5. Optimizes for cost and performance\n\nRequirements:\n- Multiple orchestration patterns\n- Dynamic strategy selection\n- Resource optimization\n- Comprehensive monitoring"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_3_starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your advanced orchestration system\n",
    "\n",
    "class OrchestrationStrategy(Enum):\n",
    "    MAP_REDUCE = \"map_reduce\"\n",
    "    PIPELINE = \"pipeline\"\n",
    "    PARALLEL = \"parallel\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "class AdvancedOrchestrator:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize orchestration components\n",
    "        pass\n",
    "    \n",
    "    def select_strategy(self, task: Dict) -> OrchestrationStrategy:\n",
    "        \"\"\"Select optimal orchestration strategy.\"\"\"\n",
    "        # TODO: Analyze task and select strategy\n",
    "        pass\n",
    "    \n",
    "    def execute_map_reduce(self, task: Dict) -> Dict:\n",
    "        \"\"\"Execute using map-reduce pattern.\"\"\"\n",
    "        # TODO: Implement map-reduce execution\n",
    "        pass\n",
    "    \n",
    "    def execute_pipeline(self, task: Dict) -> Dict:\n",
    "        \"\"\"Execute using pipeline pattern.\"\"\"\n",
    "        # TODO: Implement pipeline execution\n",
    "        pass\n",
    "    \n",
    "    def execute_parallel(self, task: Dict) -> Dict:\n",
    "        \"\"\"Execute using parallel pattern.\"\"\"\n",
    "        # TODO: Implement parallel execution\n",
    "        pass\n",
    "    \n",
    "    def orchestrate(self, task: Dict) -> Dict:\n",
    "        \"\"\"Main orchestration method.\"\"\"\n",
    "        # TODO: Select and execute strategy\n",
    "        # TODO: Track progress and optimize\n",
    "        pass\n",
    "\n",
    "# TODO: Test orchestration system\n",
    "# Test different task types\n",
    "# Verify strategy selection\n",
    "# Check performance optimization\n",
    "\n",
    "# Your test code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learner_3_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (hidden by default)\n",
    "\n",
    "\"\"\"\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "class AdvancedOrchestrator:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=5)\n",
    "        self.cost_tracker = {\"tokens\": 0, \"requests\": 0}\n",
    "        self.performance_metrics = []\n",
    "    \n",
    "    def select_strategy(self, task: Dict) -> OrchestrationStrategy:\n",
    "        '''Select optimal orchestration strategy based on task characteristics.'''\n",
    "        \n",
    "        task_type = task.get(\"type\", \"general\")\n",
    "        data_size = len(task.get(\"data\", \"\"))\n",
    "        complexity = task.get(\"complexity\", \"medium\")\n",
    "        \n",
    "        # Strategy selection logic\n",
    "        if task_type == \"aggregation\" or data_size > 5000:\n",
    "            return OrchestrationStrategy.MAP_REDUCE\n",
    "        elif task_type == \"sequential\" or \"steps\" in task:\n",
    "            return OrchestrationStrategy.PIPELINE\n",
    "        elif task_type == \"independent\" or task.get(\"parallelize\", False):\n",
    "            return OrchestrationStrategy.PARALLEL\n",
    "        else:\n",
    "            # Hybrid for complex tasks\n",
    "            if complexity == \"high\":\n",
    "                return OrchestrationStrategy.HYBRID\n",
    "            else:\n",
    "                return OrchestrationStrategy.PIPELINE\n",
    "    \n",
    "    def execute_map_reduce(self, task: Dict) -> Dict:\n",
    "        '''Execute using map-reduce pattern.'''\n",
    "        print(\"\\nðŸ—ºï¸  Executing MAP-REDUCE strategy\")\n",
    "        \n",
    "        data = task.get(\"data\", \"\")\n",
    "        map_fn = task.get(\"map_function\", \"Summarize this chunk\")\n",
    "        reduce_fn = task.get(\"reduce_function\", \"Combine these summaries\")\n",
    "        \n",
    "        # Split data\n",
    "        chunks = self._split_data(data, task.get(\"chunk_size\", 1000))\n",
    "        print(f\"  Split into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Map phase (parallel)\n",
    "        print(\"  Map phase...\")\n",
    "        map_results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = []\n",
    "            for chunk in chunks:\n",
    "                future = executor.submit(self._process_chunk, chunk, map_fn)\n",
    "                futures.append(future)\n",
    "            \n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                map_results.append(result)\n",
    "        \n",
    "        # Reduce phase\n",
    "        print(\"  Reduce phase...\")\n",
    "        reduce_prompt = f\"{reduce_fn}\\n\\nResults:\\n\" + \"\\n\".join(map_results)\n",
    "        final_result = self.llm.predict(reduce_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"map_reduce\",\n",
    "            \"chunks_processed\": len(chunks),\n",
    "            \"result\": final_result\n",
    "        }\n",
    "    \n",
    "    def execute_pipeline(self, task: Dict) -> Dict:\n",
    "        '''Execute using pipeline pattern.'''\n",
    "        print(\"\\nðŸš‡ Executing PIPELINE strategy\")\n",
    "        \n",
    "        data = task.get(\"data\", \"\")\n",
    "        steps = task.get(\"steps\", [])\n",
    "        \n",
    "        if not steps:\n",
    "            # Default pipeline\n",
    "            steps = [\n",
    "                \"Analyze the input\",\n",
    "                \"Process and transform\",\n",
    "                \"Generate final output\"\n",
    "            ]\n",
    "        \n",
    "        current_data = data\n",
    "        pipeline_results = []\n",
    "        \n",
    "        for i, step in enumerate(steps, 1):\n",
    "            print(f\"  Step {i}/{len(steps)}: {step[:30]}...\")\n",
    "            \n",
    "            prompt = f\"{step}\\n\\nInput: {current_data}\\n\\nOutput:\"\n",
    "            result = self.llm.predict(prompt)\n",
    "            \n",
    "            pipeline_results.append({\n",
    "                \"step\": i,\n",
    "                \"instruction\": step,\n",
    "                \"output\": result\n",
    "            })\n",
    "            \n",
    "            current_data = result  # Feed to next step\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"pipeline\",\n",
    "            \"steps_executed\": len(steps),\n",
    "            \"pipeline_results\": pipeline_results,\n",
    "            \"result\": current_data\n",
    "        }\n",
    "    \n",
    "    def execute_parallel(self, task: Dict) -> Dict:\n",
    "        '''Execute using parallel pattern.'''\n",
    "        print(\"\\nâš¡ Executing PARALLEL strategy\")\n",
    "        \n",
    "        data = task.get(\"data\", \"\")\n",
    "        operations = task.get(\"operations\", [])\n",
    "        \n",
    "        if not operations:\n",
    "            # Default parallel operations\n",
    "            operations = [\n",
    "                \"Provide a summary\",\n",
    "                \"Extract key points\",\n",
    "                \"Identify main themes\"\n",
    "            ]\n",
    "        \n",
    "        print(f\"  Running {len(operations)} operations in parallel\")\n",
    "        \n",
    "        # Execute all operations in parallel\n",
    "        with ThreadPoolExecutor(max_workers=len(operations)) as executor:\n",
    "            futures = []\n",
    "            for op in operations:\n",
    "                prompt = f\"{op}\\n\\nText: {data}\\n\\nResult:\"\n",
    "                future = executor.submit(self.llm.predict, prompt)\n",
    "                futures.append((op, future))\n",
    "            \n",
    "            results = []\n",
    "            for op, future in futures:\n",
    "                result = future.result()\n",
    "                results.append({\n",
    "                    \"operation\": op,\n",
    "                    \"result\": result\n",
    "                })\n",
    "        \n",
    "        # Combine results\n",
    "        combined = \"\\n\\n\".join([f\"{r['operation']}:\\n{r['result']}\" for r in results])\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"parallel\",\n",
    "            \"operations_count\": len(operations),\n",
    "            \"parallel_results\": results,\n",
    "            \"result\": combined\n",
    "        }\n",
    "    \n",
    "    def execute_hybrid(self, task: Dict) -> Dict:\n",
    "        '''Execute using hybrid pattern (map-reduce + pipeline).'''\n",
    "        print(\"\\nðŸ”„ Executing HYBRID strategy\")\n",
    "        \n",
    "        # Phase 1: Map-Reduce for initial processing\n",
    "        print(\"  Phase 1: Map-Reduce\")\n",
    "        map_reduce_task = {\n",
    "            \"data\": task.get(\"data\", \"\"),\n",
    "            \"map_function\": \"Extract important information\",\n",
    "            \"reduce_function\": \"Synthesize the extracted information\"\n",
    "        }\n",
    "        mr_result = self.execute_map_reduce(map_reduce_task)\n",
    "        \n",
    "        # Phase 2: Pipeline for refinement\n",
    "        print(\"\\n  Phase 2: Pipeline\")\n",
    "        pipeline_task = {\n",
    "            \"data\": mr_result[\"result\"],\n",
    "            \"steps\": [\n",
    "                \"Enhance and clarify the content\",\n",
    "                \"Add structure and formatting\",\n",
    "                \"Polish and finalize\"\n",
    "            ]\n",
    "        }\n",
    "        pipeline_result = self.execute_pipeline(pipeline_task)\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": \"hybrid\",\n",
    "            \"phases\": [\"map_reduce\", \"pipeline\"],\n",
    "            \"result\": pipeline_result[\"result\"]\n",
    "        }\n",
    "    \n",
    "    def _split_data(self, data: str, chunk_size: int) -> List[str]:\n",
    "        '''Split data into chunks.'''\n",
    "        words = data.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size // 5):\n",
    "            chunk = \" \".join(words[i:i + chunk_size // 5])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _process_chunk(self, chunk: str, instruction: str) -> str:\n",
    "        '''Process a single chunk.'''\n",
    "        prompt = f\"{instruction}\\n\\nChunk: {chunk}\\n\\nResult:\"\n",
    "        return self.llm.predict(prompt)\n",
    "    \n",
    "    def orchestrate(self, task: Dict) -> Dict:\n",
    "        '''Main orchestration method with optimization.'''\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Select strategy\n",
    "        strategy = self.select_strategy(task)\n",
    "        print(f\"\\nðŸŽ¯ Selected Strategy: {strategy.value}\")\n",
    "        \n",
    "        # Track initial cost\n",
    "        initial_tokens = self.cost_tracker[\"tokens\"]\n",
    "        \n",
    "        # Execute based on strategy\n",
    "        try:\n",
    "            if strategy == OrchestrationStrategy.MAP_REDUCE:\n",
    "                result = self.execute_map_reduce(task)\n",
    "            elif strategy == OrchestrationStrategy.PIPELINE:\n",
    "                result = self.execute_pipeline(task)\n",
    "            elif strategy == OrchestrationStrategy.PARALLEL:\n",
    "                result = self.execute_parallel(task)\n",
    "            else:  # HYBRID\n",
    "                result = self.execute_hybrid(task)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Estimate tokens (simplified)\n",
    "            estimated_tokens = len(str(result)) // 4\n",
    "            self.cost_tracker[\"tokens\"] += estimated_tokens\n",
    "            self.cost_tracker[\"requests\"] += 1\n",
    "            \n",
    "            # Add performance metrics\n",
    "            metrics = {\n",
    "                \"strategy\": strategy.value,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"estimated_tokens\": estimated_tokens,\n",
    "                \"estimated_cost\": estimated_tokens * 0.000002  # Example pricing\n",
    "            }\n",
    "            \n",
    "            self.performance_metrics.append(metrics)\n",
    "            result[\"metrics\"] = metrics\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Execution Metrics:\")\n",
    "            print(f\"  Time: {execution_time:.2f}s\")\n",
    "            print(f\"  Estimated Tokens: {estimated_tokens}\")\n",
    "            print(f\"  Estimated Cost: ${metrics['estimated_cost']:.4f}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"strategy\": strategy.value,\n",
    "                \"error\": str(e),\n",
    "                \"result\": None\n",
    "            }\n",
    "    \n",
    "    def get_optimization_report(self) -> Dict:\n",
    "        '''Get optimization and performance report.'''\n",
    "        if not self.performance_metrics:\n",
    "            return {\"message\": \"No metrics available\"}\n",
    "        \n",
    "        total_time = sum(m[\"execution_time\"] for m in self.performance_metrics)\n",
    "        total_cost = sum(m[\"estimated_cost\"] for m in self.performance_metrics)\n",
    "        \n",
    "        # Strategy distribution\n",
    "        from collections import Counter\n",
    "        strategy_counts = Counter(m[\"strategy\"] for m in self.performance_metrics)\n",
    "        \n",
    "        return {\n",
    "            \"total_executions\": len(self.performance_metrics),\n",
    "            \"total_time\": total_time,\n",
    "            \"total_estimated_cost\": total_cost,\n",
    "            \"avg_execution_time\": total_time / len(self.performance_metrics),\n",
    "            \"strategy_distribution\": dict(strategy_counts),\n",
    "            \"total_tokens\": self.cost_tracker[\"tokens\"],\n",
    "            \"recommendations\": self._get_optimization_recommendations()\n",
    "        }\n",
    "    \n",
    "    def _get_optimization_recommendations(self) -> List[str]:\n",
    "        '''Generate optimization recommendations.'''\n",
    "        recommendations = []\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            avg_time = sum(m[\"execution_time\"] for m in self.performance_metrics) / len(self.performance_metrics)\n",
    "            \n",
    "            if avg_time > 5:\n",
    "                recommendations.append(\"Consider increasing parallelization for better performance\")\n",
    "            \n",
    "            # Check strategy efficiency\n",
    "            strategy_times = {}\n",
    "            for m in self.performance_metrics:\n",
    "                if m[\"strategy\"] not in strategy_times:\n",
    "                    strategy_times[m[\"strategy\"]] = []\n",
    "                strategy_times[m[\"strategy\"]].append(m[\"execution_time\"])\n",
    "            \n",
    "            for strategy, times in strategy_times.items():\n",
    "                avg_strategy_time = sum(times) / len(times)\n",
    "                if avg_strategy_time > 10:\n",
    "                    recommendations.append(f\"Optimize {strategy} strategy - avg time {avg_strategy_time:.1f}s\")\n",
    "        \n",
    "        if self.cost_tracker[\"tokens\"] > 100000:\n",
    "            recommendations.append(\"High token usage - consider caching or compression\")\n",
    "        \n",
    "        return recommendations if recommendations else [\"System operating efficiently\"]\n",
    "\n",
    "# Test the orchestrator\n",
    "print(\"Advanced Orchestration System Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "orchestrator = AdvancedOrchestrator()\n",
    "\n",
    "# Test different task types\n",
    "test_tasks = [\n",
    "    {\n",
    "        \"name\": \"Large Document Summary\",\n",
    "        \"type\": \"aggregation\",\n",
    "        \"data\": \"AI is transforming industries. \" * 100,  # Large text\n",
    "        \"complexity\": \"high\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Sequential Processing\",\n",
    "        \"type\": \"sequential\",\n",
    "        \"data\": \"Raw data that needs processing\",\n",
    "        \"steps\": [\n",
    "            \"Clean and normalize the data\",\n",
    "            \"Extract meaningful patterns\",\n",
    "            \"Generate insights\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Parallel Analysis\",\n",
    "        \"type\": \"independent\",\n",
    "        \"data\": \"Complex topic requiring multiple perspectives\",\n",
    "        \"parallelize\": True,\n",
    "        \"operations\": [\n",
    "            \"Technical analysis\",\n",
    "            \"Business perspective\",\n",
    "            \"User experience view\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Complex Hybrid Task\",\n",
    "        \"type\": \"complex\",\n",
    "        \"data\": \"Multi-faceted problem requiring sophisticated processing\",\n",
    "        \"complexity\": \"high\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for task in test_tasks:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Task: {task['name']}\")\n",
    "    \n",
    "    result = orchestrator.orchestrate(task)\n",
    "    \n",
    "    print(f\"\\nResult Preview:\")\n",
    "    if result.get(\"result\"):\n",
    "        print(f\"  {result['result'][:200]}...\")\n",
    "\n",
    "# Show optimization report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nðŸ“ˆ Optimization Report:\")\n",
    "report = orchestrator.get_optimization_report()\n",
    "\n",
    "for key, value in report.items():\n",
    "    if key != \"recommendations\":\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n  Recommendations:\")\n",
    "for rec in report[\"recommendations\"]:\n",
    "    print(f\"    â€¢ {rec}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Build an advanced orchestration system with multiple processing patterns!\")\n",
    "print(\"The solution includes map-reduce, pipeline, parallel, and hybrid strategies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n\n## **Summary and Next Steps**\n\nCongratulations! You've mastered advanced LangChain patterns. You can now:\n\nâœ… Implement sophisticated routing and conditional logic\nâœ… Build resilient systems with retry and fallback mechanisms\nâœ… Create self-correcting chains\nâœ… Design map-reduce processing patterns\nâœ… Orchestrate complex multi-pattern workflows\nâœ… Optimize for performance and cost\n\n### **Key Takeaways:**\n- **Routing Enables Flexibility**: Dynamic routing adapts to different inputs\n- **Resilience is Critical**: Production systems need fallbacks and retries\n- **Self-Correction Improves Quality**: Validation and correction loops enhance output\n- **Orchestration Scales**: Map-reduce and parallel patterns handle large workloads\n- **Optimization Matters**: Track metrics and optimize strategies\n\n### **Next Steps:**\n- **Notebook 12**: Learn about Production Deployment\n- **Practice**: Implement these patterns in real applications\n- **Experiment**: Combine patterns for complex use cases\n- **Scale**: Deploy these patterns in production\n\n### **Additional Challenges:**\n1. Build a self-optimizing routing system that learns from usage\n2. Create a distributed map-reduce system across multiple servers\n3. Implement a circuit breaker pattern with adaptive thresholds\n4. Design a cost-optimized orchestrator for different LLM providers\n5. Build a meta-orchestrator that can compose orchestration patterns"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}